{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1fHIWbrpk4NMYcse3Xdkb-S8XiS2pxzJm","timestamp":1721260308720},{"file_id":"1STKhtVVaknUZiohwWLdfyoZ3zg6Veimx","timestamp":1721260179755}],"authorship_tag":"ABX9TyNrRYT+RQKEou1aT/uBNipt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["def todo(self):\n","  # List of primary issues\n","  # todo    description                                   hours   progress      Note\n","  #\n","  # todo0   A01.csv skate, long, downhill, juggling,      76      Processing    Heatmap frequent activities weekly or biweekly.\n","  #         running mean duration by category.\n","  #\n","  # todo1   A01.csv category by day of the week or        .25     DNF.          Hours are spread throughout the day.\n","  #         time of day i.e. morning, afternoon, night\n","  #         or blocks of 3.\n","  #\n","  # todo2   A01.csv nltk the 'Explanation' and 'Notes'                          Word frequency might be useful to find specific muscles.\n","  #         sections? Manual descriptions are already                           Topic analysis is included in 'Activity'. Sentiment analysis is\n","  #         included in the write-up.                                           redundant since 'Notes' is informational and not autobiographical.\n","  #\n","  # todo3   Another tutorial chapter on merge sort.\n","  #         Compare with Python's built-in len(),\n","  #         sort(), and replace().\n","  #\n","  # todo4   The graphing part could be included in        .25     DNF.          This is a good project to learn syntax and documentation since it's visual.\n","  #         Chapter 1 with pandas and SciPy.\n","  #\n","  # todo5   A01.csv longboard and running distance.       .5      DNF.          Running occured around 5 times and longboarding was recorded with time.\n","  #\n","  #\n","  # todo6   P0P1B0B1.csv timeseries graphing.             45.5    Completed\n","  #\n","  # todo7   P1.csv manual vs observed prediction\n","  #         accuracy F1 or RMSE.\n","  #\n","  # todo8   B01.csv Pearson-Correlation and day-          2       Completed     Found that there was not correlation between parametric variables.\n","  #         delayed between calories, alcohol, exercise.                        An index similar to ENSO is redundant since there were no consecutive\n","  #                                                                             observations over 4 alcohol or excessive (calorie - calorie burned).\n","  #\n","  # todo9   A01P01B01 moving window spearman correlation                        Would have to sort these for rank, which was completed in todo0.\n","  #         between activity, duration, time of day, pain,\n","  #         nutrients, calories, alcohol.\n","  #\n","  # todo10  tbd data filling and automatic predictions.\n","  #         idk if thats another chapter or avoided.\n","  #\n","  # todo11  Manual weather observations and PRISM data\n","  #         will be in a different GitHub to avoid confusion.\n","  #\n","  # Time spent at a computer programming\n","  # Total estimate  :\n","  # Total actual    :\n","  #\n","  # Purpose\n","  # The goal of writing this is to waste as much time as possible in between\n","  # skateboarding, lifting, or exercise to avoid overtraining while retaining\n","  # logical thought process during long stretches of unemployment. These were\n","  # written on a computer with a 1.5-2 hour battery to restrict excessive\n","  # programming by limiting hardware access.\n","  #\n","  # Abstract\n","  # No library Python with C-like syntax is used for data manipulation and\n","  # graphing whereby arrays are handled without dictionaries. The only\n","  # library used is Matplotlib for RGB graphing and to avoid writing a image or\n","  # video format that would likely spread misinformation. An implementation of\n","  # the merge sort algorithm was used to alphabetize exercise activity for\n","  # binning and graphing frequency by unique type. The built-in Python methods\n","  # for 'replace', 'split', 'len', and 'sort' were manually written for\n","  # learning purposes.\n","\n","  # Start date: 20250125\n","  # End date:\n","\n","  # Below is an exhaustive list of secondary issues.\n","\n","  # List of secondary issues\n","  # todo   description                                                  progress\n","  # todo0  rewrite parser for unicode csv str/int.\n","  # todo1  Stats class avoid NA, NAAN, -9999, etc.\n","  # todo2  refractor RGB_graphs.\n","  # todo3  monthly means on bar graphs.\n","  # todo4  organize merge_sort into another classe.                     Complete\n","  # todo5  modify merge sort to accept entire CSV.                      Class\n","  # todo6  Handle multi word activity descriptions consistently.        Class\n","  # todo7  switch the second capital letter to lower case if exists.    Class\n","  # todo8  unchain the four merge sort functions.                       Class\n","\n","  return\n","\n","# import sys\n","# for path in sys.path:\n","#   print(path)\n","import matplotlib.pyplot as plt\n","import exercise_module as eu\n","# eu.test_function()\n","# print(sys.version)\n","# 3.10.12 (main, Mar 22 2024, 16:50:05) [GCC 11.4.0]\n","# 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n","\n","# In development\n","###############################################\n","# Part D: Data visualization with a RGB graph #\n","###############################################\n","# Matplotlib for color because otherwise you would have\n","# to write hardware code to avoid using Python or C libraries.\n","class Graphs_rgb: # 'Graphs_rgb_dev()' in 'exercise_module.py'\n","  # Initialize the input variables\n","  def __init__(self, data):\n","    self.data = data\n","\n","  def heatmap_graph_0(self, x, y, values, title):\n","    fig, ax = plt.subplots()\n","    # Create the heatmap using pcolormesh\n","    heatmap = ax.pcolormesh(values, cmap='viridis')\n","    # Add a colorbar\n","    plt.colorbar(heatmap)\n","    # Show all ticks and label them with the respective list entries\n","    ax.set_xticks(range(len(x)), labels=x, rotation=45)\n","    ax.set_yticks(range(len(y)), labels=y)\n","\n","    ax.set_title(title)\n","    #fig.tight_layout()\n","    plt.show()\n","    # Create colorbar\n","    # cbar = ax.figure.colorbar(im, ax=ax)\n","    # cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n","\n","#################################################\n","# Part E: Part D visualization helper functions #\n","#################################################\n","# Merge sort is the fastest for worst case scenario sorting: N log(n)\n","# Implementation is from W3 and modified for AZ with ascii ord():\n","# https://www.w3schools.com/dsa/dsa_algo_mergesort.php\n","class Graphs_sort:\n","  # Initialize the input variables\n","  def __init__(self, data):\n","    self.data = data\n","\n","  # Filters the verb endings using c_replace().\n","  def filter_stop(self,column):\n","    filtered_column = []\n","    for i in column:\n","      if 'Walked' in i:\n","        filtered_column.append(\"Walk\")\n","      elif 'Juggling' in i:\n","        filtered_column.append(\"Juggle\")\n","      elif 'Driving' in i:\n","        filtered_column.append(\"Drive\")\n","\n","      # english hard idk\n","      elif 'Reading' in i:\n","        filtered_column.append(\"Read\")\n","      elif 'Writing' in i:\n","        filtered_column.append(\"Write\")\n","      elif 'No juggling' in i:\n","        filtered_column.append(\"No juggling\")\n","      elif 'Running' in i:\n","        filtered_column.append(\"Run\")\n","      elif 'Hiking' in i:\n","        filtered_column.append(\"Hike\")\n","      elif 'Rested' in i:\n","        filtered_column.append(\"Rest\")\n","      elif 'Stretched' in i:\n","        filtered_column.append(\"Stretch\")\n","\n","      elif i == 'Lifts':\n","        # Could append since this is hard coded but I wanted to test.\n","        verb_less = i.replace(\"s\", \"\")\n","        # verb_less = self.c_replace(i, \"s\", \"\")\n","        filtered_column.append(verb_less)\n","      elif 'ing' in i:\n","        verb_less = i.replace(\"ing\", \"\")\n","        # verb_less = self.c_replace(i, \"ing\", \"\")\n","        filtered_column.append(verb_less)\n","      else:\n","        filtered_column.append(i)\n","    return filtered_column\n","\n","  # Calculates duration using end - start.\n","  def sort_time(self,activity,start,end):\n","    duration = ['Duration']\n","    for i in range(1,len(start)):\n","      # Checks to see if the Activity or Start column is empty.\n","      # if len(activity[i]) == 0 or len(start[i]) == 0:\n","      #  continue\n","      # Estimates sleep at 7 hours.\n","      dur = 0\n","      if 'Sleep' == activity[i]:\n","        dur = str(7*60)\n","      else:\n","        # Converts the '100' digits to '60' minutes in hours.\n","        # Gets the end hour.\n","        if len(end[i]) == 4:      # handles 1030 4 digits\n","          end_sub = end[i][:2]\n","        elif len(end[i]) == 3:    # handles 0930 3 digits\n","          end_sub = end[i][0]\n","        else:                     # handles 0030 2 digits\n","          end_sub = 0\n","        # Gets the start hour.\n","        if len(start[i]) == 4:    # handles 1030 4 digits\n","          start_sub = start[i][:2]\n","        elif len(start[i]) == 3:  # handles 0930 3 digits\n","          start_sub = start[i][0]\n","        else:                     # handles 0030 2 digits\n","          start_sub = 0\n","        # Subtracts 40 minutes since there are 60 in an hour not 100.\n","        if start_sub == end_sub:\n","          if int(end[i]) == int(start[i]):\n","            dur = str(5)\n","          else:\n","            dur = str(int(end[i]) - int(start[i]))\n","        else:\n","          # Turn over from one day to another.\n","          if int(end[i]) < int(start[i]):\n","            # First day's amount of hours (24 - the start time hour)\n","            first_day = 23 - int(start_sub)\n","            first_day_minutes = 60 - int(start[i][2:]) # the last two digits are the minutes\n","            # Second day's hours added to the first day's hours as 'dur' as minutes.\n","            if len(end[i]) == 4:\n","              second_day_minutes = end[i][2:]\n","            elif len(end[i]) == 3:\n","              second_day_minutes = end[i][1:]\n","            else: # There are no extra hours\n","              second_day_minutes = end[i]\n","            end_sub = end_sub + first_day\n","            hunid = (int(end_sub)) * 60 # hour difference converted to minutes\n","            dur = hunid + first_day_minutes + int(second_day_minutes)\n","          else:\n","            hunid = (int(end_sub) - int(start_sub)) * 40\n","            dur = str( ( int(end[i]) - int(start[i]) ) - hunid)\n","      duration.append(dur)\n","    return duration\n","\n","  # Merge sorts a list splice of strings from 'sort_ascii()' based on 'ord()'\n","  # and returns them to 'sort_ascii()'.\n","  def merge(self,left_in,right_in):\n","      result = []\n","      result_activity = []\n","      result_id = []\n","      result_dur = []\n","      i = j = 0\n","      while i < len(left_in[1]) and j < len(right_in[1]):\n","        left = left_in[0][i]\n","        right = right_in[0][j]\n","        left_activity = left_in[1][i]\n","        right_activity = right_in[1][j]\n","        left_id = left_in[2][i]\n","        right_id = right_in[2][j]\n","        left_dur = left_in[3][i]\n","        right_dur = right_in[3][j]\n","        if left < right: # or (left_activity_replace == right_activity and left < right) ?\n","          result.append(left)\n","          result_activity.append(left_activity)\n","          result_id.append(left_id)\n","          result_dur.append(left_dur)\n","          i += 1\n","        elif left > right: # or left_activity == right_activity ?\n","          result.append(right)\n","          result_activity.append(right_activity)\n","          result_id.append(right_id)\n","          result_dur.append(right_dur)\n","          j += 1\n","        else:\n","          if len(left_activity) > len(right_activity):\n","            length = len(right_activity)\n","          else: # same length?\n","            length = len(left_activity)\n","          # Find where the two words are different at k.\n","          for k in range(1,length):\n","            if left_activity[k] != right_activity[k]:\n","              break\n","          # Handles when the comparison first words are the same but\n","          # one of the comparisons have a space and second word.\n","          left_ord = ord(left_activity[k])\n","          right_ord = ord(right_activity[k])\n","          if left_activity[:length] == right_activity[:length]:\n","            if length < len(right_activity):\n","              if left_activity == right_activity[:length]:\n","                left_ord = -1\n","                # Comparison right_activity[length:] is longer and different.\n","                right_ord = ord(right_activity[length:][0])\n","          if left_activity[:k] == right_activity[:k] and left_ord < right_ord:\n","            result.append(left)\n","            result_activity.append(left_activity)\n","            result_id.append(left_id)\n","            result_dur.append(left_dur)\n","            i += 1\n","          else:\n","            result.append(right)\n","            result_activity.append(right_activity)\n","            result_id.append(right_id)\n","            result_dur.append(right_dur)\n","            j += 1\n","      result.extend(left_in[0][i:])\n","      result.extend(right_in[0][j:])\n","      result_activity.extend(left_in[1][i:])\n","      result_activity.extend(right_in[1][j:])\n","      result_id.extend(left_in[2][i:])\n","      result_id.extend(right_in[2][j:])\n","      result_dur.extend(left_in[3][i:])\n","      result_dur.extend(right_in[3][j:])\n","\n","      return [result,result_activity,result_id,result_dur]\n","\n","  # def sort_ascii(self,time_ID,ord_list,activity_filter,duration):\n","  # todo: avoid modification of the input variables with 'input[:]'\n","  def sort_ascii(self,ord_list,activity_filter,time_ID,duration):\n","    length = len(time_ID) - 1\n","    step = 1\n","    while step < length:\n","      for i in range(1, length, 2 * step):\n","        # Time vs space trade off: if you want less space calculate the duration\n","        # with another loop before sorting. Otherwise, the End and Start columns\n","        # are included in sorting and space is linear * number of columns (4).\n","        left = [ord_list[i:i + step],activity_filter[i:i + step], time_ID[i:i + step], duration[i:i + step]]\n","        right = [ord_list[i + step:i + 2 * step],\n","                 activity_filter[i + step:i + 2 * step], time_ID[i + step:i + 2 * step], duration[i + step:i + 2 * step]]\n","        merged = self.merge(left, right)\n","        # Place the merged array back into the original array\n","        for j in range(len(merged[0])):\n","          ord_list[i + j] = merged[0][j]\n","          activity_filter[i + j] = merged[1][j]\n","          time_ID[i + j] = merged[2][j]\n","          duration[i + j] = merged[3][j]\n","      step *= 2  # Double the sub-array length for the next iteration\n","    return [activity_filter,time_ID,duration]\n","\n","  # Returns the time_id and unique activity lists.\n","  def sort_unique_words(self,activity_col):\n","    # A0_length is 0-225\n","    activity_unique = []\n","    # Unique words in Activity\n","    for i in range(len(activity_col)):\n","      if activity_col[i] not in activity_unique:\n","        if len(activity_col[i]) == 0:\n","          continue\n","        else:\n","          activity_unique.append(activity_col[i])\n","    return activity_unique\n","\n","  # Returns the sorted list into AZ bins. C esque syntax.\n","  # Dimensions: 'sort_unique_words' by the number of occurances in 'sort_ascii'.\n","  def sort_unique_bin(self,sort_unique_words,sort_ascii):\n","\n","    # Once the word is different than the next word, bin the next\n","    # word (or words) since the list is already sorted.\n","\n","    # Empty 'unique_bin' is generated with int. Could use '0's but these\n","    # are 0,1,2,...n!\n","    unique_bin = [\n","        [[x],[x]] for x in range(len(sort_unique_words))\n","        ]\n","    count = 0\n","    for i in range(len(sort_ascii[0])):\n","      # Avoids checking 'sort_ascii' past the length of the list.\n","      # print(count, unique_bin[count], len(sort_unique_words))\n","      if i == (len(sort_ascii[0])-1):\n","        # If there are exactly one entry in the final 'sort_unique_words',\n","        # 'count' of type 'int' is placed as two lists into the identifier list.\n","        if unique_bin[-1][0][0].__class__ == int:\n","          unique_bin[-1] = [[sort_unique_words[count]]]\n","        break\n","      if sort_ascii[0][i] == sort_ascii[0][i+1]:\n","        # Words are the same, 'count' does not get incremented.\n","        if unique_bin[count][0][0].__class__ == str:\n","          # If first key or 'unique_bin[count]' is str, don't include it.\n","          unique_bin[count].append([sort_ascii[0][i],sort_ascii[1][i],sort_ascii[2][i]])\n","        else:\n","          unique_bin[count] = [\n","              [sort_unique_words[count]],\n","              [sort_ascii[0][i],sort_ascii[1][i],sort_ascii[2][i]]\n","              ]\n","      elif (sort_ascii[0][i-1] != sort_ascii[0][i]) and (sort_ascii[0][i] != sort_ascii[0][i+1]):\n","        # Previous word and next word are different.\n","        unique_bin[count] = [\n","            [sort_unique_words[count]],\n","            [sort_ascii[0][i],sort_ascii[1][i],sort_ascii[2][i]]\n","            ]\n","        count += 1\n","      elif (sort_ascii[0][i-1] == sort_ascii[0][i]) and (sort_ascii[0][i] != sort_ascii[0][i+1]):\n","        # Previous word is the same, next word is different.\n","        unique_bin[count].append([sort_ascii[0][i],sort_ascii[1][i],sort_ascii[2][i]])\n","        count += 1\n","      else:\n","        count += 1\n","    # Append last element of sorted list onto the bin list at end\n","    unique_bin[-1].append([sort_ascii[0][-1],\n","                           sort_ascii[1][-1],\n","                           sort_ascii[2][-1]])\n","    return unique_bin\n","\n","  # Orders 'int' or 'float' instead of strings.\n","  # secodnary todo use one list instead of several lists (also for sort_ascii())\n","  def merge_int(self, left, right):\n","    result_int = []\n","    result_activity = []\n","    i = j = 0\n","    while i < len(left[0]) and j < len(right[0]):\n","      if left[0][i] < right[0][j]:\n","        result_int.append(left[0][i])\n","        result_activity.append(left[1][i])\n","        i += 1\n","      else:\n","        result_int.append(right[0][j])\n","        result_activity.append(right[1][j])\n","        j += 1\n","    result_int.extend(left[0][i:])\n","    result_int.extend(right[0][j:])\n","    result_activity.extend(left[1][i:])\n","    result_activity.extend(right[1][j:])\n","    return [result_int, result_activity]\n","  # Orders list of only 'int' or 'float'. From W3 schools.\n","  def merge_sort_int(self,int_in,activity_in):\n","    # The splice everything is used to avoid modification\n","    # of the input variables.\n","    array_int = int_in[:]\n","    activity = activity_in[:]\n","    step = 1  # Starting with sub-arrays of length 1\n","    length = len(array_int) - 1\n","    while step < length:\n","      for i in range(0, length, 2 * step):\n","        left = [array_int[i:i + step],activity[i:i + step]]\n","        right = [array_int[i + step:i + 2 * step],activity[i + step:i + 2 * step]]\n","        merged = self.merge_int(left, right)\n","        # Place the merged array back into the original array\n","        for j in range(len(merged[0])):\n","          array_int[i + j] = merged[0][j]\n","          activity[i + j] = merged[1][j]\n","      step *= 2  # Double the sub-array length for the next iteration\n","    return [array_int,activity]\n","\n","  # todo: use multiple variables\n","  # Merges entries if the first word in the string is the same. Uses C syntax.\n","  # If you're a stickler, replace 'for i in range()' with 'while iterator <= len(data)'\n","  def merge_similar_activities(self, sorted_list):\n","    # Specific formatting for this dataset.\n","    ######################################################################\n","    x = [i[0][0] for i in sorted_list[1:]] # or A0_sort_unique\n","    z = [len(k[1:]) for k in sorted_list[1:]] # frequency of each activity\n","    y = []\n","    # Calculate the hours for duration.\n","    for j in sorted_list[1:]:\n","      y_label = []\n","      for m in j[1:]:\n","        y_label.append(float(m[2]))\n","      y.append(round(sum(y_label) / 60, 4))\n","    # This is a bad method to avoid binning 'Skateboard paper' into\n","    # 'Skateboard' but I want to do the graphing.\n","    for ayy in range(len(x)):\n","      if x[ayy] == 'Skateboard paper':\n","        x[ayy] = 'paper Skateboard'\n","      if x[ayy] == 'Skateboard videos':\n","        x[ayy] = 'videos Skateboard'\n","\n","    ######################################################################\n","\n","    # Combining similar Activities. The dataset uses the same word plus\n","    # a number to denote multiple of the same activities on the same day.\n","    y_mean = round(sum(y) / float(len(y)), 4)\n","    count_0 = 0\n","    count_1 = 1\n","    root_bool = False\n","\n","    # Time complexity is the number of Activities.\n","    #for count_0 in range(len(y)):\n","    x[count_0] = x[count_0]\n","    y[count_0] = y[count_0]\n","    z[count_0] = z[count_0]\n","    spliced = []\n","    while True:\n","      if count_1 == len(y):\n","        x[count_0] = x[count_0]\n","        y[count_0] = y[count_0]\n","        z[count_0] = z[count_0]\n","        break\n","      x[count_1] = x[count_1]\n","      y[count_1] = y[count_1]\n","      z[count_1] = z[count_1]\n","      x_0 = x[count_0].split(' ') # c_split\n","      x_1 = x[count_1].split(' ') # c_split\n","      # Comparison operators to find the first instance of the word.\n","      # i.e. 'Eat' followed by 'Eat 0', 'Eat 1', etc.\n","      if x_0[0] == x_1[0]:\n","        if x_1[1] != 'outside':\n","          if ' ' not in x[count_0]:\n","            #if len(x_1) < 3:\n","            if root_bool == False:\n","              root_pos = count_0\n","              root_bool = True\n","      if root_bool == True:\n","        # Skip merging 'Skateboard paper' and 'Skateboard videos'.\n","        if y[root_pos] != y[count_0]:\n","          # 'y' value at 'count_0' is cumulative\n","          y[root_pos] += y[count_0]\n","          z[root_pos] += z[count_0]\n","        # The first 'y' value at 'root_pos' is the same as the total.\n","        x[count_0] = x[root_pos]\n","        y[count_0] = y[root_pos]\n","        z[count_0] = z[root_pos]\n","      else:\n","        x[count_0] = x[count_0]\n","        y[count_0] = y[count_0]\n","        z[count_0] = z[count_0]\n","      # This checks to see if the word and next word are different or the next\n","      # word is the same and is has three or more words, 'root_bool' is False.\n","      if x_0[0] != x_1[0] or len(x_1) >= 3:\n","        if root_bool == True:\n","          extra_numbers = (count_0+1) - (root_pos+1)\n","          # 'spliced' numbers list gets three values that slice the remaining\n","          # identical words in another loop.\n","          spliced = spliced + [[(count_0-extra_numbers),count_0, extra_numbers]]\n","          # The values at 'root_pos' get set to the current value at 'count_0'.\n","          x[count_0-extra_numbers] = x[count_0]\n","          y[count_0-extra_numbers] = y[count_0]\n","          z[count_0-extra_numbers] = z[count_0]\n","          root_bool = False\n","      count_0 += 1\n","      count_1 += 1\n","\n","    ######################################################################\n","    # Reorganizing the strings (bad method)\n","    # Also inserts '\\n' breaks if the string is longer than 10 characters\n","    # and more than one word.\n","    for bay in range(len(x)):\n","      if x[bay] == 'paper Skateboard':\n","        x[bay] = 'Skateboard paper'\n","      if x[bay] == 'videos Skateboard':\n","        x[bay] = 'Skateboard videos'\n","      string_count = 0\n","      for cay in x[bay]:\n","        if string_count > 9:\n","          string_break = x[bay].split(' ')\n","          string_word_count = len(string_break)\n","          if string_word_count > 1:\n","            string_word_mid = round(string_word_count / 2)\n","            string_0_half = \" \".join(string_break[:string_word_mid]) + '\\n'\n","            string_1_half = \" \".join(string_break[string_word_mid:])\n","            x[bay] = string_0_half + string_1_half\n","          break\n","        string_count += 1\n","    return [x,y,z,spliced]\n","\n","  # Uses 'merge_similar_activities' to splice the data.\n","  def merge_activities_splice(self,merged_activiites):\n","    # Time complexities is the number of repeated words that are being merged.\n","    x = []\n","    y = []\n","    z = []\n","    spler = []\n","    for k in range(len(merged_activiites[3])+1): # or while the length is less than the\n","      # The end splice.\n","      if k == len(merged_activiites[3]):\n","        # spler = spler + [[middle_0,0]] <- checks that the numbers are correct\n","        x = x + merged_activiites[0][middle_0:]\n","        y = y + merged_activiites[1][middle_0:]\n","        z = z + merged_activiites[2][middle_0:]\n","        break\n","      # The first splice.\n","      if x == []:\n","        # spler = [[0,merged_activiites[3][k][0]+1]]\n","        x = merged_activiites[0][0:merged_activiites[3][k][0]+1]\n","        y = merged_activiites[1][0:merged_activiites[3][k][0]+1]\n","        z = merged_activiites[2][0:merged_activiites[3][k][0]+1]\n","        # This part is carried into the next splice.\n","        middle_0 = merged_activiites[3][k][0]+1 + merged_activiites[3][k][2]\n","      else: # The middle splices.\n","        # spler = spler + [[middle_0, merged_activiites[3][k][0]+1]]\n","        x = x + merged_activiites[0][middle_0:merged_activiites[3][k][0]+1]\n","        y = y + merged_activiites[1][middle_0:merged_activiites[3][k][0]+1]\n","        z = z + merged_activiites[2][middle_0:merged_activiites[3][k][0]+1]\n","        # This part is carried into the next splice.\n","        middle_0 = merged_activiites[3][k][0]+1 + merged_activiites[3][k][2]\n","    return [x,y,z]\n","\n","##############################################################################\n","# Part Z: Run the functions                                                  #\n","##############################################################################\n","\n","# Part A: The path of the CSV to be parsed\n","def CSV_running(path,unflipped_col):\n","  # Create the CSV_Parser class object and open the files\n","  parser = eu.CSV_Parser(path)\n","  read = parser.file_opener()\n","  # Index the comma position from the CSV and split the characters into their values\n","  comma_indexed = parser.comma_index(read, path, 0)\n","  # Get the width of columns of the commas\n","  comma_width = parser.comma_index(read, path, 1)\n","  # Sort the list into verticle columns\n","  # The P0 csv gets flipped, except for the Stm column\n","  # Divide by two - the list of comma places is doubled for the start/end value\n","  col_width = int(((comma_width - 1 ) / 2) - 1)\n","  vert = []\n","  for i in range(0,comma_width-1,2):\n","    value_list = parser.csv_value_list(comma_indexed, read, col_width, i)\n","    if unflipped_col == 0:\n","      vert.append(value_list)\n","    else:\n","      if value_list[0] in unflipped_col:\n","        vert.append(value_list)\n","      else:\n","        flip = parser.csv_flipper(value_list, col_width)\n","        vert.append(flip)\n","  return vert\n","\n","# One month of May, 2024 observations\n","P0_path = \"/content/P0.csv\"\n","B0_path = \"/content/B0.csv\"\n","# A0 is a TSV because there are blank cells\n","A0_path = \"/content/A0.tsv\"\n","P0_unflipped_col = ['ID','Date','Day','Stm']\n","# P0_vert = CSV_running(P0_path,P0_unflipped_col)\n","# B0_vert = CSV_running(B0_path,0)\n","# A0_vert = CSV_running(A0_path,0)\n","# Four months of July-October observations\n","# P1.csv contains the pain scale and B1.csv contains the food records\n","# P1_path = \"/content/P1-Observations-PaperFigures.csv\"\n","# B1_path = \"/content/B1.csv\"\n","# A1 is a tsv because of blank cells\n","A1_path = \"/content/A1.tsv\"\n","# List of columns to not be flipepd\n","# P1_unflipped_col = ['ID','Date','Day','Stm','Notes','Notes2']\n","# P1_vert = CSV_running(P1_path,P1_unflipped_col)\n","# B1_vert = CSV_running(B1_path,0)\n","A1_vert = CSV_running(A1_path,0)\n","\n","# A0_vert\n","def A0_RGB_graph(A0_vert):\n","  # Part D RGB Graphs: A0.tsv\n","  A0_sort = Graphs_sort(A0_vert)\n","  # Calculates the duration of each activity.\n","  A0_sort_duration = A0_sort.sort_time(A0_sort.data[6],A0_sort.data[4],A0_sort.data[5])\n","  # Removes endings for similar words such as: 'Walk', 'Walks', 'Walked', 'Walking'.\n","  A0_activity_filter = A0_sort.filter_stop(A0_sort.data[6])\n","  # Sorts the list using an implementation of merge sort.\n","  ord_list = ['ord_list'] + [ord(A0_activity_filter[x][0]) for x in range(1,len(A0_activity_filter))]\n","  A0_sort_merged = A0_sort.sort_ascii(ord_list,A0_activity_filter,A0_sort.data[1],A0_sort_duration)\n","  # Finds the unique occurances of each word in 'Activity'.\n","  A0_sort_unique = A0_sort.sort_unique_words(A0_sort_merged[0])\n","  # Bins the sorted list using the unique words.\n","  A0_sort_bin = A0_sort.sort_unique_bin(A0_sort_unique,A0_sort_merged)\n","  # Merges the bins based on if the first word in the string are the same.\n","  # i.e. 'Eat' <- 'Eat 1' <- 'Eat 2' <- 'Eat 3'\n","  A0_sort_similar = A0_sort.merge_similar_activities(A0_sort_bin)\n","  A0_sort_similar_splice = A0_sort.merge_activities_splice(A0_sort_similar)\n","  # Graphs\n","  A0_sort_graph = Graphs_rgb(A0_sort_similar_splice)\n","  A0_sort_graph.rgb_timeseries_frequency()\n","  A0_sort_graph.rgb_timeseries_duration()\n","\n","# A1_vert\n","def A1_RGB_graph(A1_vert):\n","  # Part D RGB Graphs: A0.tsv\n","  A1_sort = Graphs_sort(A1_vert)\n","\n","  # Calculates the duration of each activity.\n","  A1_sort_duration = A1_sort.sort_time(A1_sort.data[6],A1_sort.data[4],A1_sort.data[5])\n","  # Removes endings for similar words such as: 'Walk', 'Walks', 'Walked', 'Walking'.\n","  A1_activity_filter = A1_sort.filter_stop(A1_sort.data[6])\n","  # Fill in the blank days of the week for 'A1_sort.data[2]' i.e. ''\n","  A1_sort_day = []\n","  for i in A1_sort.data[2]:\n","    if len(i) > 0:\n","      check = i\n","    if i == '':\n","      A1_sort_day.append(check)\n","    else:\n","      A1_sort_day.append(i)\n","  # Sorts the list using an implementation of merge sort.\n","  ord_list = ['ord_list'] + [ord(A1_sort_day[x][0]) for x in range(1,len(A1_sort_day))]\n","  A1_sort_merged = A1_sort.sort_ascii(ord_list,A1_sort_day,A1_activity_filter,A1_sort_duration)\n","  # Finds the unique occurances of each word in 'Activity'.\n","  A1_sort_unique = A1_sort.sort_unique_words(A1_sort_merged[0])\n","  # weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n","  # Bins the sorted list using the unique words.\n","  A1_sort_bin = A1_sort.sort_unique_bin(A1_sort_unique,A1_sort_merged)\n","  A1_sort_bin_day = [A1_sort_bin[0],A1_sort_bin[4],A1_sort_bin[2],A1_sort_bin[6],A1_sort_bin[7],A1_sort_bin[5],A1_sort_bin[1],A1_sort_bin[3]]\n","\n","  # Loop over each day and standardize the categories.\n","  A1_week_dur = []\n","  A1_week_freq = []\n","  for day in A1_sort_bin_day[1:]:\n","    weekday_activity = ['Activity']\n","    weekday_duration = ['Duration']\n","    for j in day[1:]:\n","      weekday_activity.append(j[1])\n","      weekday_duration.append(j[2])\n","    weekday_ord = ['weekday_ord'] + [ord(weekday_activity[x][0]) for x in range(1,len(weekday_activity))]\n","    weekday_foo_str = ['foo']\n","    weekday_foo = weekday_foo_str + [str(0) for y in range(len(weekday_activity))] # 'weekday_foo' can be any of the spreadsheet columns.\n","    A1_weekday_sort = A1_sort.sort_ascii(weekday_ord,weekday_activity,weekday_foo,weekday_duration)\n","    A1_weekday_unique = A1_sort.sort_unique_words(A1_weekday_sort[0])\n","    # Bins the sorted list using the unique words.\n","    A1_weekday_bin = A1_sort.sort_unique_bin(A1_weekday_unique,A1_weekday_sort)\n","    # Merges the bins based on if the first word in the string are the same.\n","    # i.e. 'Eat' <- 'Eat 1' <- 'Eat 2' <- 'Eat 3'\n","    A1_weekday_similar = A1_sort.merge_similar_activities(A1_weekday_bin)\n","    A1_weekday_similar_splice = A1_sort.merge_activities_splice(A1_weekday_similar)\n","    # print(A1_weekday_similar_splice)\n","\n","    # Standardize the 'Activity' values for the heatmap.\n","    A1_weekday_standard = ['Core', 'Guitar', 'Juggle', 'Longboard', 'Run', 'Skateboard\\npaper', 'Stretch', 'Walk']\n","    A1_standard_activity = []\n","    A1_standard_duration = []\n","    A1_standard_frequency = []\n","    # Results in Day (Y) by Activity (X)\n","    for k in range(len(A1_weekday_standard)):\n","      A1_standard_activity.append(A1_weekday_standard[k])\n","      A1_standard_duration.append(0)\n","      A1_standard_frequency.append(0)\n","      for m in range(len(A1_weekday_similar_splice[0])):\n","        if A1_weekday_similar_splice[0][m] == A1_weekday_standard[k]: # otherwise skip the 'Activity'\n","          A1_standard_duration[k] = A1_weekday_similar_splice[1][m]\n","          A1_standard_frequency[k] = A1_weekday_similar_splice[2][m]\n","    # print(A1_standard_activity,A1_standard_duration,A1_standard_frequency)\n","    A1_week_dur.append(A1_standard_duration)\n","    print(A1_standard_duration)\n","    A1_week_freq.append(A1_standard_frequency)\n","\n","  # Get some vert.\n","  count = 0\n","  count_ct = 0\n","  count_br = 0\n","  A1_dur_week_rev = []\n","  activity_count = []\n","  while True:\n","    if count == len(A1_week_dur): # len 7\n","      print(activity_count)\n","      A1_dur_week_rev.append(activity_count)\n","      count = 0\n","      count_ct += 1\n","      activity_count = []\n","      print()\n","      if count_br == ((len(A1_week_dur[0])) * (len(A1_week_dur))):\n","        break\n","    activity_count.append(A1_week_dur[count][count_ct])\n","    count += 1\n","    count_br += 1\n","\n","\n","\n","  weekdays = ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n","  A1_sort_graph = Graphs_rgb(0)\n","  #A1_heatmap_dur = A1_sort_graph.heatmap_graph_0(weekdays, A1_weekday_standard, A1_week_dur, 'Daily Activity Duration Total')\n","  # A1_heatmap_freq = A1_sort_graph.heatmap_graph_0(weekdays, A1_weekday_standard, A1_week_freq, 'Daily Activity Frequency Total')\n","  # A1_heatmap_dur_mean\n","  # A1_heatmap_freq_mean\n","  '''\n","  A1_week_dur\n","  A1_week_freq\n","  weekdays\n","  A1_weekday_standard\n","  '''\n","  # Heatmap Graph 0 (Days of the week)\n","\n","  # day, date, start, end, activity\n","  # A1_sort.data 2-6 # (len(5)) or 3rd to seventh item in list\n","\n","  # Graph 1\n","  # Heat map frequency top 8 activities time of day\n","  #                     0000-0659, 0700-0900, 0900-1100, 1100-1300, 1300-1500, 1500-1700, 1700-1900, 1900-2359\n","  # Guitar 0-7 (len(8))\n","  # Stretch\n","  # Sb paper\n","  # Walk\n","  # Longboard\n","  # Juggle\n","  # Core\n","  # Run\n","\n","  # Heat map duration top 8 activities time of day\n","  #                     0000-0659, 0700-0900, 0900-1100, 1100-1300, 1300-1500, 1500-1700, 1700-1900, 1900-2359\n","  # Guitar 0-7 (len(8))\n","  # Stretch\n","  # Sb paper\n","  # Walk\n","  # Longboard\n","  # Juggle\n","  # Core\n","  # Run\n","\n","  # Graph 3\n","  # Heat map frequency top 8 activities biweekly\n","  #                     0722-0804, 0805-0818, 0819-0901, 0902-0915, 0916-0929, 0930-1013, 1014-1029\n","  # Guitar 0-7 (len(8))\n","  # Stretch\n","  # Sb paper\n","  # Walk\n","  # Longboard\n","  # Juggle\n","  # Core\n","  # Run\n","\n","  # Heat map duration top 8 activities bi weekly\n","  #                     0722-0804, 0805-0818, 0819-0901, 0902-0915, 0916-0929, 0930-1013, 1014-1029\n","  # Guitar 0-7 (len(8))\n","  # Stretch\n","  # Sb paper\n","  # Walk\n","  # Longboard\n","  # Juggle\n","  # Core\n","  # Run\n","\n","\n","  # Calculates the duration of each activity.\n","  A1_sort_duration = A1_sort.sort_time(A1_sort.data[6],A1_sort.data[4],A1_sort.data[5])\n","  # Removes endings for similar words such as: 'Walk', 'Walks', 'Walked', 'Walking'.\n","  A1_activity_filter = A1_sort.filter_stop(A1_sort.data[6])\n","  # Sorts the list using an implementation of merge sort.\n","  ord_list = ['ord_list'] + [ord(A1_activity_filter[x][0]) for x in range(1,len(A1_activity_filter))]\n","  A1_sort_merged = A1_sort.sort_ascii(ord_list,A1_activity_filter,A1_sort.data[1],A1_sort_duration)\n","  # Finds the unique occurances of each word in 'Activity'.\n","  A1_sort_unique = A1_sort.sort_unique_words(A1_sort_merged[0])\n","  # Bins the sorted list using the unique words.\n","  A1_sort_bin = A1_sort.sort_unique_bin(A1_sort_unique,A1_sort_merged)\n","\n","  # Merges the bins based on if the first word in the string are the same.\n","  # i.e. 'Eat' <- 'Eat 1' <- 'Eat 2' <- 'Eat 3'\n","  A1_sort_similar = A1_sort.merge_similar_activities(A1_sort_bin)\n","  '''\n","  A1_sort_similar_splice = A1_sort.merge_activities_splice(A1_sort_similar)\n","  '''\n","  '''\n","\n","  # Sort the values for each horizontal bar graph.\n","  A1_sort_similar_2 = A1_sort.merge_sort_int(A1_sort_similar_splice[2],A1_sort_similar_splice[0])\n","  A1_sort_graph = Graphs_rgb(0)\n","  # A1_sort_graph.rgb_timeseries_frequency(A1_sort_similar_2)                     # ( 2, 0 )\n","  A1_sort_similar_1 = A1_sort.merge_sort_int(A1_sort_similar_splice[1],A1_sort_similar_splice[0])\n","  # A1_sort_graph.rgb_timeseries_duration(A1_sort_similar_1)                      # ( 0, 1 )\n","  # Total activity mean.\n","  A1_sort_duration_mean = []\n","  for i in range(len(A1_sort_similar_splice[1])):\n","    duration_mean = A1_sort_similar_splice[1][i] / A1_sort_similar_splice[2][i]\n","    A1_sort_duration_mean.append(duration_mean)\n","  A1_sort_similar_3 = A1_sort.merge_sort_int(A1_sort_duration_mean,A1_sort_similar_splice[0])\n","  # A1_sort_graph.rgb_timeseries_mean(A1_sort_similar_3)                          # ( 0,(1/2) )\n","  # Daily mean.\n","  A1_sort_daily_mean = []\n","  for i in range(len(A1_sort_similar_splice[1])):\n","    duration_mean = (A1_sort_similar_splice[2][i] / 100) * 60\n","    A1_sort_daily_mean.append(duration_mean)\n","  A1_sort_similar_4 = A1_sort.merge_sort_int(A1_sort_daily_mean,A1_sort_similar_splice[0])\n","  # A1_sort_graph.rgb_timeseries_daily_mean(A1_sort_similar_4)                    # ( 0,((1/2) / 100) * 60)\n","  '''\n","\n","# P1_RGB_graph(P1_vert)\n","# B1_RGB_graph(B1_vert)\n","# A0_RGB_graph(A0_vert)\n","A1_RGB_graph(A1_vert)\n"],"metadata":{"id":"IxtfVwAjhyaV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745434253088,"user_tz":300,"elapsed":530,"user":{"displayName":"David Leifer","userId":"06279506333224389759"}},"outputId":"37b3f186-3f48-416c-a2a8-fca116816e24"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.6667, 8.5, 1.75, 6.0, 2.5, 10.75, 5.4167, 3.5]\n","[1.25, 14.5, 3.5, 0.5, 1.75, 14.5, 4.0833, 7.0]\n","[1.05, 10.0833, 1.9167, 3.8333, 1.6667, 14.0, 6.1167, 5.5833]\n","[0.5833, 10.0833, 2.25, 4.0, 3.0, 13.25, 5.25, 2.75]\n","[0.4167, 8.5833, 1.0, 0, 0.4167, 10.5, 3.3333, 6.75]\n","[0.3333, 10.5833, 2.5, 4.5832999999999995, 1.4167, 21.25, 3.4167, 5.0]\n","[0.25, 8.0, 1.75, 8.333300000000001, 0.5, 17.25, 3.8667, 2.0]\n","\n","\n","[0.6667, 1.25, 1.05, 0.5833, 0.4167, 0.3333, 0.25]\n","\n","[8.5, 14.5, 10.0833, 10.0833, 8.5833, 10.5833, 8.0]\n","\n","[1.75, 3.5, 1.9167, 2.25, 1.0, 2.5, 1.75]\n","\n","[6.0, 0.5, 3.8333, 4.0, 0, 4.5832999999999995, 8.333300000000001]\n","\n","[2.5, 1.75, 1.6667, 3.0, 0.4167, 1.4167, 0.5]\n","\n","[10.75, 14.5, 14.0, 13.25, 10.5, 21.25, 17.25]\n","\n","[5.4167, 4.0833, 6.1167, 5.25, 3.3333, 3.4167, 3.8667]\n","\n","[3.5, 7.0, 5.5833, 2.75, 6.75, 5.0, 2.0]\n","\n"]}]},{"cell_type":"code","source":["### |a = 'abcdefg'\n","'''\n","for i in range(1,len(a)):\n","  print(a[:i])\n","  print(a[i])\n","  print('zzzzz')\n","\n","  print(a[-1])'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"nyadd2nCLeqQ","executionInfo":{"status":"ok","timestamp":1744989152983,"user_tz":300,"elapsed":19,"user":{"displayName":"David Leifer","userId":"06279506333224389759"}},"outputId":"6c2810b5-68e0-40fb-aa98-bf24388e05dc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nfor i in range(1,len(a)):\\n  print(a[:i])\\n  print(a[i])\\n  print('zzzzz')\\n\\n  print(a[-1])\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["abc = [\n","        [[0], ['a']],\n","        [[1], ['b']],\n","        [[2], ['c']],\n","        [[3], ['d']],\n","        [[4], ['e']],\n","        [[5], ['f']],\n","        [[6], ['g']],\n","        [[7], ['h']],\n","        [[8], ['i']],\n","        [[9], ['j']],\n","                      ]\n","# for i in range(10):\n","print(abc[1:2])"],"metadata":{"id":"SLyAYj_xtzdg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744137032714,"user_tz":300,"elapsed":6,"user":{"displayName":"David Leifer","userId":"06279506333224389759"}},"outputId":"3bb4744a-366f-4183-a0ca-7f695d31af9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[1], ['b']]]\n"]}]},{"cell_type":"code","source":["a = '1234'\n","b = '567'\n","c = '89'\n","\n","\n","if len(a) == 4:\n","  end_sub = a[2:]\n","  print(end_sub)\n","\n","if len(b) == 3:\n","  end_sub = b[:1]\n","  print(end_sub)\n","\n","if len(c) == 2:\n","  end_sub = c"],"metadata":{"id":"kti4uK-eVPeZ","executionInfo":{"status":"ok","timestamp":1744769169351,"user_tz":300,"elapsed":9,"user":{"displayName":"David Leifer","userId":"06279506333224389759"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1c1bc210-f9b0-4b10-b49e-62551d97b986"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["34\n","5\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"bNU5wcEd0AeC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6wounMFJaZT3"},"execution_count":null,"outputs":[]}]}