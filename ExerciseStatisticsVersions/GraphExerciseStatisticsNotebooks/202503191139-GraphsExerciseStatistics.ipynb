{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1fHIWbrpk4NMYcse3Xdkb-S8XiS2pxzJm","timestamp":1721260308720},{"file_id":"1STKhtVVaknUZiohwWLdfyoZ3zg6Veimx","timestamp":1721260179755}],"authorship_tag":"ABX9TyNIEuc4q4Hm4r5p7lB044eM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["def todo(self):\n","  # List of primary issues\n","  # todo    description                                   hours   progress      Note\n","  #\n","  # todo0   A0/1.csv skate, long, downhill, juggling,     25      Processing    rgb_unique_bin\n","  #         running mean duration by category.\n","  # todo1   A0/1.csv longboard distance, running\n","  #         distance per day, overall mean\n","  # todo2   P0.csv observations, mean in each,            20      Halted        At ARIMA\n","  #         category ARIMA (after todo3).\n","  # todo3   p1 observations, manual prediction, mean in   23.5    Halted        At ARIMA\n","  #         each category, ARIMA.\n","  # todo4   B01.csv calories, salt, fat, protein, carb,   2       Halted        At ARIMA\n","  #         alc each day, overall mean, ARIMA (todo3).\n","  # todo5   B01 weather summary, OS PRISM, exercise\n","  #         intensity.\n","  # todo6   B01 (moving window) correlation between\n","  #         exercise and ___.\n","  #         (calorie, real weather, alcohol)\n","  # todo7   B01 moving window spearman correlation\n","  #         between todo0 duration and pain, calories\n","  #         burned. duration and nutrients. real and\n","  #         summary weather. pain and real weather.\n","  #         pain and summary weather. ???.\n","  # todo8   tbd data filling idk if thats another\n","  #         chapter or avoided.\n","  #\n","  # Time spent at a computer programming\n","  # Total estimate  :\n","  # Total actual    :\n","  #\n","  # Summary\n","  # asdfasdf\n","  #\n","  # Start date: 20250125\n","  # End date:\n","\n","  # There is an exhaustive list of excessive secondary issues.\n","\n","  # List of secondary issues\n","  # todo   description                              progress\n","  # todo0  rewrite parser for unicode csv str/int\n","  # todo1  Stats class avoid NA, NAAN, -9999\n","  # todo2  refractor RGB_graphs\n","  # todo3  monthly means on bar graphs\n","  # todo4  unchain the four merge sort functions\n","  # todo5  modify merge sort to accept entire CSV\n","  # todo6  organize rgb_graphs into additional classes\n","  return\n","\n","import sys\n","import matplotlib.pyplot as plt\n","print(sys.version)\n","# 3.10.12 (main, Mar 22 2024, 16:50:05) [GCC 11.4.0]\n","# 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n","\n","# Interesting method to print callable methods on an object.\n","# print(dir(left_right))\n","# Example is obj.__class__ that prints the data type without using the type(obj) method\n","# type_check = str(left_right.__class__)\n","\n","#####################################################################################\n","# Part A: CSV or TSV parser class to open the file and parse the values into a list #\n","#####################################################################################\n","class CSV_Parser:\n","  # Initialize the input variables\n","  def __init__(self, data_path):\n","    self.data_path = data_path\n","  def file_opener(self):\n","    with open(self.data_path, \"r\") as data_open:\n","      data_read = data_open.read()\n","      return data_read\n","  # Index the commas and line breaks\n","  def comma_index(self, open_file, path, column_len):\n","    data_comma_place = [0]\n","    column_pl_len = 0\n","    path_split = [ext for ext in path]\n","    path_ext = \"\".join(path_split[-3:])\n","    for i in range(len(open_file)):\n","      data1_col1 = open_file[i]\n","      if path_ext == \"csv\":\n","        if data1_col1 == \",\":\n","          data_comma_place.append(i)\n","          data_comma_place.append(i+1)\n","      elif path_ext == \"tsv\":\n","        if data1_col1 == \"\\t\":\n","          data_comma_place.append(i)\n","          data_comma_place.append(i+1)\n","      if data1_col1 == \"\\n\":\n","        data_comma_place.append(i)\n","        data_comma_place.append(i+1)\n","        # If you want to use the function to get the column width, set to 1\n","        if column_len == 1:\n","          col_width = len(data_comma_place)\n","          return col_width\n","          break\n","    last_val = data_comma_place[-1] + 2\n","    data_comma_place.append(last_val)\n","    return data_comma_place\n","  # Splitting the csv characters into list of words based on indexed comma position\n","  def csv_value_list(self, data_comma_out, open_file, col_width, col_head):\n","    j = col_head\n","    data_val_list = []\n","    for i in range(len(data_comma_out)):\n","      if j >= len(data_comma_out):\n","        break\n","      comma_strt = data_comma_out[j]\n","      j += 1\n","      comma_end = data_comma_out[j]\n","      #print(comma_strt, \"and \", comma_end)\n","      j += 1\n","      data_val_list.append(open_file[comma_strt:comma_end])\n","      j = j + (col_width * 2)\n","    return data_val_list\n","  # Flipping the columns from high to low for readability\n","  # If the original value was 5, set it to equal 0 (no pain)\n","  # If the original value was 0, set it to equal to 5 (high pain) etc.\n","  # If none of those things are true, append the string (for the column header)\n","  def csv_flipper(self, csv_list, col_width):\n","    csv_flipped = []\n","    for i in csv_list:\n","      if i == str(5):\n","        n = str(1)\n","        csv_flipped.append(n)\n","      elif i == str(4):\n","        n = str(2)\n","        csv_flipped.append(n)\n","      elif i == str(3):\n","        n = str(3)\n","        csv_flipped.append(n)\n","      elif i == str(2):\n","        n = str(4)\n","        csv_flipped.append(n)\n","      elif i == str(1):\n","        n = str(5)\n","        csv_flipped.append(n)\n","      else:\n","        csv_flipped.append(i)\n","    return csv_flipped\n","\n","######################################################\n","# Part B: Get descriptive statistics of each column. #\n","######################################################\n","class Statistics:\n","  # Returns a dictionary with the header and mean\n","  def mu(self, col_list):\n","    total = 0\n","    counter = 0\n","    # Column has to have a header\n","    for i in col_list[1:]:\n","      if i == \"NA\":\n","        continue\n","      total = total + float(i)\n","      counter += 1\n","    mean = total / counter\n","    header_mean = [col_list[0], mean]\n","    return header_mean\n","  # Returns the 2-4 moment of the distribution\n","  # Different than Google Sheets sample vs population\n","  def mnt(self, header, mean, col_list):\n","    col_1 = len(col_list) - 1\n","    stn = 0\n","    skew = 0\n","    kurt = 0\n","    counter = 0\n","    for i in col_list[1:]:\n","      if i == \"NA\":\n","        continue\n","      # secondary todo: doesn't work with decimals\n","      n1 = int(i) - mean\n","      n1_sqr = n1 ** 2\n","      n1_cube = n1 ** 3\n","      n1_quad = n1 ** 4\n","      stn = stn + n1_sqr\n","      skew = skew + n1_cube\n","      kurt = kurt + n1_quad\n","      counter += 1\n","    # Sample variance (n-1)\n","    # Population variance (n)\n","    counter = (counter - 1)\n","    stn_small_sqr = float(stn) / counter\n","    stn_small = stn_small_sqr ** .5\n","    skew_small_sqr = float(skew) / counter\n","    skew_small = skew_small_sqr / (stn_small ** 3)\n","    kurt_small_sqr = float(kurt) / counter\n","    kurt_small = kurt_small_sqr / (stn_small ** 4)\n","    return [header, stn_small, skew_small, kurt_small]\n","  # Covariance and correlation\n","  def covar(self, x_mean, y_mean, col_1_list, col_2_list):\n","    col_len = len(col_1_list) - 1\n","    covar = 0\n","    x1y1_sum = 0\n","    counter = 0\n","    for i in range(col_len):\n","      #if i == col_len-2:\n","      #  break\n","      if col_1_list[i+1] == \"NA\":\n","        continue\n","      if col_2_list[i+1] == \"NA\":\n","        continue\n","      # print(i+3,i)\n","      # print(\"x_mean: \", x_mean[1], \"x_value: \", col_1_list[i+1])\n","      x1 = float(col_1_list[i+1]) - x_mean[1]\n","      y1 = float(col_2_list[i+1]) - y_mean[1]\n","      x1y1 = x1 * y1\n","      x1y1_sum = x1y1_sum + x1y1\n","      counter += 1\n","    covar = x1y1_sum / counter\n","    return covar\n","  def cor(self, covar, col_1_stnd, col_2_stnd):\n","    stnd12 = col_1_stnd * col_2_stnd\n","    cor = covar / stnd12\n","    return cor\n","\n","######################################################\n","# Part C: Data visualization with a timeseries graph #\n","######################################################\n","# 'Graph' class accepts three variables: verticle arranged 'data',\n","# the date column'date_col_num', and the data column 'data_col_num'\n","class Graph:\n","  # Initialize the input variables\n","  def __init__(self, data, date_col_num, data_col_num):\n","    self.data = data\n","    self.date_col_num = date_col_num\n","    self.data_col_num = data_col_num\n","  def hi_lo(self, data_col_num):\n","    # The date and date column to be used\n","    data_col = self.data[data_col_num]\n","    data_col_len = len(data_col)\n","    # High and low of values\n","    hi_lo_count = 1\n","    hi = data_col[1]\n","    lo = data_col[1]\n","    for e in range(len(data_col[1:])):\n","      hi_lo_count += 1\n","      if hi_lo_count == (len(data_col[1:]) + 1):\n","        break\n","      if hi < data_col[hi_lo_count]:\n","        hi = data_col[hi_lo_count]\n","      if lo > data_col[hi_lo_count]:\n","        lo = data_col[hi_lo_count]\n","    return [hi, lo]\n","  def binned(self, hi_lo):\n","    # high value (5 in this case or hi_lo[0])\n","    # The date and date column to be used\n","    # TODO it works but is not resuable for other data ranges\n","    date_col = self.data[self.date_col_num]\n","    data_col = self.data[self.data_col_num]\n","    data_col_len = len(data_col)\n","\n","    fiver = []\n","    fourer = []\n","    threer = []\n","    twoer = []\n","    oner = []\n","    lol_stm_date = []\n","    counter = 1\n","    # Binned with date value\n","    for i in range(data_col_len):\n","      P0_column = self.data[self.data_col_num]\n","      if counter == len(self.data[0]):\n","        break\n","      # secondary todo:\n","      if P0_column[counter] == \"NA\":\n","        counter += 1\n","        continue\n","      if float(P0_column[counter]) == 5:\n","        fiver.append([date_col[counter], P0_column[counter]])\n","      elif float(P0_column[counter]) == 4:\n","        fourer.append([date_col[counter], P0_column[counter]])\n","      elif float(P0_column[counter]) == 3:\n","        threer.append([date_col[counter], P0_column[counter]])\n","      elif float(P0_column[counter]) == 2:\n","        twoer.append([date_col[counter], P0_column[counter]])\n","      elif float(P0_column[counter]) == 1:\n","        oner.append([date_col[counter], P0_column[counter]])\n","      counter += 1\n","    # Combining the binned data into one dictionary\n","    lol_date_stm = [fiver, fourer, threer, twoer, oner]\n","    return lol_date_stm\n","\n","  def time_series(self, date_hi_lo, lol_date_stm):\n","    date_col = self.data[self.date_col_num]\n","    # Base of the month, plus 00 i.e. 500\n","    date_base = int(date_hi_lo[1]) - 1\n","    # The number of spaces is the 'day' (date - month) - 'prev_day_space_int'\n","    # The difference between the values is multiplied by ' ' for each 5,4,3,2,1\n","    # Value with a '+' character marking the position\n","    spacer = []\n","    prev_day_space_int = 0\n","    for i in lol_date_stm:\n","      spacer_mid = []\n","      for ii in i:\n","        # ii is [date, value] in order\n","        # an if else statement\n","        # 7 starts at 22, 31 days\n","        # 8 30 days\n","        # 9 30 days\n","        # 10 31 days\n","        day = int(ii[0]) - date_base\n","        # print(day)\n","        day_count = day - prev_day_space_int\n","        day_space_str = ((day_count-1)*2) * \" \"\n","        spacer_mid.append(day_space_str)\n","        prev_day_space_int = day\n","      prev_day_space_int = 0\n","      spacer.append(spacer_mid)\n","    y_val = [y for y in range(len(spacer),0,-1)]\n","    return [spacer, y_val]\n","  def time_series_print(self,spacer,y_val):\n","    date_col = self.data[self.date_col_num]\n","    for j,k in zip(spacer,y_val):\n","      # y values\n","      print(k,end=\" \")\n","      for l in j:\n","        print(l,end=\"\")\n","        print(\"+\",end=\" \")\n","      print()\n","    # x values\n","    for m in range(len(date_col[0])-1):\n","      print(\"  \",end=\"\")\n","      for n in date_col[1:]:\n","        print(n[m], end=\" \")\n","      print()\n","    return\n","  def time_series_write(self,header,txt_out,spacer,y_val):\n","    # Open the output file location and write data to the txt\n","    date_col = self.data[self.date_col_num]\n","    file_output = open(txt_out, \"w\")\n","    file_output.write(header)\n","    file_output.write(\"\\n\")\n","    file_output.write(\"\\n\")\n","    # y values\n","    for j,k in zip(spacer,y_val):\n","      file_output.write(str(k) + \" \")\n","      for l in j:\n","        file_output.write(str(l))\n","        file_output.write(\"+\" + \" \")\n","      file_output.write(\"\\n\")\n","    # x values\n","    for m in range(len(date_col[0])-1):\n","      file_output.write(\"  \")\n","      for n in date_col[1:]:\n","        file_output.write(str(n[m]) + \" \")\n","      file_output.write(\"\\n\")\n","    file_output.close()\n","    return\n","\n","###############################################\n","# Part D: Data visualization with a RGB graph #\n","###############################################\n","# Matplotlib for color because otherwise you would have\n","# to write hardware code to avoid using Python or C libraries.\n","class Graphs_rgb:\n","  # Initialize the input variables\n","  def __init__(self, data):\n","    self.data = data\n","\n","  # Four utility functions daisy chained to rgb_timeseries_bar()\n","  # Minor todo: unchain them lol\n","  def rgb_timeseries_mean(self,formatted_data_group):\n","    # Input is list (1-4) of lists (95) of each columns values without NA\n","    # i.e. [[dist1],[dist1],[dist1], etc]\n","    date_col_len = len(formatted_data_group[0])\n","    group_mean = []\n","    # Length of the column (95 without \"NA\" as filtered in rgb_date_time)\n","    for i in range(date_col_len):\n","      row_list = []\n","      # Length of columns to be summarized (1-4) 95 row_list values\n","      for j in range(len(formatted_data_group)):\n","        row_list.append(formatted_data_group[j][i])\n","      # Mean at each day for each group\n","      row_count = len(row_list)\n","      row_sum = sum(row_list)\n","      row_mean = row_sum / row_count\n","      group_mean.append(row_mean)\n","    return group_mean\n","\n","  def rgb_date_time(self,csv_groups,date_col):\n","    day_count = len(self.data[1])\n","    k = 0\n","    group_dist = []\n","    for i in csv_groups:\n","      dist0 = []\n","      dist1 = []\n","      dist2 = []\n","      for j in range(day_count):\n","        if j == (day_count-1):\n","          break\n","        if i[j+1] == \"NA\":\n","          continue\n","        else:\n","          # Formatting the date\n","          # year = 2024\n","          date_length = date_col[j+1]\n","          if len(date_length) < 4:\n","            month = date_length[:1]\n","            day = date_length[1:]\n","          else:\n","            month = date_length[:2]\n","            day = date_length[2:]\n","          date_format0 = month + \"/\" + day\n","          dist0.append(date_format0)\n","          dist1.append(int(i[j+1]))\n","          date_format1 = month + \"/\" + day\n","          if int(day) % 5 == 0:\n","            dist2.append(date_format1)\n","          else:\n","            dist2.append(\" \")\n","            continue\n","      group_dist.append([dist0,dist1,dist2])\n","      k += 1\n","    return group_dist\n","\n","  def rgb_P1_style(self,final_title,line):\n","    plt.yticks(range(1,6))\n","    if final_title == 'Stamina':\n","      plt.ylabel(final_title)\n","    else:\n","      plt.title(final_title)\n","      plt.ylabel(\"Pain\")\n","      if line == 1:\n","        plt.legend()\n","    return\n","\n","  def rgb_B1_style(self,final_title,line):\n","    if final_title == 'Calories':\n","      plt.yticks(range(1200,4500,400))\n","      plt.ylabel(\"Intake\")\n","      plt.title(final_title)\n","    elif final_title == 'Alcohol Servings':\n","      plt.yticks(range(0,16))\n","      plt.title(\"Alcohol\")\n","      plt.ylabel(\"Servings\")\n","    elif final_title == 'Exercise':\n","      plt.yticks(range(0,3))\n","      plt.title(final_title)\n","      # plt.ylabel(\"Calories Out\")\n","      plt.text(.1,.5, \"Calories Out \\n2 = 250+ \\n1 = 1-249\",\n","         bbox={'facecolor': 'white', 'alpha': .75, 'pad': 10})\n","    else:\n","      plt.yticks(range(1,6))\n","      if line == 1:\n","        plt.title(\"Nutrients\")\n","      else:\n","        plt.title(final_title)\n","      plt.ylabel(\"Intake\")\n","      plt.legend()\n","    return\n","\n","  # Bar plots for each column\n","  def rgb_timeseries_bar(self,title_full,start_val,P1_B1):\n","    for i in range(start_val,len(title_full)+start_val):\n","      formatted_csv_group = self.rgb_date_time([self.data[i]],self.data[1])\n","      fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(15, 5))\n","      ax.bar(formatted_csv_group[0][0], formatted_csv_group[0][1], width=0.8, align='edge')\n","      final_title = title_full[i-start_val]\n","      # Format the title, yticks, and ylabel\n","      if P1_B1 == 0:\n","        self.rgb_P1_style(final_title,0)\n","      elif P1_B1 == 1:\n","        self.rgb_B1_style(final_title,0)\n","      elif P1_B1 == 2:\n","        pass\n","        # self.rgb_A0_style(final_title,0)\n","      plt.xticks(formatted_csv_group[0][0], labels=formatted_csv_group[0][2])\n","      plt.margins()\n","      plt.grid()\n","      # plt.savefig(final_title + '.jpg')\n","    return\n","\n","\n","  # Returns a date list without blanks\n","  def rgb_date_list(self):\n","    # date_literal is 0-30 days\n","    date_literal = []\n","    # Makes a list with only the dates\n","    for i in range(1,len(self.data[2])):\n","      if len(self.data[2][i]) > 0:\n","        date_literal.append(self.data[2][i])\n","    return date_literal\n","\n","  # Multiple lines same graphs.\n","  def rgb_timeseries_line(self,title_full,start_val,groups_num,title_label,P1_B1):\n","    data = self.data\n","    # secondary todo: name instead of number position\n","    j = 1\n","    # Adding multiple lines to a single plot by group with formatting\n","    for i in range(len(groups_num)):\n","      subset0 = groups_num[i:j][0]\n","      if subset0 == groups_num[-1]:\n","        break\n","      subset1 = groups_num[i+1:j+1][0]\n","      csv_groups = data[subset0:subset1]\n","      formatted_csv_group = self.rgb_date_time(csv_groups,self.data[1])\n","      # Format subplot\n","      fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(15, 5))\n","      # Get the formatted_csv_group second list of values in each group\n","      dist1_list = [dist1[1] for dist1 in formatted_csv_group]\n","      # First 3 columns in data are ID, while the title list isn't.\n","      # Subtract each subset by the start_val of the values (excluding date, id, etc)\n","      title_group = title_full[(subset0-start_val):(subset1-start_val)]\n","      # y = each dist1 in formatted_csv_group, x = every date value, x labels = every 5th date value\n","      for k in range(len(dist1_list)):\n","        ax.plot(formatted_csv_group[0][0], dist1_list[k], label=title_group[k], linewidth=4)\n","        # Format the title, yticks, and ylabel\n","        if P1_B1 == 0:\n","          self.rgb_P1_style(title_label[j-1],1)\n","        elif P1_B1 == 1:\n","          self.rgb_B1_style(title_label[j-1],1)\n","      # Chart formatting and save\n","      plt.xticks(formatted_csv_group[0][0], labels=formatted_csv_group[0][2])\n","      plt.grid()\n","      plt.margins()\n","      #plt.savefig(title_label[j-1] + '.jpg')\n","      j += 1\n","    return\n","\n","  # Summarized with mean\n","  def rgb_timeseries_small(self,csv_groups_num,legend_label,ax):\n","    csv = self.data\n","    j = 1\n","    for i in range(len(csv_groups_num)):\n","      subset0 = csv_groups_num[i:j][0]\n","      if subset0 == csv_groups_num[-1]:\n","        break\n","      subset1 = csv_groups_num[i+1:j+1][0]\n","      csv_groups = csv[subset0:subset1]\n","      # Builds an array to skip NA and format the date\n","      # [[[dist0],[1],[2]],[[dist0],[1],[2]], etc]]]\n","      formatted_csv_group = self.rgb_date_time(csv_groups,self.data[1])\n","      # Get the formatted_csv_group second list of values in each group\n","      dist1_list = [dist1[1] for dist1 in formatted_csv_group]\n","      # Summarize each body part's group with mean\n","      dist1_group_mean = self.rgb_timeseries_mean(dist1_list)\n","      # y = group mean, x = every date value, x labels = every 5th date value\n","      # Specified in rgb_date_time function\n","      ax.plot(formatted_csv_group[0][0], dist1_group_mean, label=legend_label[j-1], linewidth=4)\n","      plt.xticks(formatted_csv_group[0][0], labels=formatted_csv_group[0][2])\n","      j += 1\n","    return\n","\n","# Merge sort is the fastest for worst case scenario sorting: N log(n)\n","# Implementation is from W3 and modified for AZ with ascii ord():\n","# https://www.w3schools.com/dsa/dsa_algo_mergesort.php\n","class Graphs_sort:\n","  # Initialize the input variables\n","  def __init__(self, data):\n","    self.data = data\n","  # The functions merge, c_replace, filter_stop are daisy chained\n","  # and called in sort_ascii with filter_stop().\n","  # secondary todo: unchain them.\n","\n","  # Returns the same inputs with ord value of the first letter unless the second\n","  # letter of the comparison words are different, then they swap places.\n","  # The secondary todos would make good computer science class questions.\n","  def ord_sort(self,left_activity,right_activity,left_id,right_id,left_dur,right_dur):\n","    # Return the same values as input along with ASCII of the two input\n","    # word's first letter if no swapping is required.\n","    left_swap = ord(left_activity[0])\n","    right_swap = ord(right_activity[0])\n","    left_activity_swap = left_activity\n","    right_activity_swap = right_activity\n","    left_id_swap = left_id\n","    right_id_swap = right_id\n","    left_dur_swap = left_dur\n","    right_dur_swap = right_dur\n","    # Length of the shorter of the two comparison words.\n","    if len(left_activity) > len(right_activity):\n","      length = len(right_activity)\n","    else:\n","      length = len(left_activity)\n","    # Loop over 'length' of the shorter word.\n","    for i in range(length):\n","      # Test each of the comparison words letter's ASCII number\n","      left_ord = ord(left_activity[i])\n","      right_ord = ord(right_activity[i])\n","      # If the two words are the same, do nothing.\n","      if left_activity == right_activity:\n","        break\n","      # If the two comparison word's ASCII numbers are the same, check the next.\n","      elif left_ord == right_ord:\n","        continue\n","      # If the left ASCII is larger than the right ASCII, swap the values.\n","      elif left_ord > right_ord:\n","        left_swap = right_ord\n","        right_swap = left_ord\n","        left_activity_swap = right_activity\n","        right_activity_swap = left_activity\n","        left_id_swap = right_id\n","        right_id_swap = left_id\n","        left_dur_swap = right_dur\n","        right_dur_swap = left_dur\n","        break\n","      # The other case is if the right is larger than the left, do nothing.\n","      else:\n","        break\n","    return [left_swap,right_swap,left_activity_swap,right_activity_swap,left_id_swap,right_id_swap,left_dur_swap,right_dur_swap]\n","\n","  # Merge calls ord_sum to calculate the ASCII of the string.\n","  # secondary todo: refractor to accept additional columns\n","  def merge(self,left_in,right_in):\n","      result = []\n","      result_activity = []\n","      result_id = []\n","      result_dur = []\n","      i = j = 0\n","      while i < len(left_in[1]) and j < len(right_in[1]):\n","        left = left_in[0][i]\n","        right = right_in[0][j]\n","        left_activity = left_in[1][i]\n","        right_activity = right_in[1][j]\n","        left_id = left_in[2][i]\n","        right_id = right_in[2][j]\n","        left_dur = left_in[3][i]\n","        right_dur = right_in[3][j]\n","\n","        left_right = self.ord_sort(left_activity,right_activity,left_id,right_id,left_dur,right_dur)\n","\n","        # [yes,left_activity_swap,right_activity_swap,left_id_swap,right_id_swap,left_dur_swap,right_dur_swap]\n","\n","        if left < right:\n","          result.append(left)\n","          result_activity.append(left_activity)\n","          result_id.append(left_id)\n","          result_dur.append(left_dur)\n","          i += 1\n","        else:\n","          result.append(right)\n","          result_activity.append(right_activity)\n","          result_id.append(right_id)\n","          result_dur.append(right_dur)\n","          j += 1\n","      result.extend(left_in[0][i:])\n","      result.extend(right_in[0][j:])\n","      result_activity.extend(left_in[1][i:])\n","      result_activity.extend(right_in[1][j:])\n","      result_id.extend(left_in[2][i:])\n","      result_id.extend(right_in[2][j:])\n","      result_dur.extend(left_in[3][i:])\n","      result_dur.extend(right_in[3][j:])\n","\n","      return [result,result_activity,result_id,result_dur]\n","\n","  def c_replace(self, string, word, sub_word):\n","    # Input string and replace the word with the sub_word. Similar to Python's:\n","    # string.replace(word,subword)\n","    str_replace = \"\"\n","    word_len = len(word)\n","    str_len = len(string)\n","    count = 0\n","    for i in range(str_len-word_len+1):\n","      if string[count:word_len] == word:\n","        str_replace += sub_word\n","        count += len(word)\n","        word_len += len(word)\n","      else:\n","        str_replace += string[count]\n","        count += 1\n","        word_len += 1\n","    return str_replace\n","\n","  def filter_stop(self,column):\n","    # Filters the verb endings using c_replace(). Similar to the previous one liner:\n","    # activity_arr = [filter_word if x == filter_word + 'ing' or x == filter_word + 'ed' else x for x in self.data[6]]\n","    filtered_column = []\n","    for i in column:\n","      if 'Walked' in i:\n","        filtered_column.append(\"Walk\")\n","      elif 'Juggling' in i:\n","        filtered_column.append(\"Juggle\")\n","      elif 'Driving' in i:\n","        filtered_column.append(\"Drive\")\n","      elif i == 'Lifts':\n","        # Could append since this is hard coded but I wanted to test.\n","        verb_less = self.c_replace(i, \"s\", \"\")\n","        filtered_column.append(verb_less)\n","      elif 'ing' in i:\n","        verb_less = self.c_replace(i, \"ing\", \"\")\n","        filtered_column.append(verb_less)\n","      else:\n","        filtered_column.append(i)\n","    return filtered_column\n","\n","  # Calculates duration using end - start.\n","  def sort_time(self,activity,start,end):\n","    duration = ['Duration']\n","    for i in range(1,len(start)):\n","      # Checks to see if the Activity or Start column is empty.\n","      if len(activity[i]) == 0 or len(start[i]) == 0:\n","        continue\n","      # Estimates sleep at 7 hours.\n","      elif 'Sleep' == activity[i]:\n","        duration.append(str(7*60))\n","      else:\n","        # Gets the hour.\n","        if len(end[i]) > 3:\n","          end_sub = end[i][:2]\n","        else:\n","          end_sub = end[i][0]\n","        if len(start[i]) > 3:\n","          start_sub = start[i][:2]\n","        else:\n","          start_sub = start[i][0]\n","        # Subtracts 40 minutes since there are 60 in an hour not 100.\n","        if start_sub == end_sub:\n","          duration.append(str(int(end[i]) - int(start[i])))\n","        else:\n","          hunid = (int(end_sub) - int(start_sub)) * 40\n","          duration.append(str((int(end[i]) - int(start[i])) - hunid))\n","    return duration\n","\n","  # def sort_ascii(self,time_ID,ord_list,activity_filter,duration):\n","  def sort_ascii(self,activity_filter,time_ID,duration):\n","    length = len(time_ID) - 1\n","    step = 1\n","    ord_list = [0] + [x for x in range(len(activity_filter)-1)]\n","    while step < length:\n","      for i in range(1, length, 2 * step):\n","        # Time vs space trade off: if you want less space calculate the duration\n","        # with another loop before sorting. Otherwise, the End and Start columns\n","        # are included in sorting and space is linear * number of columns (4).\n","        left = [ord_list[i:i + step],activity_filter[i:i + step], time_ID[i:i + step], duration[i:i + step]]\n","        right = [ord_list[i + step:i + 2 * step],activity_filter[i + step:i + 2 * step], time_ID[i + step:i + 2 * step], duration[i + step:i + 2 * step]]\n","        merged = self.merge(left, right)\n","        # Place the merged array back into the original array\n","        for j in range(len(merged[0])):\n","          ord_list[i + j] = merged[0][j]\n","          activity_filter[i + j] = merged[1][j]\n","          time_ID[i + j] = merged[2][j]\n","          duration[i + j] = merged[3][j]\n","      step *= 2  # Double the sub-array length for the next iteration\n","    return [time_ID,ord_list,activity_filter,duration]\n","\n","  # Returns the time_id and unique activity lists\n","  def sort_unique_words(self,activity_col):\n","    # A0_length is 0-225\n","    activity_unique = []\n","    # Unique words in Activity\n","    for i in range(len(activity_col)):\n","      if activity_col[i] not in activity_unique:\n","        if len(activity_col[i]) == 0:\n","          continue\n","        else:\n","          activity_unique.append(activity_col[i])\n","    return activity_unique\n","\n","  def sort_unique_bin(self,activity_unique,sort_ascii):\n","    # todo bin each value with the output from rgb_unique_words\n","    return\n","\n","##############################################################################\n","# Part Z: Run the functions                                                  #\n","##############################################################################\n","\n","# Part A: The path of the CSV to be parsed\n","def CSV_running(path,unflipped_col):\n","  # Create the CSV_Parser class object and open the files\n","  parser = CSV_Parser(path)\n","  read = parser.file_opener()\n","  # Index the comma position from the CSV and split the characters into their values\n","  comma_indexed = parser.comma_index(read, path, 0)\n","  # Get the width of columns of the commas\n","  comma_width = parser.comma_index(read, path, 1)\n","  # Sort the list into verticle columns\n","  # The P0 csv gets flipped, except for the Stm column\n","  # Divide by two - the list of comma places is doubled for the start/end value\n","  col_width = int(((comma_width - 1 ) / 2) - 1)\n","  vert = []\n","  for i in range(0,comma_width-1,2):\n","    value_list = parser.csv_value_list(comma_indexed, read, col_width, i)\n","    if unflipped_col == 0:\n","      vert.append(value_list)\n","    else:\n","      if value_list[0] in unflipped_col:\n","        vert.append(value_list)\n","      else:\n","        flip = parser.csv_flipper(value_list, col_width)\n","        vert.append(flip)\n","  return vert\n","\n","# One month of May, 2024 observations\n","P0_path = \"/content/P0.csv\"\n","B0_path = \"/content/B0.csv\"\n","# A0 is a TSV because there are blank cells\n","A0_path = \"/content/A0.tsv\"\n","P0_unflipped_col = ['ID','Date','Day','Stm']\n","# P0_vert = CSV_running(P0_path,P0_unflipped_col)\n","# B0_vert = CSV_running(B0_path,0)\n","A0_vert = CSV_running(A0_path,0)\n","# Four months of July-October observations\n","# P1.csv contains the pain scale and B1.csv contains the food records\n","# P1_path = \"/content/P1-Observations-PaperFigures.csv\"\n","# B1_path = \"/content/B1.csv\"\n","# A1 is a tsv because of blank cells\n","A1_path = \"/content/A1.tsv\"\n","# List of columns to not be flipepd\n","# P1_unflipped_col = ['ID','Date','Day','Stm','Notes','Notes2']\n","# P1_vert = CSV_running(P1_path,P1_unflipped_col)\n","# B1_vert = CSV_running(B1_path,0)\n","# A1_vert = CSV_running(A1_path,0)\n","\n","# Part B: Get descriptive statistics\n","def stats_def(P1_vert,B1_vert):\n","  stats_class = Statistics()\n","  # The first three columns are skipped because they are ID, Date, and Day\n","  # These two loops calculate the means and moments\n","  P1_means_list = []\n","  P1_stnd_list = []\n","  B1_means_list = []\n","  B1_stnd_list = []\n","  # secondary todo: might make these functions\n","  for l in P1_vert[3:]:\n","    P1_means = stats_class.mu(l)\n","    P1_means_list.append(P1_means)\n","    P1_mnt2_4 = stats_class.mnt(P1_means[0],P1_means[1],l)\n","    P1_stnd_list.append(P1_mnt2_4[1])\n","  for m in B1_vert[2:]:\n","    B1_means = stats_class.mu(m)\n","    B1_means_list.append(B1_means)\n","    B1_mnt2_4 = stats_class.mnt(B1_means[0],B1_means[1],m)\n","    B1_stnd_list.append(B1_mnt2_4[1])\n","  # The nested loops calculates the covariance and correlations between B0 and P0\n","  for n in range(len(P1_vert[3:])):\n","    print(\"x: \", P1_vert[n+3][0])\n","    for o in range(len(B1_vert[2:])):\n","      print(\"    and \", B1_vert[o+2][0])\n","      P1B1_covar = stats_class.covar(P1_means_list[n],B1_means_list[o],P1_vert[n+3],B1_vert[o+2])\n","      P1B1_cor = stats_class.cor(P1B1_covar,P1_stnd_list[n],B1_stnd_list[o])\n","      print(P1B1_cor)\n","    print()\n","\n","# Part C: Data visualization ASCII\n","def P1_ASCII_graph(P1_vert):\n","  title_full = ['Stamina',\n","                'Feet','Ankle','Calves',\n","                'Knees','Quadriceps','Gluteus','Groin',\n","                'Abdominals','Lower Back',\n","                'Latissimus Dorsi','Trapezius','Shoulders',\n","                'Chest','Triceps','Biceps',\n","                'Neck','Head']\n","  graph_count = 3\n","  for p in P1_vert[3:]:\n","    #if graph_count == 4:\n","    #  break\n","    # Initialize graph class\n","    P1_graph = Graph(P1_vert, 1, graph_count)\n","    # ASCII Graphs\n","    # date_col_num = 1 # data_col_num = each successive column\n","    # this would be a loop over columns 3-20, 1st column is the date\n","    # print(p[0])\n","    # print()\n","    P1_hi_lo = P1_graph.hi_lo(graph_count)\n","    date_hi_lo = P1_graph.hi_lo(1)\n","    P1_binned = P1_graph.binned(P1_hi_lo)\n","    P1_time_series = P1_graph.time_series(date_hi_lo,P1_binned)\n","    # P1_graph.time_series_print(P1_time_series[0],P1_time_series[1])\n","    # P1_file_out = \"/content/P1_\" + p[0] + \".txt\"\n","    # P1_time_series_write = P1_graph.time_series_write(p[0],P1_file_out,P1_time_series[0],P1_time_series[1])\n","    # print(\"\\n\")\n","    graph_count += 1\n","\n","def B1_ASCII_graph(B1_vert):\n","  # Did not finish\n","  graph_count = 2\n","  for p in B1_vert[2:]:\n","    #if graph_count == 4:\n","    #  break\n","    # Initialize graph class\n","    B1_graph = Graph(B1_vert, 1, graph_count)\n","    # ASCII Graphs\n","    # date_col_num = 1 # data_col_num = each successive column\n","    # this would be a loop over columns 3-20, 1st column is the date\n","    # print(p[0])\n","    # print()\n","    B1_hi_lo = B1_graph.hi_lo(graph_count)\n","    date_hi_lo = B1_graph.hi_lo(1)\n","    B1_binned = B1_graph.binned(B1_hi_lo)\n","    B1_time_series = B1_graph.time_series(date_hi_lo,B1_binned)\n","    # B0_graph.time_series_print(B0_time_series[0],B0_time_series[1])\n","    # print()\n","    # B0_file_out = \"/content/B0_\" + p[0] + \".txt\"\n","    # B0_time_series_write = B0_graph.time_series_write(p[0],B0_file_out,B0_time_series[0],B0_time_series[1])\n","    # print(\"\\n\")\n","    graph_count += 1\n","\n","# Part D: Data visualization RGB\n","def P1_RGB_graph(P1_vert):\n","\n","  title_full = ['Stamina',\n","                'Feet','Ankle','Calves',\n","                'Knees','Quadriceps','Gluteus','Groin',\n","                'Abdominals','Lower Back',\n","                'Latissimus Dorsi','Trapezius','Shoulders',\n","                'Chest','Triceps','Biceps',\n","                'Neck','Head']\n","  P1_rgb = Graphs_rgb(P1_vert)\n","  P1_B1 = 0\n","  start_val = 3\n","  # Draws the bar charts\n","  P1_rgb_bar = P1_rgb.rgb_timeseries_bar(title_full,start_val,P1_B1)\n","  # RGB Line Graphs by Group\n","  # Uses the position of each body part name in the title_full list\n","  P1_groups_num = [3,4,7,11,13,16,19,21]\n","  P1_title_label = ['Stamina','Lower Legs','Upper Legs','Core','Upper Back','Arms','Head']\n","  # P1_rgb_line = P1_rgb.rgb_timeseries_line(title_full,start_val,P1_groups_num,P1_title_label,P1_B1)\n","\n","  # Line graphs by upper/lower body group means\n","  def small():\n","    csv_groups_list = [[3,4],[4,7,11,13],[13,16,19,21]]\n","    legend_label = [['Stamina'],['Lower Legs','Upper Legs','Core'],['Upper Back','Arms','Head']]\n","    k0 = 0\n","    for csv_groups_num in csv_groups_list:\n","      fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(15, 5))\n","      P1_rgb_line_smallest = P1_rgb.rgb_timeseries_small(csv_groups_num,legend_label[k0],ax)\n","      # Plot formatting\n","      plt.margins()\n","      plt.grid()\n","      plt.yticks(range(1,6))\n","      if sum(csv_groups_num) == sum(csv_groups_list[1]):\n","        ax.legend()\n","        plt.title(\"Lower Body\")\n","        plt.ylabel(\"Pain\")\n","        #plt.savefig(\"Lower Body Pain.jpg\")\n","      elif sum(csv_groups_num) == sum(csv_groups_list[2]):\n","        ax.legend()\n","        plt.title(\"Upper Body\")\n","        plt.ylabel(\"Pain\")\n","        #plt.savefig(\"Upper Body Pain.jpg\")\n","      else:\n","        plt.ylabel(\"Stamina\")\n","        #plt.savefig(\"Stamina.jpg\")\n","      k0 += 1\n","\n","  # Smallest on one graph\n","  def smallest():\n","    # csv_groups_list = [[3,4],[4,12],[13,21]]\n","    # legend_label = [['Stamina'],['Lower Body'], ['Upper Body']]\n","    csv_groups_list = [[4,21]]\n","    legend_label = [['Pain']]\n","    fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(15, 5))\n","    k1 = 0\n","    for csv_groups_num in csv_groups_list:\n","      P1_rgb_line_smallest = P1_rgb.rgb_timeseries_small(csv_groups_num,legend_label[k1],ax)\n","      k1 += 1\n","    # Plot formatting\n","    plt.margins()\n","    plt.grid()\n","    # plt.legend()\n","    plt.yticks(range(1,6))\n","    plt.ylabel(\"Pain\")\n","    plt.savefig('P1_smallerest.jpg')\n","  # small()\n","  # smallest()\n","\n","def B1_RGB_graph(B1_vert):\n","  title_full = ['Calories','Exercise',            # Group 0\n","              'Salt', 'Fat', 'Protein',           # Group 1\n","              'Carbohydrates', 'Alcohol Servings' # Group 3\n","              ]                                   # etc\n","  B1_rgb = Graphs_rgb(B1_vert)\n","  P1_B1 = 1\n","  start_val = 2\n","  # Part D RGB Graphs: B1.csv\n","  B1_rgb_bar = B1_rgb.rgb_timeseries_bar(title_full,start_val,P1_B1)\n","  # RGB Line Graphs by Group for B0.csv\n","  # Uses the position of each title in the title_full list\n","  B1_groups_num = [2,3,4,8,9]\n","  B1_title_label = ['Calories','Exercise','Nutrients','Alcohol Servings']\n","  # Line graph is not appropriate for calories, exercise, and alcohol servings\n","  #B1_rgb_line = B1_rgb.rgb_timeseries_line(title_full,start_val,B1_groups_num,B1_title_label,P1_B1)\n","\n","# A0_vert\n","def A0_RGB_graph(A0_vert):\n","  title_full = ['','',            # Group 0\n","              '', '',           # Group 1\n","              '', '','' # Group 3\n","              ]                                   # etc\n","  A0_sort = Graphs_sort(A0_vert)\n","  P1_B1 = 2\n","  start_val = 1\n","  # Part D RGB Graphs: A0.tsv\n","  A0_sort_duration = A0_sort.sort_time(A0_sort.data[6],A0_sort.data[4],A0_sort.data[5])\n","  A0_activity_filter = A0_sort.filter_stop(A0_sort.data[6])\n","\n","  # ord_list_0 = [0] + [ord(A0_activity_filter[x][0]) for x in range(1,len(A0_activity_filter))]\n","\n","  # sort_ascii =     self,time_ID,activity_filter,duration\n","  A0_sort_sorted = A0_sort.sort_ascii(A0_activity_filter,A0_sort.data[1],A0_sort_duration)\n","\n","  for i in range(len(A0_sort_sorted[0])):\n","    print(A0_sort_sorted[0][i],A0_sort_sorted[1][i],A0_sort_sorted[2][i],A0_sort_sorted[3][i])\n","\n","  #A0_sort_unique = A0_sort.sort_unique_words(A0_sort_sorted[1])\n","  #A0_sort_bin = A0_sort.sort_unique_bin(A0_sort_unique,A0_sort_sorted)\n","\n","  #A0_rgb_time = A0_rgb.rgb_time()\n","  #A0_rgb_duration = A0_rgb.rgb_duration(A0_rgb_unique_words,A0_rgb_time)\n","  # A0_rgb.rgb_date_list()\n","  # RGB Line Graphs by Group for B0.csv\n","  # Uses the position of each title in the title_full list\n","  # B1_groups_num = [2,3,4,8,9]\n","  # B1_title_label = ['Calories','Exercise','Nutrients','Alcohol Servings']\n","  # Line graph is not appropriate for calories, exercise, and alcohol servings\n","  # B1_rgb_line = B1_rgb.rgb_timeseries_line(title_full,start_val,B1_groups_num,B1_title_label,P1_B1)\n","\n","# A0_vert\n","def A1_RGB_graph(A0_vert):\n","  title_full = ['','',            # Group 0\n","              '', '',           # Group 1\n","              '', '','' # Group 3\n","              ]                                   # etc\n","  A1_rgb = Graphs_rgb(A1_vert)\n","  P1_B1 = 2\n","  start_val = 1\n","  # Part D RGB Graphs: A0.tsv\n","  filter_word = 'Walk'\n","  A1_rgb_unique_words = A1_rgb.rgb_unique_words(filter_word)\n","  A1_rgb_time = A1_rgb.rgb_time()\n","\n","  # A1_rgb_duration = A1_rgb.rgb_duration(A1_rgb_unique_words,A1_rgb_time)\n","  # A1_rgb.rgb_date_list()\n","  # RGB Line Graphs by Group for B0.csv\n","  # Uses the position of each title in the title_full list\n","  # B1_groups_num = [2,3,4,8,9]\n","  # B1_title_label = ['Calories','Exercise','Nutrients','Alcohol Servings']\n","  # Line graph is not appropriate for calories, exercise, and alcohol servings\n","  # B1_rgb_line = B1_rgb.rgb_timeseries_line(title_full,start_val,B1_groups_num,B1_title_label,P1_B1)\n","# P1_RGB_graph(P1_vert)\n","\n","# B1_RGB_graph(B1_vert)\n","A0_RGB_graph(A0_vert)\n","# A1_RGB_graph(A1_vert)"],"metadata":{"id":"IxtfVwAjhyaV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742402308252,"user_tz":300,"elapsed":38,"user":{"displayName":"David Leifer","userId":"06279506333224389759"}},"outputId":"075e4b8d-fd8a-4efc-8a4e-f622f5abefdb"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n","Time_ID 0 Activty Duration\n","0 0 Stretch 90\n","1 1 Dynamic warmup 15\n","2 2 Plyometrics 15\n","3 3 Walk 45\n","4 4 Guitar 150\n","5 5 Dynamic warmup 15\n","6 6 Skateboard basement 30\n","7 7 Lift 15\n","8 8 Wrench 120\n","9 9 Wrench 120\n","10 10 Walk 30\n","11 11 Guitar 60\n","12 12 RPI Firmware 90\n","13 13 Guitar 60\n","14 14 Wrench 60\n","15 15 Wrench 60\n","16 16 Rest 195\n","17 17 Stretch 15\n","18 18 Rest 105\n","19 19 Guitar 30\n","20 20 Dynamic warmup 15\n","21 21 Wrench 15\n","22 22 Skateboard basement 30\n","23 23 Run, walk 30\n","24 24 Stretch 15\n","25 25 Rest 300\n","26 26 Guitar 60\n","27 27 Stretch 60\n","28 28 Walk 60\n","29 29 Rest 285\n","30 30 Juggle 25\n","31 31 Stretch 20\n","32 32 Core 15\n","33 33 Walk 30\n","34 34 Lotion 120\n","35 35 Animation 240\n","36 36 Guitar 45\n","37 37 Plyometrics 45\n","38 38 Run 30\n","39 39 Rest 300\n","40 40 Guitar 60\n","41 41 Read 270\n","42 42 Stretch 15\n","43 43 Guitar 30\n","44 44 Read 60\n","45 45 Skateboard outside 60\n","46 46 Walk 30\n","47 47 Mobility 15\n","48 48 Animation 120\n","49 49 Guitar 40\n","50 50 Stretch 10\n","51 51 Dynamic warmup 10\n","52 52 Plyometrics 30\n","53 53 Upperbody 20\n","54 54 Run, walk 50\n","55 55 Rest 704\n","56 56 Rest 45\n","57 57 Stretch 15\n","58 58 Rest 90\n","59 59 Guitar 45\n","60 60 Skateboard basement 315\n","61 61 Mobility 30\n","62 62 Mobility 15\n","63 63 Mobility 15\n","64 64 Read 210\n","65 65 Guitar 60\n","66 66 Walk 30\n","67 67 Rugby 90\n","68 68 Guitar 60\n","69 69 Stretch 30\n","70 70 Mobility 30\n","71 71 Guitar 90\n","72 72 Walk 30\n","73 73 Read 60\n","74 74 Mobility 15\n","75 75 Walk 30\n","76 76 Guitar 30\n","77 77 Walk 30\n","78 78 Mobility 45\n","79 79 Skateboard outside 90\n","80 80 Rest 195\n","81 81 Read 120\n","82 82 Ollie Notes 60\n","83 83 Guitar 30\n","84 84 Guitar 30\n","85 85 Read 90\n","86 86 Read 90\n","87 87 Skateboard basement 30\n","88 88 Dynamic warmup 15\n","89 89 Mobility 45\n","90 90 Cool down 35\n","91 91 Shower 15\n","92 92 Eat 70\n","93 93 Rest 210\n","94 94 Read 240\n","95 95 Rest 630\n","96 96 Read 240\n","97 97 Drive 60\n","98 98 Mobility 60\n","99 99 Mobility 60\n","100 100 Read 180\n","101 101 Cleaned room 120\n","102 102 Mobility 5\n","103 103 Mobility 60\n","104 104 Guitar 120\n","105 105 Mobility 30\n","106 106 Walk 30\n","107 107 Skateboard basement 30\n","108 108 Skateboard outside 15\n","109 109 Skateboard outside 15\n","110 110 Eat 45\n","111 111 Read 30\n","112 112 Guitar 30\n","113 113 Guitar 15\n","114 114 Walk 60\n","115 115 Cook 55\n","116 116 Eat 0 15\n","117 117 Coffee 15\n","118 118 Guitar 0 50\n","119 119 Relax 15\n","120 120 Pre exercise test 40\n","121 121 Plyometrics 0 60\n","122 122 Mobility 0.0 5\n","123 123 Mobility 1.0 10\n","124 124 Mobility 2.0 15\n","125 125 Mobility 3.0 10\n","126 126 Rest 0 120\n","127 127 Walk 45\n","128 128 Eat 1 30\n","129 129 Read 60\n","130 130 Tests 30\n","131 131 Guitar 2, Rest 1 210\n","132 132 Walk 30\n","133 133 Sleep 420\n","134 134 Coffee 30\n","135 135 Guitar 30\n","136 136 Relax 15\n","137 137 Pre exercise test 15\n","138 138 Juggle 15\n","139 139 Skateboard basement 60\n","140 140 Cook 90\n","141 141 Rest 0 180\n","142 142 Read 210\n","143 143 Eat 60\n","144 144 Tests 15\n","145 145 Guitar 2, Rest 1 30\n","146 146 Sleep 420\n","147 147 Coffee 15\n","148 148 Eat 15\n","149 149 Mobility 0.1 30\n","150 150 Relax 30\n","151 151 Longboard 45\n","152 152 Rest 0 150\n","153 153 Mobility 3.1 15\n","154 154 Sleep 420\n","155 155 Guitar 45\n","156 156 Eat 60\n","157 157 Guitar 2, Rest 1 60\n","158 158 Sleep 420\n","159 159 Eat 5\n","160 160 Coffee 30\n","161 161 Guitar 60\n","162 162 Computer 60\n","163 163 Eat 15\n","164 164 Relax 5\n","165 165 Mobility 15\n","166 166 Lift 15\n","167 167 Longboard 0 45\n","168 168 Cook 45\n","169 169 Eat 30\n","170 170 Eat 30\n","171 171 Eat 30\n","172 172 Guitar 2, Rest 1 30\n","173 173 Sleep 420\n","174 174 Eat 60\n","175 175 Coffee 30\n","176 176 Guitar 105\n","177 177 Relax 15\n","178 178 Pre exercise test 15\n","179 179 Plyometrics 1 15\n","180 180 Lift 15\n","181 181 Eat 75\n","182 182 Rest 0 270\n","183 183 Eat 60\n","184 184 Guitar 2, Rest 1 240\n","185 185 Sleep 420\n","186 186 Eat 60\n","187 187 Coffee 90\n","188 188 Guitar 30\n","189 189 Pre exercise test 15\n","190 190 Skateboard outside 60\n","191 191 Eat 90\n","192 192 Juggle 15\n","193 193 Rest 0 165\n","194 194 Read 60\n","195 195 Eat 60\n","196 196 Guitar 2, Rest 1 240\n","197 197 Sleep 420\n","198 198 Eat 5\n","199 199 Coffee 25\n","200 200 Read 180\n","201 201 Eat 30\n","202 202 Guitar 0 60\n","203 203 Eat 30\n","204 204 Rest 0 135\n","205 205 Juggle 60\n","206 206 Rest 1 75\n","207 207 Guitar 1 30\n","208 208 Eat 30\n","209 209 Rest 2 240\n","210 210 Sleep 420\n","211 211 Juggle 30\n","212 212 Skateboard basement 120\n","213 213 Juggle 60\n","214 214 Guitar 1 75\n","215 215 Juggle 60\n","216 216 Longboard 30\n","217 217 Guitar 1 30\n","218 218 Run 60\n","219 219 Guitar 0 30\n","220 220 Plyometrics 120\n","221 221 Longboard 75\n","222 222 Guitar 1 30\n","223 223 Guitar 0 30\n","224 224 Plyometrics 120\n","225 225 Juggle 60\n","226 226 Guitar 1 90\n"]}]},{"cell_type":"code","source":["a = [1, 2, 3, 4, 5]\n","print(a[1:])"],"metadata":{"id":"6pQFdEQFs7F9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742323682749,"user_tz":300,"elapsed":22,"user":{"displayName":"David Leifer","userId":"06279506333224389759"}},"outputId":"39fa24f8-527c-4784-e97f-9b31611806f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2, 3, 4, 5]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"paYvybT55Xyv"},"execution_count":null,"outputs":[]}]}
