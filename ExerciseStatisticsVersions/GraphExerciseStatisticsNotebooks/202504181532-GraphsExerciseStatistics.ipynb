{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1fHIWbrpk4NMYcse3Xdkb-S8XiS2pxzJm","timestamp":1721260308720},{"file_id":"1STKhtVVaknUZiohwWLdfyoZ3zg6Veimx","timestamp":1721260179755}],"authorship_tag":"ABX9TyMwR/GL4luah2DHca60asry"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["def todo(self):\n","  # List of primary issues\n","  # todo    description                                   hours   progress      Note\n","  #\n","  # todo0   A01.csv skate, long, downhill, juggling,      66      Processing\n","  #         running mean duration by category.\n","  #\n","  # todo1   A01.csv category by day of the week or        .25     DNF.          Hours are spread throughout the day.\n","  #         time of day i.e. morning, afternoon, night\n","  #         or blocks of 3.\n","  #\n","  # todo2   A01.csv nltk the 'Explanation' and 'Notes'                          Word frequency might be useful to find specific muscles.\n","  #         sections? Manual descriptions are already                           Topic analysis is included in 'Activity'. Sentiment analysis is\n","  #         included in the write-up.                                           redundant since 'Notes' is informational and not autobiographical.\n","  #\n","  # todo3   Another tutorial chapter on merge sort.\n","  #         Compare with Python's built-in len(),\n","  #         sort(), and replace().\n","  #\n","  # todo4   The graphing part could be included in        .25     DNF.          This is a good project to learn syntax and documentation since it's visual.\n","  #         Chapter 1 with pandas and SciPy.\n","  #\n","  # todo5   A01.csv longboard and running distance.       .5      DNF.          Running occured around 5 times and longboarding was recorded with time.\n","  #\n","  #\n","  # todo6   P0P1B0B1.csv timeseries graphing.             45.5    Completed\n","  #\n","  # todo7   P1.csv manual vs observed prediction\n","  #         accuracy F1 or RMSE.\n","  #\n","  # todo8   B01.csv Pearson-Correlation and day-          2       Completed     Found that there was not correlation between parametric variables.\n","  #         delayed between calories, alcohol, exercise.                        An index similar to ENSO is redundant since there were no consecutive\n","  #                                                                             observations over 4 alcohol or excessive (calorie - calorie burned).\n","  #\n","  # todo9   A01P01B01 moving window spearman correlation                        Would have to sort these for rank, which was completed in todo0.\n","  #         between activity, duration, time of day, pain,\n","  #         nutrients, calories, alcohol.\n","  #\n","  # todo10  tbd data filling and automatic predictions.\n","  #         idk if thats another chapter or avoided.\n","  #\n","  # todo11  Manual weather observations and PRISM data\n","  #         will be in a different GitHub to avoid confusion.\n","  #\n","  # Time spent at a computer programming\n","  # Total estimate  :\n","  # Total actual    :\n","  #\n","  # Purpose\n","  # The goal of writing this is to waste as much time as possible in between\n","  # skateboarding, lifting, or exercise to avoid overtraining while retaining\n","  # logical thought process during long stretches of unemployment. These were\n","  # written on a computer with a 1.5-2 hour battery to restrict excessive\n","  # programming by limiting hardware access.\n","  #\n","  # Abstract\n","  # No library Python with C-like syntax is used for data manipulation and\n","  # graphing whereby arrays are handled without dictionaries. The only\n","  # library used is Matplotlib for RGB graphing and to avoid writing a image or\n","  # video format that would likely spread misinformation. An implementation of\n","  # the merge sort algorithm was used to alphabetize exercise activity for\n","  # binning and graphing frequency by unique type. The built-in Python methods\n","  # for 'replace', 'split', 'len', and 'sort' were manually written for\n","  # learning purposes.\n","\n","  # Start date: 20250125\n","  # End date:\n","\n","  # Below is an exhaustive list of secondary issues.\n","\n","  # List of secondary issues\n","  # todo   description                                                  progress\n","  # todo0  rewrite parser for unicode csv str/int.\n","  # todo1  Stats class avoid NA, NAAN, -9999, etc.\n","  # todo2  refractor RGB_graphs.\n","  # todo3  monthly means on bar graphs.\n","  # todo4  organize merge_sort into another classe.                     Complete\n","  # todo5  modify merge sort to accept entire CSV.                      Class\n","  # todo6  Handle multi word activity descriptions consistently.        Class\n","  # todo7  switch the second capital letter to lower case if exists.    Class\n","  # todo8  unchain the four merge sort functions.                       Class\n","\n","  return\n","\n","# import sys\n","# for path in sys.path:\n","#   print(path)\n","import matplotlib.pyplot as plt\n","import exercise_module as eu\n","# eu.test_function()\n","# print(sys.version)\n","# 3.10.12 (main, Mar 22 2024, 16:50:05) [GCC 11.4.0]\n","# 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n","\n","# In development\n","###############################################\n","# Part D: Data visualization with a RGB graph #\n","###############################################\n","# Matplotlib for color because otherwise you would have\n","# to write hardware code to avoid using Python or C libraries.\n","class Graphs_rgb:\n","  # Initialize the input variables\n","  def __init__(self, data):\n","    self.data = data\n","\n","  # Four utility functions daisy chained to rgb_timeseries_bar()\n","  # Minor todo: unchain them lol\n","  def rgb_timeseries_mean(self,formatted_data_group):\n","    # Input is list (1-4) of lists (95) of each columns values without NA\n","    # i.e. [[dist1],[dist1],[dist1], etc]\n","    date_col_len = len(formatted_data_group[0])\n","    group_mean = []\n","    # Length of the column (95 without \"NA\" as filtered in rgb_date_time)\n","    for i in range(date_col_len):\n","      row_list = []\n","      # Length of columns to be summarized (1-4) 95 row_list values\n","      for j in range(len(formatted_data_group)):\n","        row_list.append(formatted_data_group[j][i])\n","      # Mean at each day for each group\n","      row_count = len(row_list)\n","      row_sum = sum(row_list)\n","      row_mean = row_sum / row_count\n","      group_mean.append(row_mean)\n","    return group_mean\n","\n","  def rgb_date_time(self,csv_groups,date_col):\n","    day_count = len(self.data[1])\n","    k = 0\n","    group_dist = []\n","    for i in csv_groups:\n","      dist0 = []\n","      dist1 = []\n","      dist2 = []\n","      for j in range(day_count):\n","        if j == (day_count-1):\n","          break\n","        if i[j+1] == \"NA\":\n","          continue\n","        else:\n","          # Formatting the date\n","          # year = 2024\n","          date_length = date_col[j+1]\n","          if len(date_length) < 4:\n","            month = date_length[:1]\n","            day = date_length[1:]\n","          else:\n","            month = date_length[:2]\n","            day = date_length[2:]\n","          date_format0 = month + \"/\" + day\n","          dist0.append(date_format0)\n","          dist1.append(int(i[j+1]))\n","          date_format1 = month + \"/\" + day\n","          if int(day) % 5 == 0:\n","            dist2.append(date_format1)\n","          else:\n","            dist2.append(\" \")\n","            continue\n","      group_dist.append([dist0,dist1,dist2])\n","      k += 1\n","    return group_dist\n","\n","  def rgb_P1_style(self,final_title,line):\n","    plt.yticks(range(1,6))\n","    if final_title == 'Stamina':\n","      plt.ylabel(final_title)\n","    else:\n","      plt.title(final_title)\n","      plt.ylabel(\"Pain\")\n","      if line == 1:\n","        plt.legend()\n","    return\n","\n","  def rgb_B1_style(self,final_title,line):\n","    if final_title == 'Calories':\n","      plt.yticks(range(1200,4500,400))\n","      plt.ylabel(\"Intake\")\n","      plt.title(final_title)\n","    elif final_title == 'Alcohol Servings':\n","      plt.yticks(range(0,16))\n","      plt.title(\"Alcohol\")\n","      plt.ylabel(\"Servings\")\n","    elif final_title == 'Exercise':\n","      plt.yticks(range(0,3))\n","      plt.title(final_title)\n","      # plt.ylabel(\"Calories Out\")\n","      plt.text(.1,.5, \"Calories Out \\n2 = 250+ \\n1 = 1-249\",\n","         bbox={'facecolor': 'white', 'alpha': .75, 'pad': 10})\n","    else:\n","      plt.yticks(range(1,6))\n","      if line == 1:\n","        plt.title(\"Nutrients\")\n","      else:\n","        plt.title(final_title)\n","      plt.ylabel(\"Intake\")\n","      plt.legend()\n","    return\n","\n","  # Bar plots for each column\n","  def rgb_timeseries_bar(self,title_full,start_val,P1_B1):\n","    for i in range(start_val,len(title_full)+start_val):\n","      formatted_csv_group = self.rgb_date_time([self.data[i]],self.data[1])\n","      fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(15, 5))\n","      ax.bar(formatted_csv_group[0][0], formatted_csv_group[0][1], width=0.8, align='edge')\n","      final_title = title_full[i-start_val]\n","      # Format the title, yticks, and ylabel\n","      if P1_B1 == 0:\n","        self.rgb_P1_style(final_title,0)\n","      elif P1_B1 == 1:\n","        self.rgb_B1_style(final_title,0)\n","      elif P1_B1 == 2:\n","        pass\n","        # self.rgb_A0_style(final_title,0)\n","      plt.xticks(formatted_csv_group[0][0], labels=formatted_csv_group[0][2])\n","      plt.margins()\n","      plt.grid()\n","      # plt.savefig(final_title + '.jpg')\n","    return\n","\n","  # Returns a date list without blanks\n","  def rgb_date_list(self):\n","    # date_literal is 0-30 days\n","    date_literal = []\n","    # Makes a list with only the dates\n","    for i in range(1,len(self.data[2])):\n","      if len(self.data[2][i]) > 0:\n","        date_literal.append(self.data[2][i])\n","    return date_literal\n","\n","  # Multiple lines same graphs.\n","  def rgb_timeseries_line(self,title_full,start_val,groups_num,title_label,P1_B1):\n","    data = self.data\n","    # secondary todo: name instead of number position\n","    j = 1\n","    # Adding multiple lines to a single plot by group with formatting\n","    for i in range(len(groups_num)):\n","      subset0 = groups_num[i:j][0]\n","      if subset0 == groups_num[-1]:\n","        break\n","      subset1 = groups_num[i+1:j+1][0]\n","      csv_groups = data[subset0:subset1]\n","      formatted_csv_group = self.rgb_date_time(csv_groups,self.data[1])\n","      # Format subplot\n","      fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(15, 5))\n","      # Get the formatted_csv_group second list of values in each group\n","      dist1_list = [dist1[1] for dist1 in formatted_csv_group]\n","      # First 3 columns in data are ID, while the title list isn't.\n","      # Subtract each subset by the start_val of the values (excluding date, id, etc)\n","      title_group = title_full[(subset0-start_val):(subset1-start_val)]\n","      # y = each dist1 in formatted_csv_group, x = every date value, x labels = every 5th date value\n","      for k in range(len(dist1_list)):\n","        ax.plot(formatted_csv_group[0][0], dist1_list[k], label=title_group[k], linewidth=4)\n","        # Format the title, yticks, and ylabel\n","        if P1_B1 == 0:\n","          self.rgb_P1_style(title_label[j-1],1)\n","        elif P1_B1 == 1:\n","          self.rgb_B1_style(title_label[j-1],1)\n","      # Chart formatting and save\n","      plt.xticks(formatted_csv_group[0][0], labels=formatted_csv_group[0][2])\n","      plt.grid()\n","      plt.margins()\n","      #plt.savefig(title_label[j-1] + '.jpg')\n","      j += 1\n","    return\n","\n","  # Summarized with mean\n","  def rgb_timeseries_small(self,csv_groups_num,legend_label,ax):\n","    csv = self.data\n","    j = 1\n","    for i in range(len(csv_groups_num)):\n","      subset0 = csv_groups_num[i:j][0]\n","      if subset0 == csv_groups_num[-1]:\n","        break\n","      subset1 = csv_groups_num[i+1:j+1][0]\n","      csv_groups = csv[subset0:subset1]\n","      # Builds an array to skip NA and format the date\n","      # [[[dist0],[1],[2]],[[dist0],[1],[2]], etc]]]\n","      formatted_csv_group = self.rgb_date_time(csv_groups,self.data[1])\n","      # Get the formatted_csv_group second list of values in each group\n","      dist1_list = [dist1[1] for dist1 in formatted_csv_group]\n","      # Summarize each body part's group with mean\n","      dist1_group_mean = self.rgb_timeseries_mean(dist1_list)\n","      # y = group mean, x = every date value, x labels = every 5th date value\n","      # Specified in rgb_date_time function\n","      ax.plot(formatted_csv_group[0][0], dist1_group_mean, label=legend_label[j-1], linewidth=4)\n","      plt.xticks(formatted_csv_group[0][0], labels=formatted_csv_group[0][2])\n","      j += 1\n","    return\n","\n","  # A01.csv frequency of merged 'Activity'.\n","  def rgb_timeseries_frequency(self):\n","    x = self.data[0]\n","    y = self.data[2]\n","\n","    fig, ax = plt.subplots(figsize=(15, 25))\n","    ax.barh(x, y, # linewidth=2\n","                                height=.5)\n","    plt.title('Activity Frequency July-October, 2024', fontsize=20)\n","    # plt.xticks(x, labels=x, rotation=90, ha='right', fontsize=10)\n","    plt.xticks(fontsize=14)\n","    ax.tick_params(axis='y', pad=50)\n","    plt.yticks(x, labels=x, #rotation=45,\n","               ha='center', fontsize=14)\n","    plt.grid()\n","    plt.margins(y=0.01)\n","    plt.savefig('A1 Activity Frequency July-October, 2024' + '.jpg')\n","    return\n","\n","  # A01.csv duration of merged 'Activity' hours.\n","  def rgb_timeseries_duration(self):\n","    x = self.data[0]\n","    y = self.data[1]\n","\n","    fig, ax = plt.subplots(figsize=(15, 25))\n","    ax.barh(x, y, # linewidth=2\n","                                height=.5)\n","    plt.title('Activity Duration July-October, 2024', fontsize=20)\n","    # plt.xticks(x, labels=x, rotation=90, ha='right', fontsize=10)\n","    plt.xticks(fontsize=14)\n","    ax.tick_params(axis='y', pad=50)\n","    plt.yticks(x, labels=x, #rotation=45,\n","               ha='center', fontsize=14)\n","    plt.grid()\n","    plt.margins(y=0.01)\n","    plt.xlabel('Hours', fontsize=14)\n","    plt.savefig('A1 Activity Duration July-October, 2024' + '.jpg')\n","    return\n","\n","  # A01.csv mean of merged 'Activity' frequency / (sum of minutes)\n","  def rgb_timeseries_mean(self):\n","    x = self.data[0]\n","    y = []\n","    for i in range(len(self.data[1])):\n","      mean = self.data[1][i] / self.data[2][i]\n","      y = y + [mean]\n","    fig, ax = plt.subplots(figsize=(15, 25))\n","    ax.barh(x, y, # linewidth=2\n","                                height=.5)\n","    plt.title('Total Activity Average\\nJuly-October, 2024', fontsize=20)\n","    # plt.xticks(x, labels=x, rotation=90, ha='right', fontsize=10)\n","    plt.xticks(fontsize=14)\n","    ax.tick_params(axis='y', pad=50)\n","    plt.yticks(x, labels=x, #rotation=45,\n","               ha='center', fontsize=14)\n","    plt.grid()\n","    plt.margins(y=0.01)\n","    plt.xlabel('Hours', fontsize=14)\n","    plt.savefig('A1 Total Activity Average\\nJuly-October, 2024' + '.jpg')\n","\n","  # A01.csv mean of merged 'Activity' frequency / (sum of minutes)\n","  def rgb_timeseries_daily_mean(self):\n","    x = self.data[0]\n","    y = []\n","    for i in range(len(self.data[1])):\n","      daily_mean = (self.data[2][i] / 100) * 60\n","      y = y + [daily_mean]\n","    fig, ax = plt.subplots(figsize=(15, 25))\n","    ax.barh(x, y, # linewidth=2\n","                                height=.5)\n","    plt.title('Daily Activity Average\\nJuly-October, 2024', fontsize=20)\n","    # plt.xticks(x, labels=x, rotation=90, ha='right', fontsize=10)\n","    plt.xticks(fontsize=14)\n","    ax.tick_params(axis='y', pad=50)\n","    plt.yticks(x, labels=x, #rotation=45,\n","               ha='center', fontsize=14)\n","    plt.grid()\n","    plt.margins(y=0.01)\n","    plt.xlabel('Minutes', fontsize=14)\n","    plt.savefig('A1 Daily Activity Average\\nJuly-October, 2024' + '.jpg')\n","\n","  ##############################################################################'\n","  # Early iterations of A0 graphs.\n","  # Graph 0) Occurrence of each activity. Includes every Activity.\n","  def graph_0_frequency(self):\n","    '''\n","    # The activities with multiple same day were inconsistently collected.\n","    x = [i[0][0] for i in A0_sort_bin[1:]] # or A0_sort_unique\n","    y = [len(k[1:]) for k in A0_sort_bin[1:]] # number of each activity\n","    fig, ax = plt.subplots(figsize=(12, 15))\n","    ax.bar(x, y, linewidth=2)\n","    plt.title('Number of Activities, May 2024')\n","    plt.xticks(x, labels=x, rotation=90, ha='right', fontsize=10)\n","    plt.grid()\n","    plt.margins()\n","    # plt.xlabel('Activity')\n","    # plt.legend()\n","    # plt.savefig('AZ Activity' + '.jpg')\n","    plt.show()\n","    '''\n","    return\n","  # Graph 1) Duration of each activity. Includes every Activity duration in hours.\n","  def graph_1_duration(self):\n","    '''\n","    # One option is to sort by the number in each bin to arrange by frequency.\n","    # sort_unique_len = []\n","    # for i in range(len(A0_sort_bin)):\n","    #   sort_unique_len.append([A0_sort_bin[i][0][0],len(A0_sort_bin[i][1:])])\n","    # sort_unique_int = sorted(sort_unique_len, key=lambda sort_sub: sort_sub[1],\n","                               # reverse=True\n","\n","    # todo use a list of lists in the sort functions (i.e. [x,y])\n","    # sort_int = merge_sort_int(A0_sort_bin)\n","    # x = [i[0] for i in sort_unique_int[1:]] # or A0_sort_unique\n","    # y = [k[1] for k in sort_unique_int[1:]]\n","\n","    x = [i[0][0] for i in A0_sort_bin[1:]] # or A0_sort_unique\n","    y = []\n","    for j in A0_sort_bin[1:]:\n","      y_labeled = []\n","      for m in j[1:]:\n","        y_labeled.append(int(m[2]))\n","      y.append(round(sum(y_labeled) / 60,4))\n","\n","    # Remove rest and read\n","    y_count = len(y)\n","    y_sum = sum(y)\n","    y_mean = round(y_sum / y_count, 4)\n","    yy = y\n","    count_2 = 0\n","    for xx in yy:\n","      num = round(xx - y_mean, 4)\n","      # Outliers greater than or less than 30\n","      if num > 30 or num < -30:\n","        # print(count_2, xx, '-', y_mean, num)\n","        x.pop(count_2)\n","        y.pop(count_2)\n","      count_2 += 1\n","\n","    fig, ax = plt.subplots(figsize=(15, 12))\n","    ax.bar(x, y, linewidth=2)\n","    plt.title('Duration of Activities, May 2024')\n","    plt.xticks(x, labels=x, rotation=90, ha='center', fontsize=10)\n","    plt.grid()\n","    plt.margins()\n","    plt.ylabel('Hours')\n","    plt.text(.1,33.5, \"Exceeded 50 hours: Rest and Sleep\",\n","          bbox={'facecolor': 'white', 'alpha': .75, 'pad': 10})\n","    # plt.legend()\n","    # plt.savefig('AZ Activity Duration' + '.jpg')\n","    plt.show()\n","    '''\n","    return\n","\n","#################################################\n","# Part E: Part D visualization helper functions #\n","#################################################\n","# Merge sort is the fastest for worst case scenario sorting: N log(n)\n","# Implementation is from W3 and modified for AZ with ascii ord():\n","# https://www.w3schools.com/dsa/dsa_algo_mergesort.php\n","class Graphs_sort:\n","  # Initialize the input variables\n","  def __init__(self, data):\n","    self.data = data\n","\n","  # Filters the verb endings using c_replace().\n","  def filter_stop(self,column):\n","    filtered_column = []\n","    for i in column:\n","      if 'Walked' in i:\n","        filtered_column.append(\"Walk\")\n","      elif 'Juggling' in i:\n","        filtered_column.append(\"Juggle\")\n","      elif 'Driving' in i:\n","        filtered_column.append(\"Drive\")\n","\n","      # english hard idk\n","      elif 'Reading' in i:\n","        filtered_column.append(\"Read\")\n","      elif 'Writing' in i:\n","        filtered_column.append(\"Write\")\n","      elif 'No juggling' in i:\n","        filtered_column.append(\"No juggling\")\n","      elif 'Running' in i:\n","        filtered_column.append(\"Run\")\n","      elif 'Hiking' in i:\n","        filtered_column.append(\"Hike\")\n","      elif 'Rested' in i:\n","        filtered_column.append(\"Rest\")\n","\n","      elif i == 'Lifts':\n","        # Could append since this is hard coded but I wanted to test.\n","        verb_less = i.replace(\"s\", \"\")\n","        # verb_less = self.c_replace(i, \"s\", \"\")\n","        filtered_column.append(verb_less)\n","      elif 'ing' in i:\n","        verb_less = i.replace(\"ing\", \"\")\n","        # verb_less = self.c_replace(i, \"ing\", \"\")\n","        filtered_column.append(verb_less)\n","      else:\n","        filtered_column.append(i)\n","    return filtered_column\n","\n","  # Calculates duration using end - start.\n","  def sort_time(self,activity,start,end):\n","    duration = ['Duration']\n","    for i in range(1,len(start)):\n","      # Checks to see if the Activity or Start column is empty.\n","      # if len(activity[i]) == 0 or len(start[i]) == 0:\n","      #  continue\n","      # Estimates sleep at 7 hours.\n","      dur = 0\n","      if 'Sleep' == activity[i]:\n","        dur = str(7*60)\n","      else:\n","        # Converts the '100' digits to '60' minutes in hours.\n","        # Gets the end hour.\n","        if len(end[i]) == 4:      # handles 1030 4 digits\n","          end_sub = end[i][:2]\n","        elif len(end[i]) == 3:    # handles 0930 3 digits\n","          end_sub = end[i][0]\n","        else:                     # handles 0030 2 digits\n","          end_sub = 0\n","        # Gets the start hour.\n","        if len(start[i]) == 4:    # handles 1030 4 digits\n","          start_sub = start[i][:2]\n","        elif len(start[i]) == 3:  # handles 0930 3 digits\n","          start_sub = start[i][0]\n","        else:                     # handles 0030 2 digits\n","          start_sub = 0\n","        # Subtracts 40 minutes since there are 60 in an hour not 100.\n","        if start_sub == end_sub:\n","          if int(end[i]) == int(start[i]):\n","            dur = str(5)\n","          else:\n","            dur = str(int(end[i]) - int(start[i]))\n","        else:\n","          # Turn over from one day to another.\n","          if int(end[i]) < int(start[i]):\n","            # First day's amount of hours (24 - the start time hour)\n","            first_day = 23 - int(start_sub)\n","            first_day_minutes = 60 - int(start[i][2:]) # the last two digits are the minutes\n","            # Second day's hours added to the first day's hours as 'dur' as minutes.\n","            if len(end[i]) == 4:\n","              second_day_minutes = end[i][2:]\n","            elif len(end[i]) == 3:\n","              second_day_minutes = end[i][1:]\n","            else: # There are no extra hours\n","              second_day_minutes = end[i]\n","            end_sub = end_sub + first_day\n","            hunid = (int(end_sub)) * 60 # hour difference converted to minutes\n","            dur = hunid + first_day_minutes + int(second_day_minutes)\n","          else:\n","            hunid = (int(end_sub) - int(start_sub)) * 40\n","            dur = str( ( int(end[i]) - int(start[i]) ) - hunid)\n","      duration.append(dur)\n","    return duration\n","\n","  # Merge calls ord_sum to calculate the ASCII of the string.\n","  # secondary todo: refractor to accept additional columns\n","  def merge(self,left_in,right_in):\n","      result = []\n","      result_activity = []\n","      result_id = []\n","      result_dur = []\n","      i = j = 0\n","      while i < len(left_in[1]) and j < len(right_in[1]):\n","        left = left_in[0][i]\n","        right = right_in[0][j]\n","        left_activity = left_in[1][i]\n","        right_activity = right_in[1][j]\n","        left_id = left_in[2][i]\n","        right_id = right_in[2][j]\n","        left_dur = left_in[3][i]\n","        right_dur = right_in[3][j]\n","        if left < right: # or (left_activity_replace == right_activity and left < right) ?\n","          result.append(left)\n","          result_activity.append(left_activity)\n","          result_id.append(left_id)\n","          result_dur.append(left_dur)\n","          i += 1\n","        elif left > right: # or left_activity == right_activity ?\n","          result.append(right)\n","          result_activity.append(right_activity)\n","          result_id.append(right_id)\n","          result_dur.append(right_dur)\n","          j += 1\n","        else:\n","          if len(left_activity) > len(right_activity):\n","            length = len(right_activity)\n","          else: # same length?\n","            length = len(left_activity)\n","          # Find where the two words are different at k.\n","          for k in range(1,length):\n","            if left_activity[k] != right_activity[k]:\n","              break\n","          # Handles when the comparison first words are the same but\n","          # one of the comparisons have a space and second word.\n","          left_ord = ord(left_activity[k])\n","          right_ord = ord(right_activity[k])\n","          if left_activity[:length] == right_activity[:length]:\n","            if length < len(right_activity):\n","              if left_activity == right_activity[:length]:\n","                left_ord = -1\n","                # Comparison right_activity[length:] is longer and different.\n","                right_ord = ord(right_activity[length:][0])\n","          if left_activity[:k] == right_activity[:k] and left_ord < right_ord:\n","            result.append(left)\n","            result_activity.append(left_activity)\n","            result_id.append(left_id)\n","            result_dur.append(left_dur)\n","            i += 1\n","          else:\n","            result.append(right)\n","            result_activity.append(right_activity)\n","            result_id.append(right_id)\n","            result_dur.append(right_dur)\n","            j += 1\n","      result.extend(left_in[0][i:])\n","      result.extend(right_in[0][j:])\n","      result_activity.extend(left_in[1][i:])\n","      result_activity.extend(right_in[1][j:])\n","      result_id.extend(left_in[2][i:])\n","      result_id.extend(right_in[2][j:])\n","      result_dur.extend(left_in[3][i:])\n","      result_dur.extend(right_in[3][j:])\n","\n","      return [result,result_activity,result_id,result_dur]\n","\n","  # Accepts a list 'ord_column' ASCII representation of the first character\n","  # for the 'activity_column', the string list 'activity_column'\n","  # to be sorted, and 'data' or the original CSV or TSV spreadsheet.\n","  def sort_ascii(self,ord_column,activity_column,data):\n","    column_length = len(data[0]) - 1\n","    data_length = len(data) - 1\n","    step = 1\n","    while step < column_length:\n","      for i in range(1, column_length, 2 * step):\n","        # for k in data:\n","        left = [ord_column[i:i + step],activity_column[i:i + step],data]\n","        right = [ord_column[i + step:i + 2 * step],activity_column[i + step:i + 2 * step],data]\n","        merged = self.merge(left, right)\n","        # Place the merged array back into the original array\n","        for j in range(len(merged[0])):\n","          ord_column[i + j] = merged[0][j]\n","      step *= 2  # Double the sub-array length for the next iteration\n","    return data\n","\n","  # Returns the time_id and unique activity lists.\n","  def sort_unique_words(self,activity_col):\n","    # A0_length is 0-225\n","    activity_unique = []\n","    # Unique words in Activity\n","    for i in range(len(activity_col)):\n","      if activity_col[i] not in activity_unique:\n","        if len(activity_col[i]) == 0:\n","          continue\n","        else:\n","          activity_unique.append(activity_col[i])\n","    return activity_unique\n","\n","  # Returns the sorted list into AZ bins.\n","  # Dimensions: 'sort_unique_words' by the number of occurances in 'sort_ascii'.\n","  def sort_unique_bin(self,sort_unique_words,sort_ascii):\n","\n","    # Once the word is different than the next word, bin the next\n","    # word (or words) since the list is already sorted.\n","\n","    # Empty 'unique_bin' is generated with int. Could use '0's but these\n","    # are 0,1,2,...n!\n","    unique_bin = [\n","        [[x],[x]] for x in range(len(sort_unique_words))\n","        ]\n","    count = 0\n","    for i in range(len(sort_ascii[0])):\n","      # Avoids checking 'sort_ascii' past the length of the list.\n","      if i == (len(sort_ascii[0])-1):\n","        break\n","      if sort_ascii[0][i] == sort_ascii[0][i+1]:\n","        # Words are the same, 'count' does not get incremented.\n","        if unique_bin[count][0][0].__class__ == str:\n","          # If first key or 'unique_bin[count]' is str, don't include it.\n","          unique_bin[count].append([sort_ascii[0][i],sort_ascii[1][i],sort_ascii[2][i]])\n","        else:\n","          unique_bin[count] = [\n","              [sort_unique_words[count]],\n","              [sort_ascii[0][i],sort_ascii[1][i],sort_ascii[2][i]]\n","              ]\n","      elif (sort_ascii[0][i-1] != sort_ascii[0][i]) and (sort_ascii[0][i] != sort_ascii[0][i+1]):\n","        # Previous word and next word are different.\n","        unique_bin[count] = [\n","            [sort_unique_words[count]],\n","            [sort_ascii[0][i],sort_ascii[1][i],sort_ascii[2][i]]\n","            ]\n","        count += 1\n","      elif (sort_ascii[0][i-1] == sort_ascii[0][i]) and (sort_ascii[0][i] != sort_ascii[0][i+1]):\n","        # Previous word is the same, next word is different.\n","        unique_bin[count].append([sort_ascii[0][i],sort_ascii[1][i],sort_ascii[2][i]])\n","        count += 1\n","      else:\n","        count += 1\n","    # Append last element of sorted list onto the bin list at end\n","    unique_bin[-1].append([sort_ascii[0][-1],\n","                           sort_ascii[1][-1],\n","                           sort_ascii[2][-1]])\n","\n","    return unique_bin\n","\n","\n","\n","\n","\n","  # todo use one list instead of several lists (also for sort_ascii())\n","  def merge_int(self, left, right):\n","      result = []\n","      result_unique = []\n","      i = j = 0\n","      while i < len(left[0]) and j < len(right[0]):\n","        print(left[0][i], right[0][j])\n","        if left[0][i] < right[0][j]:\n","            result.append(left[0][i])\n","            result_unique.append(left[1][i])\n","            i += 1\n","        else:\n","            result.append(right[0][j])\n","            result_unique.append(right[1][i])\n","            j += 1\n","      result.extend(left[0][i:])\n","      result.extend(right[0][j:])\n","      result.extend(left[1][i:])\n","      result.extend(right[1][j:])\n","      return result\n","  # tod0\n","  def merge_sort_int(self, array_int, array_str):\n","      step = 1  # Starting with sub-arrays of length 1\n","      length = len(array_int[0])\n","      while step < length:\n","        for i in range(0, length, 2 * step):\n","          left = [array_int[i:i + step][0],array_int[i:i + step][1]]\n","          right = [array_int[i + step:i + 2 * step][0], array_int[i + step:i + 2 * step][1]]\n","          merged = self.merge_int(left, right)\n","          # Place the merged array back into the original array\n","          for j in range(len(merged[0])):\n","            array_int[0][i + j] = merged[0][j]\n","            array_int[1][i + j] = merged[1][j]\n","        step *= 2  # Double the sub-array length for the next iteration\n","      return array_int\n","\n","\n","  # todo: use multiple variables\n","  # Merges entries if the first word in the string is the same. Uses C syntax.\n","  # If you're a stickler, replace 'for i in range()' with 'while iterator <= len(data)'\n","  def merge_similar_activities(self, sorted_list):\n","    # Specific formatting for this dataset.\n","    ######################################################################\n","\n","    x = [i[0][0] for i in sorted_list[1:]] # or A0_sort_unique\n","    z = [len(k[1:]) for k in sorted_list[1:]] # frequency of each activity\n","    y = []\n","    # Calculate the hours for duration.\n","    for j in sorted_list[1:]:\n","      y_label = []\n","      for m in j[1:]:\n","        y_label.append(float(m[2]))\n","      y.append(round(sum(y_label) / 60, 4))\n","    # This is a bad method to avoid binning 'Skateboard paper' into\n","    # 'Skateboard' but I want to do the graphing.\n","    for ayy in range(len(x)):\n","      if x[ayy] == 'Skateboard paper':\n","        x[ayy] = 'paper Skateboard'\n","      if x[ayy] == 'Skateboard videos':\n","        x[ayy] = 'videos Skateboard'\n","\n","    ######################################################################\n","\n","    # Combining similar Activities. The dataset uses the same word plus\n","    # a number to denote multiple of the same activities on the same day.\n","    y_mean = round(sum(y) / float(len(y)), 4)\n","    count_0 = 0\n","    count_1 = 1\n","    root_bool = False\n","\n","    # Time complexity is the number of Activities.\n","    #for count_0 in range(len(y)):\n","    x[count_0] = x[count_0]\n","    y[count_0] = y[count_0]\n","    z[count_0] = z[count_0]\n","    spliced = []\n","    while True:\n","      if count_1 == len(y):\n","        x[count_0] = x[count_0]\n","        y[count_0] = y[count_0]\n","        z[count_0] = z[count_0]\n","        break\n","      x[count_1] = x[count_1]\n","      y[count_1] = y[count_1]\n","      z[count_1] = z[count_1]\n","      x_0 = x[count_0].split(' ') # c_split\n","      x_1 = x[count_1].split(' ') # c_split\n","      # Comparison operators to find the first instance of the word.\n","      # i.e. 'Eat' followed by 'Eat 0', 'Eat 1', etc.\n","      if x_0[0] == x_1[0]:\n","        if x_1[1] != 'outside':\n","          if ' ' not in x[count_0]:\n","            #if len(x_1) < 3:\n","            if root_bool == False:\n","              root_pos = count_0\n","              root_bool = True\n","      if root_bool == True:\n","        # Skip merging 'Skateboard paper' and 'Skateboard videos'.\n","        if y[root_pos] != y[count_0]:\n","          # 'y' value at 'count_0' is cumulative\n","          y[root_pos] += y[count_0]\n","          z[root_pos] += z[count_0]\n","        # The first 'y' value at 'root_pos' is the same as the total.\n","        x[count_0] = x[root_pos]\n","        y[count_0] = y[root_pos]\n","        z[count_0] = z[root_pos]\n","      else:\n","        x[count_0] = x[count_0]\n","        y[count_0] = y[count_0]\n","        z[count_0] = z[count_0]\n","      # This checks to see if the word and next word are different or the next\n","      # word is the same and is has three or more words, 'root_bool' is False.\n","      if x_0[0] != x_1[0] or len(x_1) >= 3:\n","        if root_bool == True:\n","          extra_numbers = (count_0+1) - (root_pos+1)\n","          # 'spliced' numbers list gets three values that slice the remaining\n","          # identical words in another loop.\n","          spliced = spliced + [[(count_0-extra_numbers),count_0, extra_numbers]]\n","          # The values at 'root_pos' get set to the current value at 'count_0'.\n","          x[count_0-extra_numbers] = x[count_0]\n","          y[count_0-extra_numbers] = y[count_0]\n","          z[count_0-extra_numbers] = z[count_0]\n","          root_bool = False\n","      count_0 += 1\n","      count_1 += 1\n","\n","    ######################################################################\n","    # Reorganizing the strings (bad method)\n","    # Also inserts '\\n' breaks if the string is longer than 10 characters\n","    # and more than one word.\n","    for bay in range(len(x)):\n","      if x[bay] == 'paper Skateboard':\n","        x[bay] = 'Skateboard paper'\n","      if x[bay] == 'videos Skateboard':\n","        x[bay] = 'Skateboard videos'\n","      string_count = 0\n","      for cay in x[bay]:\n","        if string_count > 9:\n","          string_break = x[bay].split(' ')\n","          string_word_count = len(string_break)\n","          if string_word_count > 1:\n","            string_word_mid = round(string_word_count / 2)\n","            string_0_half = \" \".join(string_break[:string_word_mid]) + '\\n'\n","            string_1_half = \" \".join(string_break[string_word_mid:])\n","            x[bay] = string_0_half + string_1_half\n","          break\n","        string_count += 1\n","    return [x,y,z,spliced]\n","\n","  # Uses 'merge_similar_activities' to splice the data.\n","  def merge_activities_splice(self,merged_activiites):\n","    # Time complexities is the number of repeated words that are being merged.\n","    x = []\n","    y = []\n","    z = []\n","    spler = []\n","    for k in range(len(merged_activiites[3])+1): # or while the length is less than the\n","      # The end splice.\n","      if k == len(merged_activiites[3]):\n","        # spler = spler + [[middle_0,0]] <- checks that the numbers are correct\n","        x = x + merged_activiites[0][middle_0:]\n","        y = y + merged_activiites[1][middle_0:]\n","        z = z + merged_activiites[2][middle_0:]\n","        break\n","      # The first splice.\n","      if x == []:\n","        # spler = [[0,merged_activiites[3][k][0]+1]]\n","        x = merged_activiites[0][0:merged_activiites[3][k][0]+1]\n","        y = merged_activiites[1][0:merged_activiites[3][k][0]+1]\n","        z = merged_activiites[2][0:merged_activiites[3][k][0]+1]\n","        # This part is carried into the next splice.\n","        middle_0 = merged_activiites[3][k][0]+1 + merged_activiites[3][k][2]\n","      else: # The middle splices.\n","        # spler = spler + [[middle_0, merged_activiites[3][k][0]+1]]\n","        x = x + merged_activiites[0][middle_0:merged_activiites[3][k][0]+1]\n","        y = y + merged_activiites[1][middle_0:merged_activiites[3][k][0]+1]\n","        z = z + merged_activiites[2][middle_0:merged_activiites[3][k][0]+1]\n","        # This part is carried into the next splice.\n","        middle_0 = merged_activiites[3][k][0]+1 + merged_activiites[3][k][2]\n","    return [x,y,z]\n","\n","\n","##############################################################################\n","# Part Z: Run the functions                                                  #\n","##############################################################################\n","\n","# Part A: The path of the CSV to be parsed\n","def CSV_running(path,unflipped_col):\n","  # Create the CSV_Parser class object and open the files\n","  parser = eu.CSV_Parser(path)\n","  read = parser.file_opener()\n","  # Index the comma position from the CSV and split the characters into their values\n","  comma_indexed = parser.comma_index(read, path, 0)\n","  # Get the width of columns of the commas\n","  comma_width = parser.comma_index(read, path, 1)\n","  # Sort the list into verticle columns\n","  # The P0 csv gets flipped, except for the Stm column\n","  # Divide by two - the list of comma places is doubled for the start/end value\n","  col_width = int(((comma_width - 1 ) / 2) - 1)\n","  vert = []\n","  for i in range(0,comma_width-1,2):\n","    value_list = parser.csv_value_list(comma_indexed, read, col_width, i)\n","    if unflipped_col == 0:\n","      vert.append(value_list)\n","    else:\n","      if value_list[0] in unflipped_col:\n","        vert.append(value_list)\n","      else:\n","        flip = parser.csv_flipper(value_list, col_width)\n","        vert.append(flip)\n","  return vert\n","\n","# One month of May, 2024 observations\n","P0_path = \"/content/P0.csv\"\n","B0_path = \"/content/B0.csv\"\n","# A0 is a TSV because there are blank cells\n","A0_path = \"/content/A0.tsv\"\n","P0_unflipped_col = ['ID','Date','Day','Stm']\n","# P0_vert = CSV_running(P0_path,P0_unflipped_col)\n","# B0_vert = CSV_running(B0_path,0)\n","# A0_vert = CSV_running(A0_path,0)\n","# Four months of July-October observations\n","# P1.csv contains the pain scale and B1.csv contains the food records\n","# P1_path = \"/content/P1-Observations-PaperFigures.csv\"\n","# B1_path = \"/content/B1.csv\"\n","# A1 is a tsv because of blank cells\n","A1_path = \"/content/A1.tsv\"\n","# List of columns to not be flipepd\n","# P1_unflipped_col = ['ID','Date','Day','Stm','Notes','Notes2']\n","# P1_vert = CSV_running(P1_path,P1_unflipped_col)\n","# B1_vert = CSV_running(B1_path,0)\n","A1_vert = CSV_running(A1_path,0)\n","\n","# Part B: Get descriptive statistics\n","def stats_def(P1_vert,B1_vert):\n","  stats_class = Statistics()\n","  # The first three columns are skipped because they are ID, Date, and Day\n","  # These two loops calculate the means and moments\n","  P1_means_list = []\n","  P1_stnd_list = []\n","  B1_means_list = []\n","  B1_stnd_list = []\n","  # secondary todo: might make these functions\n","  for l in P1_vert[3:]:\n","    P1_means = stats_class.mu(l)\n","    P1_means_list.append(P1_means)\n","    P1_mnt2_4 = stats_class.mnt(P1_means[0],P1_means[1],l)\n","    P1_stnd_list.append(P1_mnt2_4[1])\n","  for m in B1_vert[2:]:\n","    B1_means = stats_class.mu(m)\n","    B1_means_list.append(B1_means)\n","    B1_mnt2_4 = stats_class.mnt(B1_means[0],B1_means[1],m)\n","    B1_stnd_list.append(B1_mnt2_4[1])\n","  # The nested loops calculates the covariance and correlations between B0 and P0\n","  for n in range(len(P1_vert[3:])):\n","    print(\"x: \", P1_vert[n+3][0])\n","    for o in range(len(B1_vert[2:])):\n","      print(\"    and \", B1_vert[o+2][0])\n","      P1B1_covar = stats_class.covar(P1_means_list[n],B1_means_list[o],P1_vert[n+3],B1_vert[o+2])\n","      P1B1_cor = stats_class.cor(P1B1_covar,P1_stnd_list[n],B1_stnd_list[o])\n","      print(P1B1_cor)\n","    print()\n","\n","# Part C: Data visualization ASCII\n","def P1_ASCII_graph(P1_vert):\n","  title_full = ['Stamina',\n","                'Feet','Ankle','Calves',\n","                'Knees','Quadriceps','Gluteus','Groin',\n","                'Abdominals','Lower Back',\n","                'Latissimus Dorsi','Trapezius','Shoulders',\n","                'Chest','Triceps','Biceps',\n","                'Neck','Head']\n","  graph_count = 3\n","  for p in P1_vert[3:]:\n","    #if graph_count == 4:\n","    #  break\n","    # Initialize graph class\n","    P1_graph = Graph(P1_vert, 1, graph_count)\n","    # ASCII Graphs\n","    # date_col_num = 1 # data_col_num = each successive column\n","    # this would be a loop over columns 3-20, 1st column is the date\n","    # print(p[0])\n","    # print()\n","    P1_hi_lo = P1_graph.hi_lo(graph_count)\n","    date_hi_lo = P1_graph.hi_lo(1)\n","    P1_binned = P1_graph.binned(P1_hi_lo)\n","    P1_time_series = P1_graph.time_series(date_hi_lo,P1_binned)\n","    # P1_graph.time_series_print(P1_time_series[0],P1_time_series[1])\n","    # P1_file_out = \"/content/P1_\" + p[0] + \".txt\"\n","    # P1_time_series_write = P1_graph.time_series_write(p[0],P1_file_out,P1_time_series[0],P1_time_series[1])\n","    # print(\"\\n\")\n","    graph_count += 1\n","\n","def B1_ASCII_graph(B1_vert):\n","  # Did not finish\n","  graph_count = 2\n","  for p in B1_vert[2:]:\n","    #if graph_count == 4:\n","    #  break\n","    # Initialize graph class\n","    B1_graph = Graph(B1_vert, 1, graph_count)\n","    # ASCII Graphs\n","    # date_col_num = 1 # data_col_num = each successive column\n","    # this would be a loop over columns 3-20, 1st column is the date\n","    # print(p[0])\n","    # print()\n","    B1_hi_lo = B1_graph.hi_lo(graph_count)\n","    date_hi_lo = B1_graph.hi_lo(1)\n","    B1_binned = B1_graph.binned(B1_hi_lo)\n","    B1_time_series = B1_graph.time_series(date_hi_lo,B1_binned)\n","    # B0_graph.time_series_print(B0_time_series[0],B0_time_series[1])\n","    # print()\n","    # B0_file_out = \"/content/B0_\" + p[0] + \".txt\"\n","    # B0_time_series_write = B0_graph.time_series_write(p[0],B0_file_out,B0_time_series[0],B0_time_series[1])\n","    # print(\"\\n\")\n","    graph_count += 1\n","\n","# Part D: Data visualization RGB\n","def P1_RGB_graph(P1_vert):\n","\n","  title_full = ['Stamina',\n","                'Feet','Ankle','Calves',\n","                'Knees','Quadriceps','Gluteus','Groin',\n","                'Abdominals','Lower Back',\n","                'Latissimus Dorsi','Trapezius','Shoulders',\n","                'Chest','Triceps','Biceps',\n","                'Neck','Head']\n","  P1_rgb = Graphs_rgb(P1_vert)\n","  P1_B1 = 0\n","  start_val = 3\n","  # Draws the bar charts\n","  P1_rgb_bar = P1_rgb.rgb_timeseries_bar(title_full,start_val,P1_B1)\n","  # RGB Line Graphs by Group\n","  # Uses the position of each body part name in the title_full list\n","  P1_groups_num = [3,4,7,11,13,16,19,21]\n","  P1_title_label = ['Stamina','Lower Legs','Upper Legs','Core','Upper Back','Arms','Head']\n","  # P1_rgb_line = P1_rgb.rgb_timeseries_line(title_full,start_val,P1_groups_num,P1_title_label,P1_B1)\n","\n","  # Line graphs by upper/lower body group means\n","  def small():\n","    csv_groups_list = [[3,4],[4,7,11,13],[13,16,19,21]]\n","    legend_label = [['Stamina'],['Lower Legs','Upper Legs','Core'],['Upper Back','Arms','Head']]\n","    k0 = 0\n","    for csv_groups_num in csv_groups_list:\n","      fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(15, 5))\n","      P1_rgb_line_smallest = P1_rgb.rgb_timeseries_small(csv_groups_num,legend_label[k0],ax)\n","      # Plot formatting\n","      plt.margins()\n","      plt.grid()\n","      plt.yticks(range(1,6))\n","      if sum(csv_groups_num) == sum(csv_groups_list[1]):\n","        ax.legend()\n","        plt.title(\"Lower Body\")\n","        plt.ylabel(\"Pain\")\n","        #plt.savefig(\"Lower Body Pain.jpg\")\n","      elif sum(csv_groups_num) == sum(csv_groups_list[2]):\n","        ax.legend()\n","        plt.title(\"Upper Body\")\n","        plt.ylabel(\"Pain\")\n","        #plt.savefig(\"Upper Body Pain.jpg\")\n","      else:\n","        plt.ylabel(\"Stamina\")\n","        #plt.savefig(\"Stamina.jpg\")\n","      k0 += 1\n","\n","  # Smallest on one graph\n","  def smallest():\n","    # csv_groups_list = [[3,4],[4,12],[13,21]]\n","    # legend_label = [['Stamina'],['Lower Body'], ['Upper Body']]\n","    csv_groups_list = [[4,21]]\n","    legend_label = [['Pain']]\n","    fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(15, 5))\n","    k1 = 0\n","    for csv_groups_num in csv_groups_list:\n","      P1_rgb_line_smallest = P1_rgb.rgb_timeseries_small(csv_groups_num,legend_label[k1],ax)\n","      k1 += 1\n","    # Plot formatting\n","    plt.margins()\n","    plt.grid()\n","    # plt.legend()\n","    plt.yticks(range(1,6))\n","    plt.ylabel(\"Pain\")\n","    plt.savefig('P1_smallerest.jpg')\n","  # small()\n","  # smallest()\n","\n","def B1_RGB_graph(B1_vert):\n","  title_full = ['Calories','Exercise',            # Group 0\n","              'Salt', 'Fat', 'Protein',           # Group 1\n","              'Carbohydrates', 'Alcohol Servings' # Group 3\n","              ]                                   # etc\n","  B1_rgb = Graphs_rgb(B1_vert)\n","  P1_B1 = 1\n","  start_val = 2\n","  # Part D RGB Graphs: B1.csv\n","  B1_rgb_bar = B1_rgb.rgb_timeseries_bar(title_full,start_val,P1_B1)\n","  # RGB Line Graphs by Group for B0.csv\n","  # Uses the position of each title in the title_full list\n","  B1_groups_num = [2,3,4,8,9]\n","  B1_title_label = ['Calories','Exercise','Nutrients','Alcohol Servings']\n","  # Line graph is not appropriate for calories, exercise, and alcohol servings\n","  #B1_rgb_line = B1_rgb.rgb_timeseries_line(title_full,start_val,B1_groups_num,B1_title_label,P1_B1)\n","\n","# A0_vert\n","def A0_RGB_graph(A0_vert):\n","  # Part D RGB Graphs: A0.tsv\n","  A0_sort = Graphs_sort(A0_vert)\n","  # Calculates the duration of each activity.\n","  A0_sort_duration = A0_sort.sort_time(A0_sort.data[6],A0_sort.data[4],A0_sort.data[5])\n","  # Removes endings for similar words such as: 'Walk', 'Walks', 'Walked', 'Walking'.\n","  A0_activity_filter = A0_sort.filter_stop(A0_sort.data[6])\n","  # Sorts the list using an implementation of merge sort.\n","  ord_list = ['ord_list'] + [ord(A0_activity_filter[x][0]) for x in range(1,len(A0_activity_filter))]\n","  A0_sort_merged = A0_sort.sort_ascii(ord_list,A0_activity_filter,A0_sort.data[1],A0_sort_duration)\n","  # Finds the unique occurances of each word in 'Activity'.\n","  A0_sort_unique = A0_sort.sort_unique_words(A0_sort_merged[0])\n","  # Bins the sorted list using the unique words.\n","  A0_sort_bin = A0_sort.sort_unique_bin(A0_sort_unique,A0_sort_merged)\n","  # Merges the bins based on if the first word in the string are the same.\n","  # i.e. 'Eat' <- 'Eat 1' <- 'Eat 2' <- 'Eat 3'\n","  A0_sort_similar = A0_sort.merge_similar_activities(A0_sort_bin)\n","  A0_sort_similar_splice = A0_sort.merge_activities_splice(A0_sort_similar)\n","  # Graphs\n","  A0_sort_graph = Graphs_rgb(A0_sort_similar_splice)\n","  A0_sort_graph.rgb_timeseries_frequency()\n","  A0_sort_graph.rgb_timeseries_duration()\n","\n","# A1_vert\n","def A1_RGB_graph(A1_vert):\n","  # Part D RGB Graphs: A0.tsv\n","  A1_sort = Graphs_sort(A1_vert)\n","  # Calculates the duration of each activity.\n","  A1_sort_duration = A1_sort.sort_time(A1_sort.data[6],A1_sort.data[4],A1_sort.data[5])\n","\n","  # Removes endings for similar words such as: 'Walk', 'Walks', 'Walked', 'Walking'.\n","  A1_activity_filter = A1_sort.filter_stop(A1_sort.data[6])\n","  # Sorts the list using an implementation of merge sort.\n","  ord_list = ['ord_list'] + [ord(A1_activity_filter[x][0]) for x in range(1,len(A1_activity_filter))]\n","  # todo order entire list of lists.\n","  A1_sort_merged = A1_sort.sort_ascii([A1_sort.data[0],\n","                                       A1_sort.data[1],\n","                                       A1_sort.data[2],\n","                                       A1_sort.data[3],\n","                                       A1_sort.data[4],\n","                                       A1_sort.data[5],\n","                                       A1_sort.data[6],\n","                                       A1_sort.data[7],\n","                                       A1_sort.data[8],\n","                                       A1_sort_duration],\n","                                       A1_activity_filter,\n","                                       ord_list)\n","\n","  '''\n","  # Finds the unique occurances of each word in 'Activity'.\n","  A1_sort_unique = A1_sort.sort_unique_words(A1_sort_merged[0])\n","  # Bins the sorted list using the unique words.\n","  A1_sort_bin = A1_sort.sort_unique_bin(A1_sort_unique,A1_sort_merged)\n","  # Merges the bins based on if the first word in the string are the same.\n","  # i.e. 'Eat' <- 'Eat 1' <- 'Eat 2' <- 'Eat 3'\n","  A1_sort_similar = A1_sort.merge_similar_activities(A1_sort_bin)\n","  A1_sort_similar_splice = A1_sort.merge_activities_splice(A1_sort_similar)\n","  # Graphs\n","\n","  A1_sort_graph = Graphs_rgb(A1_sort_similar_splice) # the time might be messed up since there were 40 walk occurences but only 5 hours.\n","  # A1_sort_graph = Graphs_rgb(A1_sort_bin)\n","  A1_sort_graph.rgb_timeseries_frequency()    # ( 0, 2 )\n","  A1_sort_graph.rgb_timeseries_duration()     # ( 0, 1 )\n","  A1_sort_graph.rgb_timeseries_mean()         # ( 0,(1/2) )\n","  A1_sort_graph.rgb_timeseries_daily_mean()   # ( 0,((1/2) / 100) )\n","  '''\n","\n","# P1_RGB_graph(P1_vert)\n","# B1_RGB_graph(B1_vert)\n","# A0_RGB_graph(A0_vert)\n","A1_RGB_graph(A1_vert)\n"],"metadata":{"id":"IxtfVwAjhyaV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745006820699,"user_tz":300,"elapsed":184,"user":{"displayName":"David Leifer","userId":"06279506333224389759"}},"outputId":"8a133603-d48e-45a7-f1ea-3feb23b933d9"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["['Day_ID', '0', '', '', '', '', '1', '', '', '', '2', '', '', '', '3', '', '', '', '', '4', '', '', '', '5', '', '', '', '6', '', '', '', '', '7', '', '', '', '', '', '', '8', '', '', '', '', '', '', '', '9', '', '', '', '', '', '10', '', '', '', '11', '', '', '', '', '', '', '12', '', '', '', '', '', '13', '', '', '', '14', '', '', '', '', '', '', '15', '', '', '', '', '', '16', '', '', '', '', '', '', '', '17', '', '', '', '18', '', '', '', '', '', '', '19', '', '', '', '', '', '20', '', '', '', '21', '', '', '', '', '22', '', '', '', '', '', '23', '', '', '', '', '', '24', '25', '', '', '', '', '', '', '26', '', '', '', '', '', '', '27', '', '', '', '', '', '', '28', '', '', '', '', '', '', '29', '', '', '', '', '', '', '', '30', '', '', '', '', '', '', '', '', '', '', '31', '', '', '', '', '32', '', '', '', '33', '', '', '', '', '34', '', '', '', '', '', '', '', '35', '', '', '', '', '', '', '', '36', '', '', '', '', '', '', '37', '', '', '', '', '', '', '', '', '38', '', '', '', '', '', '39', '', '', '', '', '', '40', '', '', '', '41', '', '', '', '', '', '42', '', '', '', '43', '', '', '44', '', '', '', '45', '', '', '', '46', '47', '', '', '48', '', '', '49', '', '', '', '', '', '', '', '', '', '', '', '', '50', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '51', '', '', '', '', '', '', '52', '', '', '', '', '', '', '', '53', '', '', '', '', '', '', '', '', '', '', '54', '', '', '', '', '55', '', '', '', '', '56', '', '', '', '', '', '57', '', '', '58', '', '', '', '59', '', '', '', '60', '', '', '', '', '', '61', '', '', '', '62', '', '', '', '63', '', '', '', '64', '', '', '', '', '', '', '', '', '', '65', '', '', '', '', '', '66', '', '', '', '', '', '', '67', '', '', '', '', '', '', '68', '', '', '', '69', '', '', '', '', '70', '', '', '', '71', '', '', '', '', '72', '', '', '73', '', '', '74', '', '75', '', '', '', '', '76', '', '', '77', '', '', '', '', '', '', '78', '', '', '', '', '', '79', '', '80', '', '', '81', '', '82', '', '', '', '83', '', '', '84', '', '85', '', '', '', '', '', '', '86', '', '', '87', '', '', '', '88', '', '', '', '', '89', '', '', '', '90', '', '', '91', '', '', '', '', '', '', '92', '', '', '', '', '', '', '93', '', '', '', '94', '', '', '', '95', '', '', '', '', '', '', '96', '', '', '', '97', '', '', '', '', '98', '', '', '', '', '', '99', '', '', '', '', '']\n","['Time_ID', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '450', '451', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '480', '481', '482', '483', '484', '485', '486', '487', '488', '489', '490', '491', '492', '493', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '510', '511', '512', '513', '514', '515', '516', '517', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '533', '534', '535', '536', '537', '538']\n","['Day', 'Monday', '', '', '', '', 'Tuesday', '', '', '', 'Wednesday', '', '', '', 'Thursday', '', '', '', '', 'Friday', '', '', '', 'Saturday', '', '', '', 'Sunday', '', '', '', '', 'Monday', '', '', '', '', '', '', 'Tuesday', '', '', '', '', '', '', '', 'Wednesday', '', '', '', '', '', 'Thursday', '', '', '', 'Friday', '', '', '', '', '', '', 'Saturday', '', '', '', '', '', 'Sunday', '', '', '', 'Monday', '', '', '', '', '', '', 'Tuesday', '', '', '', '', '', 'Wednesday', '', '', '', '', '', '', '', 'Thursday', '', '', '', 'Friday', '', '', '', '', '', '', 'Saturday', '', '', '', '', '', 'Sunday', '', '', '', 'Monday', '', '', '', '', 'Tuesday', '', '', '', '', '', 'Wednesday', '', '', '', '', '', 'Thursday', 'Friday', '', '', '', '', '', '', 'Saturday', '', '', '', '', '', '', 'Sunday', '', '', '', '', '', '', 'Monday', '', '', '', '', '', '', 'Tuesday', '', '', '', '', '', '', '', 'Wednesday', '', '', '', '', '', '', '', '', '', '', 'Thursday', '', '', '', '', 'Friday', '', '', '', 'Saturday', '', '', '', '', 'Sunday', '', '', '', '', '', '', '', 'Monday', '', '', '', '', '', '', '', 'Tuesday', '', '', '', '', '', '', 'Wednesday', '', '', '', '', '', '', '', '', 'Thursday', '', '', '', '', '', 'Friday', '', '', '', '', '', 'Saturday', '', '', '', 'Sunday', '', '', '', '', '', 'Monday', '', '', '', 'Tuesday', '', '', 'Wednesday', '', '', '', 'Thursday', '', '', '', 'Friday', 'Saturday', '', '', 'Sunday', '', '', 'Monday', '', '', '', '', '', '', '', '', '', '', '', '', 'Tuesday', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Wednesday', '', '', '', '', '', '', 'Thursday', '', '', '', '', '', '', '', 'Friday', '', '', '', '', '', '', '', '', '', '', 'Saturday', '', '', '', '', 'Sunday', '', '', '', '', 'Monday', '', '', '', '', '', 'Tuesday', '', '', 'Wednesday', '', '', '', 'Thursday', '', '', '', 'Friday', '', '', '', '', '', 'Saturday', '', '', '', 'Sunday', '', '', '', 'Monday', '', '', '', 'Tuesday', '', '', '', '', '', '', '', '', '', 'Wednesday', '', '', '', '', '', 'Thursday', '', '', '', '', '', '', 'Friday', '', '', '', '', '', '', 'Saturday', '', '', '', 'Sunday', '', '', '', '', 'Monday', '', '', '', 'Tuesday', '', '', '', '', 'Wednesday', '', '', 'Thursday', '', '', 'Friday', '', 'Saturday', '', '', '', '', 'Sunday', '', '', 'Monday', '', '', '', '', '', '', 'Tuesday', '', '', '', '', '', 'Wednesday', '', 'Thursday', '', '', 'Friday', '', 'Saturday', '', '', '', 'Sunday', '', '', 'Monday', '', 'Tuesday', '', '', '', '', '', '', 'Wednesday', '', '', 'Thursday', '', '', '', 'Friday', '', '', '', '', 'Saturday', '', '', '', 'Sunday', '', '', 'Monday', '', '', '', '', '', '', 'Tuesday', '', '', '', '', '', '', 'Wednesday', '', '', '', 'Thursday', '', '', '', 'Friday', '', '', '', '', '', '', 'Saturday', '', '', '', 'Sunday', '', '', '', '', 'Monday', '', '', '', '', '', 'Tuesday', '', '', '', '', '']\n","['Date', '722', '', '', '', '', '723', '', '', '', '724', '', '', '', '725', '', '', '', '', '726', '', '', '', '727', '', '', '', '728', '', '', '', '', '729', '', '', '', '', '', '', '730', '', '', '', '', '', '', '', '731', '', '', '', '', '', '801', '', '', '', '802', '', '', '', '', '', '', '803', '', '', '', '', '', '804', '', '', '', '805', '', '', '', '', '', '', '806', '', '', '', '', '', '807', '', '', '', '', '', '', '', '808', '', '', '', '809', '', '', '', '', '', '', '810', '', '', '', '', '', '811', '', '', '', '812', '', '', '', '', '813', '', '', '', '', '', '814', '', '', '', '', '', '815', '816', '', '', '', '', '', '', '817', '', '', '', '', '', '', '818', '', '', '', '', '', '', '819', '', '', '', '', '', '', '820', '', '', '', '', '', '', '', '821', '', '', '', '', '', '', '', '', '', '', '822', '', '', '', '', '823', '', '', '', '824', '', '', '', '', '825', '', '', '', '', '', '', '', '826', '', '', '', '', '', '', '', '827', '', '', '', '', '', '', '828', '', '', '', '', '', '', '', '', '829', '', '', '', '', '', '830', '', '', '', '', '', '831', '', '', '', '901', '', '', '', '', '', '902', '', '', '', '903', '', '', '904', '', '', '', '905', '', '', '', '906', '907', '', '', '908', '', '', '909', '', '', '', '', '', '', '', '', '', '', '', '', '910', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '911', '', '', '', '', '', '', '912', '', '', '', '', '', '', '', '913', '', '', '', '', '', '', '', '', '', '', '914', '', '', '', '', '915', '', '', '', '', '916', '', '', '', '', '', '917', '', '', '918', '', '', '', '919', '', '', '', '920', '', '', '', '', '', '921', '', '', '', '922', '', '', '', '923', '', '', '', '924', '', '', '', '', '', '', '', '', '', '925', '', '', '', '', '', '926', '', '', '', '', '', '', '927', '', '', '', '', '', '', '928', '', '', '', '929', '', '', '', '', '930', '', '', '', '1001', '', '', '', '', '1002', '', '', '1003', '', '', '1004', '', '1005', '', '', '', '', '1006', '', '', '1007', '', '', '', '', '', '', '1008', '', '', '', '', '', '1009', '', '1010', '', '', '1011', '', '1012', '', '', '', '1013', '', '', '1014', '', '1015', '', '', '', '', '', '', '1016', '', '', '1017', '', '', '', '1018', '', '', '', '', '1019', '', '', '', '1020', '', '', '1021', '', '', '', '', '', '', '1022', '', '', '', '', '', '', '1023', '', '', '', '1024', '', '', '', '1025', '', '', '', '', '', '', '1026', '', '', '', '1027', '', '', '', '', '1028', '', '', '', '', '', '1029', '', '', '', '', '']\n","['Start', '800', '1000', '1700', '1730', '1800', '900', '1100', '1600', '1630', '800', '1000', '1600', '1630', '900', '1100', '1600', '1600', '1800', '900', '1600', '1700', '1800', '900', '1400', '1600', '1630', '500', '1000', '1015', '1700', '1830', '810', '1000', '1100', '1400', '1530', '1900', '1915', '930', '1145', '1200', '1230', '1245', '1430', '1500', '1900', '800', '1030', '1200', '1245', '1645', '2000', '800', '1130', '1330', '1545', '730', '1200', '1530', '1630', '1700', '1745', '1800', '500', '530', '900', '1600', '1630', '1715', '630', '730', '930', '1800', '900', '1000', '1130', '1345', '1445', '1545', '1630', '815', '1000', '1100', '1215', '1530', '1800', '845', '915', '1335', '1435', '1525', '1530', '1900', '2100', '930', '1600', '1700', '2230', '0', '830', '1030', '1045', '1400', '1600', '1800', '800', '930', '1100', '1200', '1330', '2200', '900', '1000', '1230', '1545', '830', '1100', '1145', '1600', '1800', '100', '800', '1015', '1215', '1223', '1400', '845', '1045', '1200', '1215', '1400', '1930', '900', '900', '1100', '1600', '1515', '1900', '2000', '2030', '815', '915', '1200', '1400', '1715', '1730', '2100', '800', '1000', '1130', '1140', '1430', '1630', '1730', '900', '1045', '1300', '1400', '1545', '1615', '2230', '900', '1000', '1115', '1630', '1645', '1830', '1845', '2300', '0', '800', '1100', '1115', '1230', '1330', '1400', '1530', '1745', '1900', '2100', '800', '1100', '1330', '1800', '2200', '800', '1000', '1100', '1230', '15', '815', '1200', '1330', '1600', '900', '1000', '1030', '1230', '1300', '1745', '1815', '2300', '730', '945', '1015', '1100', '1115', '1330', '1900', '2030', '800', '1045', '1115', '1205', '1915', '1945', '2200', '500', '900', '930', '1105', '1300', '1515', '1800', '2000', '2230', '800', '1000', '1015', '1050', '1230', '2000', '800', '1000', '1030', '1200', '1445', '1700', '830', '1100', '1155', '1900', '945', '1115', '1200', '1500', '1730', '1800', '0', '1500', '1530', '1600', '1000', '1200', '1805', '1000', '1115', '1145', '2000', '930', '1030', '1130', '1600', '1030', '500', '1800', '2030', '930', '1800', '1945', '900', '945', '1200', '1255', '1400', '1530', '1630', '1900', '2030', '2115', '2145', '1100', '1130', '100', '315', '815', '815', '820', '935', '1015', '1100', '1130', '1145', '1215', '1230', '1330', '1400', '1800', '2145', '800', '930', '1000', '1315', '1500', '1545', '1610', '715', '730', '900', '1145', '1430', '1600', '1635', '1815', '740', '745', '950', '1030', '1035', '1040', '1200', '1545', '1415', '1930', '2100', '700', '800', '1230', '1400', '1545', '815', '830', '930', '1045', '1800', '815', '1000', '1200', '1645', '2000', '2100', '845', '1200', '1230', '1000', '1100', '1215', '2030', '900', '945', '1115', '1900', '300', '845', '915', '1015', '1115', '1215', '1100', '1230', '1400', '2015', '1600', '1645', '1745', '1900', '300', '930', '1030', '1800', '815', '845', '1000', '1100', '1130', '1430', '1700', '1900', '1930', '2100', '830', '900', '1100', '1500', '1730', '2100', '700', '1000', '1100', '1500', '1730', '1815', '1845', '800', '1030', '1245', '1300', '1445', '1550', '1830', '815', '1030', '1900', '1115', '1030', '1100', '1900', '1915', '1945', '815', '900', '1230', '1300', '930', '1000', '1100', '1930', '2300', '930', '1100', '1500', '1100', '1145', '1600', '945', '1900', '930', '1030', '1130', '1800', '1900', '1030', '1400', '1700', '930', '945', '1015', '1200', '1430', '1800', '2000', '1030', '1300', '1330', '1430', '1730', '1900', '1000', '1115', '1200', '1230', '1715', '800', '1900', '900', '1000', '1900', '2030', '1515', '1600', '1615', '930', '1830', '815', '900', '1430', '1715', '1900', '1925', '1935', '915', '1015', '1600', '1230', '1300', '1615', '1845', '900', '945', '1015', '1115', '1900', '900', '945', '1200', '1800', '930', '1500', '1600', '830', '930', '1045', '1500', '1530', '2030', '2315', '815', '945', '1045', '1330', '1500', '1545', '1630', '830', '1000', '1100', '1800', '1300', '1400', '1500', '2330', '1045', '1115', '1230', '1330', '1430', '1545', '2000', '1030', '1800', '1830', '1900', '1000', '1130', '1245', '1330', '1900', '900', '930', '1130', '1200', '1800', '2100', '900', '930', '1130', '1200', '1800', '2100']\n","['End', '1000', '1100', '1730', '1800', '1815', '1100', '1130', '1630', '1700', '1000', '1045', '1630', '1700', '1100', '1130', '1615', '1700', '1830', '1100', '1700', '1730', '1830', '1100', '1500', '1630', '1700', '600', '1015', '1100', '1715', '1915', '850', '1030', '1400', '1430', '1630', '1915', '1930', '1130', '1200', '1230', '1245', '1300', '1500', '1515', '1915', '835', '1200', '1245', '1250', '1700', '2015', '845', '1300', '1530', '1645', '945', '1300', '1600', '1700', '1745', '1800', '1815', '530', '630', '930', '1630', '1715', '1725', '730', '930', '1130', '1900', '1000', '1100', '1245', '1445', '1545', '1600', '1700', '945', '1015', '1215', '1300', '1545', '1930', '900', '1115', '1435', '1525', '1530', '1535', '2000', '2300', '1430', '1630', '1800', '2330', '30', '1030', '1045', '1145', '1600', '1630', '1930', '900', '1015', '1130', '1330', '1530', '2207', '1000', '1130', '1330', '1615', '1045', '1145', '1200', '1700', '1900', '230', '1000', '1215', '1223', '1230', '1530', '1030', '1200', '1215', '1230', '1515', '2030', '945', '1030', '1130', '1615', '1545', '2000', '2030', '2040', '915', '930', '1230', '1415', '1730', '1845', '2130', '1000', '1130', '1140', '1150', '1500', '1730', '1800', '1030', '1215', '1345', '1530', '1615', '1700', '50', '1000', '1015', '1230', '1645', '1730', '1845', '1900', '2359', '200', '1030', '1115', '1145', '1300', '1400', '1530', '1630', '1900', '1910', '2115', '1045', '1300', '1330', '1815', '2300', '1000', '1100', '1145', '1300', '100', '1030', '1330', '1400', '1700', '945', '1030', '1200', '1300', '1345', '1815', '1900', '2330', '930', '1000', '1100', '1115', '1130', '1400', '1915', '2045', '1030', '1100', '1205', '1220', '1945', '2000', '2245', '530', '930', '1015', '1205', '1305', '1530', '1910', '2030', '2300', '1000', '1015', '1030', '1150', '1240', '2015', '1000', '1015', '1100', '1205', '1535', '1715', '1030', '1130', '1245', '1910', '1115', '1145', '1215', '1545', '1800', '1845', '100', '1530', '1545', '1620', '1030', '1245', '1830', '1115', '1145', '1230', '2100', '1000', '1045', '1200', '1645', '1415', '900', '1900', '2200', '1130', '1900', '1950', '945', '1145', '1255', '1325', '1530', '1545', '1700', '1930', '2045', '2120', '2150', '1115', '1130', '130', '330', '815', '820', '845', '955', '1100', '1130', '1145', '1215', '1230', '1330', '1400', '1430', '1815', '2205', '930', '1000', '1020', '1345', '1545', '1610', '1620', '730', '845', '915', '1200', '1500', '1630', '1700', '1830', '745', '945', '1030', '1035', '1040', '1050', '1330', '1600', '1445', '2030', '2130', '800', '830', '1300', '1445', '1700', '830', '900', '1030', '1200', '1845', '845', '1045', '1230', '1700', '2020', '2120', '900', '1220', '1430', '1045', '1215', '1300', '2115', '945', '1115', '1120', '1920', '330', '900', '1000', '1100', '1215', '1240', '1230', '1245', '1530', '2100', '1615', '1745', '1815', '1930', '330', '945', '1130', '1945', '830', '900', '1100', '1130', '1200', '1445', '1715', '1910', '1945', '2215', '845', '1100', '1130', '1515', '1745', '2130', '720', '1015', '1300', '1515', '1745', '1845', '1900', '1030', '1045', '1300', '1415', '1530', '1615', '1900', '1030', '1045', '2000', '1145', '1100', '1115', '1915', '1945', '2100', '900', '930', '1300', '1315', '1000', '1100', '1130', '2000', '2330', '1100', '1145', '1515', '1145', '1210', '1615', '1045', '1915', '1000', '1115', '1145', '1830', '2000', '1115', '1430', '1730', '945', '1000', '1115', '1215', '1500', '1830', '2020', '1100', '1330', '1345', '1515', '1745', '1915', '1030', '1215', '1215', '1330', '1745', '815', '1930', '930', '1100', '1930', '2100', '1600', '1615', '1630', '945', '1915', '845', '945', '1500', '1745', '1925', '1935', '1945', '945', '1115', '1615', '1300', '1320', '1715', '1915', '915', '1000', '1025', '1230', '2000', '915', '1000', '1300', '1815', '1000', '1515', '1700', '900', '1030', '1215', '1530', '1600', '2100', '2330', '845', '1015', '1115', '1400', '1515', '1600', '1645', '850', '1100', '1115', '1830', '1400', '1500', '1510', '2345', '1115', '1145', '1330', '1430', '1445', '1600', '2015', '1100', '1830', '1900', '1920', '1030', '1245', '1330', '1400', '1930', '930', '1000', '1200', '1215', '1815', '2115', '930', '1000', '1200', '1215', '1815', '2115']\n","['Activty', 'Skateboard paper', 'Guitar', 'Skateboarding basement', 'Skateboarding', 'Stretched', 'Skateboard paper', 'Longboarding L', 'Juggling', 'Guitar', 'Skateboard paper', 'Longboarding L', 'Juggling', 'Guitar', 'Skateboard paper', 'Walk', 'Longboarding L', 'Skateboard paper', 'Guitar', 'Skateboard paper', 'Skateboard paper', 'Juggling', 'Guitar', 'Skateboard paper', 'Skateboard paper', 'Juggling', 'Run', 'Run', 'Juggling', 'Skateboarding basement', 'Core', 'Stretching', 'Run', 'Guitar', 'Read', 'Juggling', 'Skateboarding basement', 'Core', 'Stretching', 'Skateboard paper', 'Walk', 'Lift', 'Core', 'Stretch', 'Skateboard paper', 'Juggling', 'Guitar', 'Run', 'Skateboard paper', 'Guitar', 'No juggling', 'Skateboarding basement', 'Stretch', 'Walk', 'Read', 'Guitar', 'Skateboarding basement', 'Skateboard paper', 'Guitar', 'Walking', 'Juggling', 'Skateboarding basement', 'Core', 'Stretch', 'Guitar', 'Read', 'Longboarding L', 'Run juggle', 'Skateboarding basement', 'Lift', 'Skateboard paper', 'Guitar', 'Skateboard paper', 'Walk', 'Guitar', 'Skateboarding', 'Cooking', 'Reading', 'Guitar', 'Stretch', 'Juggling', 'Skateboard paper', 'Guitar', 'Skateboarding', 'Stretch', 'Juggling', 'Skateboard crafting', 'Juggling', 'Skateboard paper', 'Skateboarding basement', 'Skateboarding outside', 'Core', 'Stretch', 'Guitar', 'Read', 'Break day drink beer', 'Juggle', 'Guitar', 'Walk', 'Guitar', 'Skateboard paper', 'Guitar', 'Longboarding', 'Skateboard paper', 'Guitar', 'Skateboarding', 'Longboarding', 'Skateboard paper', 'Guitar', 'Skateboard paper', 'Skateboard crafting', 'Stretch', 'Skateboard paper', 'Skateboarding', 'Skateboard paper', 'Stretch', 'Skateboard paper', 'Running', 'Stretch', 'Skateboard paper', 'Guitar', 'Skateboard paper', 'Skateboard paper', 'Skateboarding', 'Core', 'Stretch', 'Skateboard paper', 'Skateboard paper', 'Skateboarding', 'Core', 'Stretch', 'Read', 'Guitar', 'Stretch', 'Guitar', 'Walk', 'Stretch', 'Juggle', 'Guitar', 'Juggle', 'Skateboarding basement', 'Longboarding', 'Stretch', 'Juggling', 'Juggling', 'Stretch', 'Guitar', 'Longboarding', 'Read', 'Longboarding', 'Core', 'Stretch', 'Read', 'Juggling', 'Longboarding', 'Skateboard paper', 'Juggling', 'Guitar', 'Skateboard paper', 'Longboarding', 'Juggling', 'Walking', 'Guitar', 'Stretching', 'Walking', 'Juggling', 'Longboarding', 'Stretching', 'Juggling', 'Writing', 'Writing', 'Writing', 'Walking', 'Juggling', 'Writing', 'Guitar', 'Stretching', 'Guitar', 'Longboarding', 'Stretching', 'Stretching', 'Skateboard paper', 'Skateboarding', 'Stretching', 'Stretching', 'Guitar', 'Skateboard paper', 'Guitar', 'Skateboard paper', 'Juggling', 'Guitar', 'Skateboard paper', 'Skateboarding', 'Stretching', 'Guitar', 'Skateboard paper', 'Guitar', 'Skateboard paper', 'Walking', 'Stretching', 'Juggling', 'Longboarding', 'Guitar', 'Skateboard paper', 'Juggling', 'Skateboarding', 'Core', 'Stretching', 'Guitar', 'Guitar', 'Stretching', 'Skateboard paper', 'Juggling', 'Skateboarding', 'Stretching', 'Guitar', 'Stretching', 'Guitar', 'Guitar', 'Guitar', 'Skateboard paper', 'Skateboarding', 'Stretching', 'Guitar', 'Walking', 'Stretching', 'Guitar', 'Skateboard paper', 'Guitar', 'Walking', 'Skateboarding', 'Stretching', 'Stretching', 'Skateboard paper', 'Guitar', 'Walking', 'Stretching', 'Longboarding L', 'Stretching', 'Skateboard paper', 'Juggling', 'Longboarding L', 'Stretching', 'Skateboard paper', 'Guitar', 'Stretching', 'Walking', 'Slowed heart rate', 'Examined legs', 'Skateboard paper', 'Skateboard paper', 'Data backup', 'Running', 'Skateboard paper', 'Walking', 'Guitar', 'Skateboard paper', 'Guitar', 'Running', 'Skateboard paper', 'Guitar', 'Walking', 'Guitar', 'Walking', 'Driving', 'Driving', 'Longboarding L', 'Skateboard paper', 'Skateboard paper', 'Running', 'Closed AC vent in room', 'Guitar', 'Skateboard paper', 'Skateboarding', 'Core', 'Nap time', 'Rested', 'Guitar', 'Guitar', 'Stretching', 'Boiled water', 'Boiled water', 'Lotion', 'Sleep', 'Woke up', 'Woke up', 'Woke up', 'Eat', 'Guitar', 'Walking', 'Running', 'Stretching', 'Shower', 'Eat', 'Stretching', 'Nap time', 'Rest', 'Stretching', 'Eat', 'Stretching', 'Skateboard paper', 'Stretching', 'Walking', 'Guitar', 'Juggling', 'Running', 'Stretching', 'Guitar', 'Skateboard paper', 'Walking', 'Walking', 'Walking', 'Walking', 'Running', 'Stretching', 'Guitar', 'Skateboard paper', 'Skateboarding', 'Stretching', 'Core', 'Stretching', 'Skateboard paper', 'Walking', 'Guitar', 'Walking', 'Stretching', 'Skateboard paper', 'Guitar', 'Stretching', 'Walking', 'Longboarding', 'Stretching', 'Guitar', 'Guitar', 'Stretching', 'Guitar', 'Guitar', 'Walking', 'Guitar', 'Guitar', 'Walking', 'Walking', 'Guitar', 'Longboarding L', 'Reading', 'Skateboarding', 'Running', 'Stretching', 'Stretching', 'Guitar', 'Skateboard paper', 'Stretching', 'Guitar', 'Guitar', 'Guitar', 'Skateboard cleaning', 'Skateboarding', 'Running', 'Stretching', 'Skateboard paper', 'Guitar', 'Skateboard paper', 'Walking', 'Guitar', 'Longboarding L', 'Running', 'Reading', 'Guitar', 'Guitar', 'Hiking', 'Guitar', 'Guitar', 'Walking', 'Skateboarding', 'Running', 'Cool down', 'Guitar', 'Reading', 'Juggling', 'Guitar', 'Reading', 'Guitar', 'Reading', 'Walking', 'Longboarding L', 'Juggling', 'Walking', 'Longboarding L', 'Guitar', 'Longboarding L', 'Guitar', 'Guitar', 'Juggling', 'Walking', 'Skateboard paper', 'Guitar', 'Guitar', 'Skateboard paper', 'Longboarding L', 'Running', 'Guitar', 'Skateboard paper', 'Guitar', 'Skateboard videos', 'Guitar', 'Skateboard videos', 'Guitar', 'Guitar', 'Reading', 'Skateboard videos', 'Skateboard paper', 'Guitar', 'Walking', 'Core', 'Guitar', 'Longboarding', 'Core', 'Guitar', 'Guitar', 'Skateboard paper', 'Longboarding L', 'Guitar', 'Longboarding L', 'Core', 'Guitar', 'Longboarding L', 'Guitar', 'Guitar', 'Longboarding L', 'Core', 'Plyometrics', 'Stretching', 'Longboarding L', 'Guitar', 'Stretching', 'Stretching', 'Guitar', 'Skateboarding', 'Stretching', 'Walking', 'Guitar', 'Stretching', 'Guitar', 'Longboarding L', 'Stretching', 'Walking', 'Stretching', 'Guitar', 'Guitar', 'Skateboarding', 'Stretching', 'Longboarding L', 'Walking', 'Stretching', 'Guitar', 'Guitar', 'Skateboarding', 'Guitar', 'Stretching', 'Longboarding L', 'Core', 'Stretching', 'Stretching', 'Walking', 'Guitar', 'Longboarding L', 'Guitar', 'Walking', 'Running', 'Core', 'Stretching', 'Guitar', 'Longboarding L', 'Guitar', 'Guitar', 'Stretching', 'Longboarding L', 'Stretching', 'Guitar', 'Walking', 'Stretching', 'Skateboarding', 'Walking', 'Stretching', 'Guitar', 'Longboarding L', 'Guitar', 'Guitar', 'Stretching', 'Skateboard Cleaning', 'Guitar', 'Skateboarding', 'Skateboard Cleaning', 'Guitar', 'Walking', 'Walking', 'Stretching', 'Guitar', 'Walking', 'Guitar', 'Walking', 'Stretching', 'Stretching', 'Stretching', 'Guitar', 'Skateboarding', 'Core', 'Guitar', 'Walking', 'Longboard L', 'Stretching', 'Stretching', 'Guitar', 'Stretching', 'Walking', 'Longboard L', 'Stretching', 'Stretching', 'Guitar', 'Guitar', 'Walking', 'Longboard L', 'Stretching', 'Guitar', 'Walking', 'Longboard L', 'Stretching', 'Guitar', 'Guitar', 'Stretching', 'Walking', 'Stretching', 'Guitar', 'Stretching', 'Guitar', 'Stretching', 'Walking', 'Stretching', 'Guitar', 'Stretching']\n","['Explaination', 'This was a rest week where I found data issues', '', 'Indoors', 'Outdoors', 'Left lateralis vastus quadriceps sore', '', 'Rest week, dry shins, bruised quad', '', '', '', '', 'Relearning after week break from R palm lacerations', '', '', '', '', '', '', '', '', 'Took several days to get okay again', '', '', '', '', '1 mile moderate, 1 mile jog', '~3 mile, 1 mile walk', 'Made improvements on ball height', 'Toerail to heelrail. Very warm out and not skateable for the next week.', '', 'Left pelivc tilt, groin, hip flexor', '30 minutes, 10 minute walk', 'Much easier with R palm skin. And not cut L finger (from chopping vegetables)', 'Neurology - Unit 1 neuron permeability for K and Na, electrical connectivity based on axon myelin between neurons to increase nervous system speed since pure water is a poor conductor.', 'Indoors, used basement ceiling floorboards to control height', 'R Toerail to heel rail to 3/4 flip. Os flip. R kickflip. Jolt through L bicep femoris and broken board. Previously broke one ply layer of wood with similar effects. From 1/2 kickflip and landing on top of of upside down U shaped board. Also modified nose moving trucks back for more length.', '', '', '', 'Rugby juggle', 'L single leg squat, rdl, lunge', 'Handstands, L side plank, L db oblique raises', 'Hamstrings, abductors, hip flexors', 'Very close to kickflip, backfoot habitually moves backwards', '', '', '2.5 miles walk .5 miles', 'r squared formulas seem different on different sites', '1014: 3 areas I have to look and 4-5 errors', 'Removed several pavement fragments from partially healed palm', 'Left glute sore from single leg squats, broke the newish board', 'L lateral tilt, hamstrings, abductors, hip flexors', 'L leg glute sore', 'Muscular imbalance for powerlifters, jumpers, and skateboarders.', 'Went through all the sheet music', 'Did all the tricks except os and primo flips. Almost kickflip', 'Rewriting the csv parser, could use B method but A is more readable', 'Also noodled for an hour afterward but not included in time listed.', '', '5 ball is hard', 'Dead front leg on R kickflip, did more L tricks', 'Handstands, pushup, bosu ball, yoga ball', 'Hip flexors and groin', 'BWV 565 octopus organ song', 'Reduced saturated fat/exercise increases neuron sheath conductivity', 'LR Manuals, nose manuals, bs varial 180s, and R backleg revert', 'Scrubbed sb paint off curb I think turkey boy went back with chalk', 'Landed a stationary kickflip! Switch leg unalive.', 'L single leg romanian, lunge, bosu ball', 'Revised outline', '', 'Gripped new board', '', 'Printed more sheet music', 'Manuals, space walks, reverts, ollies, R FS 180, 12 BS 180 landed 2', 'Made health bowl and 3 buritos for later. Also did laundry. I did that every 10-14 days for 2 months.', 'Myeline sheaths in muscle faster. Not in brain more leaky', 'BWV 965, 999v1, 999v2', 'Lateral upper leg muscles and hamstrings', '5 Ball was more consistent', 'Premature hypothesis: I get injured more frequently after an hour of skate', 'BWV 999 v1 v2', 'BS180 bend knees more on landing to absorb weight', 'Hip flexor and groin', 'Lost a ball :(', 'Repurposing broken board into a penny board without power tools', \"Found a ball ; ' D\", '', 'Switch only. Kickflip is difficult.', 'Mostly switch FS180', 'Ball pichael', 'Abduction stretch', '1004 site read. 1004 play through', 'Spicy food effect on cells', '', '3 ball clap R L and one hand 3 ball 4 catches', 'BWV 1014 site reading', 'I have an idea, lets use the same 10 costumes for security in every area.', 'BWV 1014 - had to look 3-4 times', 'Read about parametric vs nonparametric. Outlined future work.', 'BWV 999 warmup', 'R ollie manuals, R FS revert, L manuals, quick L turns, RL spacewalks', 'Fixed moment function, started graph function', 'BWV 1014', 'R BS 180s, R curb ollies, L handful of curb ollies', 'RL FS revert, RL manuals, quick L turns, RL spacewalks. TODO find a way to land with legs more bent to absorb more. Otherwise the smaller ankle and calf muscles get sore and long recovery. Also harder to land with 1/4 bent legs', 'Binned dates based on value', 'BWV 999, end half of 1014', 'Graph prints 5-1 but not sequentially', 'Semi even nail hammers on broken board, sawed/chiseled, sanded', 'Abductors and hamstrings', 'Refractored graph', 'BS 180 problem list: Problem 1) Board fully rotates, back foot doesnt stay on. 2) Board fully rotates, back foot on, weight too far forward. 3) BS 120-150 lands, stops moving forward.', 'Fixed print issue, refractored graph, decided to make a class', 'Groin and hip flexors - priority on groin since adductors help landing', 'Spaces for timeseries graph', '', 'Hamstrings', 'Spaces for timeseries graph', \"1st song group reading since it's been a week\", 'Spaces for timeseries graph', 'Frustratingly makes javascript-esque sense', 'R BS reverts, R BS 180s, L ollies', 'Shoulder stretches, pushups', 'Hamstrings, tfl', 'Almost finished graph, todo print the first value line', '', 'R BS 180 revert, R BS 180- R heel goes right more on land, L FS 180 revert', 'Pichael and handstands. Handful of nonconsecutive handstand pushups.', 'Groin and hip flexors', 'Exercise/high fat, exercise, improved myelin sheath. Fat reduced sheath.', 'BWV 565, 999', 'Groin, lower leg lateral muscles', 'Dropped guitar and hot af in room made for exhausting tuning issues', 'Threw tennis ball, P1 moved to prediction/observation spreadsheet', 'Pectoralis and shoulders', '', 'BWV 565, 999 1 & 2 site reading, 1014 from memory (5-6 major problems)', '9 or 10 5 ball!', 'R L manual pose hard surface', 'R L manuals, R BS 180 revert, L FS 180 revert', 'Abductors and hamstrings', '5-7 flashes 5 ball. Manual poses.', '5 ball flashes (post two beers)', 'Hamstrings, shoulders', '999, 1014, 565', 'Mainly curb drops no tricks', 'Verilog is a hardware langauge to write basic functions for Assembly. Raspberry pi CPU datasheet, taking notes on the first 100 pages of Neuro book', 'R FS 180 reverts, L BS 180 reverts', 'Leg raises lo mid hi and obliques, close mid wide pushups, planks, handstands', 'Side splits and variations, hamstrings', 'Neuro book notes', '7-8 5 balls, 3 ball hand and feet', 'Repurposed board', 'Finished refractoring', 'Walked and juggled 3-4 balls, 3 ball foot/hand juggle, stationary 5-8 5 ball', 'BWV 1013, 1001, AL', 'Graph ascii txt', '', 'Foot juggling', '~3 miles', 'BWV 1014, matched rap samples', 'Neck, traps, shoulders, hamstrings, calfs', '~3 miles', 'Foot juggling', 'Handful of reverts', 'Hip flexors and hamstrings', 'Foot juggling', 'Music list', 'Music list with other societal rants', 'Society rants', '.5 miles also foot juggle', '3 ball foot hand and 5 ball practice', 'society.', 'BWV 1004 review', 'Massaged ankles, calfs, thighs', '1004 60-64, 95-96, 108, 119, 140-146, 180-183, 205-207, 221-226, 235-249', 'LR FSBS 180 revert, R BS 180', 'Abductors, hamstrings', 'Abductors, hamstrings', 'Metadata of short form video blog posts from skateboard and music notes', 'Solidly fast high ollies and FS 180s. BS 180 is hard.', 'Hamstrings especially right bicep femoris', 'Lateral muscles and hamstrings', 'BWV 1004 60-64, 95-96, 108, 119, 140-146, 180-183, 188-189', 'Not real text data to do lexicon word analysis in the python statistics', 'BWV 180-183, 188-189 I learned these sections intoxicated and the rest sober and can only play them in those states so im relearning these sober.', 'Metadata', '3 ball foot/hand, 5 ball flashes -> 7 and 8 throw collides I think the left hand doesnt throw the 8th ball horizontally enough', '1004 60-64, 95-96, 108, 119, 140-146', 'Save graphs as txt without risk dying after making a bad img decoder with my face in virus porn', 'Bad stationary L FS 180, RL pop shuv, front shuv', 'Front splits, side splits, laying down hamstring raise/abductor stretch, calfs, shoulders', 'BWV 1004 180-183, 188-189, 205-208', 'Low level programming and how statistics are abused with psychology', 'BWV 1004 188-189, 205-208', 'Off topic rant on more hypothetical science and technology issues', '1.5 miles', 'Front lunge, side splits, laying down hamstring raise/abductor stretch, calf, shoulder', '7 or 8 catch 5 ball, i ended up with 3 balls in left hand I think a left ball went straight up', 'BS hippie jump', 'BWV 1004 188-189, 205-208', 'Finished hypothetical functions and rants. Combined most of the docs', '3 ball hand/foot', 'Goal: 2 BS turns/nose revert/hippe jump, 1 moving/non ollie, land 3 moving/non BS 180. Left foot is next to incline, right foot is slightly forward on tail than i think it should be', 'Handstand pushups, alt grip pushups, pulling exercises', 'Front/side lunge, abductor stretch. Almost can do front splits, new goal is to do side splits to increase adductor strength', 'BWV 1004 215-242', 'BWv 1004 1/2 tempo, post triplets-end 2-3 errors', 'Laying down leg raise/cross, chest/shoulders', 'Organize the materials into one document.', '3 ball hand and foot', 'BS 180 10/35 -> back foot is off too much it doesnt land, too on board doesnt rotate 180', 'Front/side lunge, hamstrings, abductors', 'Tried all of 04, 5-7 places that were not good', 'Laying down leg raise/cross, chest/shoulders', 'Chord improv', 'BWV 1004: 60-64, 95-96, had issues with 140-146', 'BWV 1004: 221-226, 233-236 248', 'Organize the materials into one document.', 'L ollies, FS revert, FS 180s', 'Rubbed muscles, stretched hamstrings', 'BWV 04 221-end', '~3.5 miles throwing ball left arm', 'Laying down leg raise/cross, hamstrings, chest/shoulders', 'BWV 04 140-end', 'First semester outline', 'BWV 04 220-end', '', '10-15 minute warmup not included, L ollies, FS 180 reverts, FS 180. R leg tired. Butt landed primo', 'Side lung, hamstrings, mule kick pose increase blood to butt bruise', 'Laying down leg raise/cross, hamstrings, chest/shoulders', 'Began second semester outline', 'BWV 04 220-256, 139-256 7 bad notes 3 tempo slow downs', '~1 mile', 'Abductors and adductors', 'L hippe jumps, 180 varial, FS turns, FS 180 reverts', 'Side/front lunges, hamstrings/calfs, chest/shoulders', 'Second semester outline', '3 ball, lateral stairs, hand/feet', 'L hippe jumps, 180 varial, FS turns, FS 180 reverts L lower abdominal, L groin, R abductors are sore. L quadratus from mule kick posing.', 'Side/front lunges, hamstrings/calfs', 'Finished correlation section', 'Bwv 04 140-end 11 errors', 'Hamstring, calf, shoulders, cross legs', '~2.5 miles', 'For my health', 'Vastus lateralis twitch while leg is raised probably from overuse and dehydration', 'Sleep issues from low exercise. Hypothesized about python/C feature summarizations.', 'Edited python ramblings.', '', '~1 mile fast and 1 mile slow. Revealed muscular soreness. Concentrated on gait symmetry while tired. Healed R lateral tendon sprained caused heel click doesnt happen when ankle is actively flexed, which also makes sure ankle isnt rotating outward while running. Or its from shot put spin. Side lunges revealed groin and adductor soreness. Might not skate for a few days.', 'Organized github repository', 'Juggle walked hands/feet 3 ball', 'bwv 565 1-8', 'More random thoughts', 'bwv 565 most of page 1, sight reading 2-16', '~3.5 miles', 'Editing notes', '565 page 1', '.4 miles', '565 page 2', '~2.0 miles', 'Cousin Wedding', '', 'L FS 180 body variel, FS revert, Hippie Jump', 'Paragraph notes', 'Article notes', '~2 miles, walked 1.5 miles', '', '565 page 1 & 2', 'Class outline', 'R BS 180', 'Varying width pushups, handstands, tricept/bicept, core, shoulder, pecs, calf, hamstring, back', '', '', '565 page 1, 2', '565 p 1, 2', 'arms, hamstrings, lunges, abductors', 'Turned off air, put steaming water in living room', 'Steaming water in south upstairs bedroom', 'Dry house, moisturize or acne and brain rot', '', 'HVAC discharged heat into bedroom converted from former attic. Turned air on, laid in basement', 'Probably phone, went to the bathroom', 'Parents being loud', 'Decided to skip Skateboard paper and skateboarding. New strategy to avoid 7-20 day recovery', '565 page 1, 2', 'Riverside', '~3 miles, walked 1 mile', 'Abductor and neck muscles, mainly cooled down', '', '', 'Shoulders', '', '', 'Quadratus lumborum and hamstrings', '', 'Dorsal scapula nerve entrapment, stretched scalene and rhomboid.', 'Finished second class, started graph third class', 'Shoulder, neck, trapezious', '1 mile', 'BWV 565 page 1-2 and all', '2,3,4,5 ball. Over/under hand throws.', '1.5 miles, walked .5 miles', 'Slow chair tricep dips to stretch pinched neck', 'BWV 565 page 1-2', 'Finished Chapter 3. todo edit', '.5 miles', '.5 miles', '1.2 miles', '1.2 miles', '2 miles', 'Hamstrings/arms', 'BWV 565', 'Started editing Chapter1, decided to start Chapter 0', 'Very tired, bs180s kept dragging front landing toes', 'Hamstrings, shoulders, core, adductors, abductors', 'Varrying height leg raise variations', 'Slowly raised leg up/down. Slowly moved arms in circle. Laid on ground and observed muscle pain.', 'Chapter 0', '1 mile', '565 page 3-4', '2.5 miles', 'While laying down, slowly raised leg and LR lateral hold. Slowly moved arms in circle.', 'Chapter 0', '565 3-4', 'While laying down, slowly raised leg and LR lateral hold. Slowly moved arms in circle.', '2.5 miles', 'Nose turns, nose tick tacks, nose manuals, nollies, several long R manuals. 4 miles', 'Shoulder movements, hamstrings', 'BWV 565 3-4', 'Jazzing. 565 into page 5', 'While laying down, slowly raised leg and LR lateral hold amd rubbed legs/feet for 25 minutes.', 'BWV 565 3-4, noodling', 'BWV 565 3-4', '2.5 miles', 'BWV 565 3-4', 'BWV 565 1-6', '1 mile', '1 mile', 'BWV 565 5-6', '1 mile', 'Neurology Unit 2 5th chapter', '15 minute warmup, R BS 180s, very high L FS 180s, R moving pop shuv', '~7.2 miles, 3 block slowest run and 2 block cool down', 'Calfs, shoulders, feet, hamstrings. LR lateral, dorsal, proximal laying on basement cement to dispell excessive nerve and muscular electricity.', 'Head, shoulders, knees, toes. Laid on ground and stretched calf', 'BWV 565 5-6', 'Chapter 0 explained what a int64', 'Abductors and shoulders', 'BWV 565 5-6', 'BWV 565 5-6', 'BWV 565 5-6', 'One truck expands more with heat. When cold (and humidity?) bearings are loose and stick more', 'Tired L adductor and RL ankles and feet bad ollies. Landed several moving smaller pop shuv.', '~7.2 similar to Wednesday', 'Mainly stood and abductors', 'Chapter 0 operations and conditionals. Started iterations', 'BWV 565 5-6', 'Edited ch4 data.', '~2 miles', 'BWV 1-5', 'BS 180 hippie jumps', '~3 miles 4 block cool down', 'Neurology', 'BWV 565 5-6', 'BWV 565 5-6', '', 'BWV 565 1-6, jazz', 'BWV 565 6', '1 mile', 'L Stationary BS 180, FS 180, worked on hard turns', '3 miles, slow 3 blocks, walked 2 blocks', 'Stood outside, shuffled feet', 'BWV 565 1-6, 6-7', 'Neurology', '2, 3 hand foot, 4, 5', 'BWV 565 1-7', 'Neurology finished chapter 5', 'BWV 565 p. 6', 'Neurology - notes chapter 1', '1 mile', 'Tic tocs. Learned about projectors again.', '3 hand foot, 4', '.75 miles. Learned about projectors again.', 'Tic tocs', '565 p 6-7', 'L downhill. Worked on speed and slow heel standing slides.', '565 p 6-7', '565 p 6-7', '3 hand foot, 4', '1 mile', 'Finished chapter 1 neurology notes and chapter 0 outline', 'BWV 565 p 7', 'BWV 565 p 7', 'Started editing chapter 0-3 and making a pdf', 'Rear wheel 180 slides and body variels. R vastus lateralis is sore.', '~2.8 miles, slow run 2 blocks, 4 block walk', 'BWV 565 p 7', 'Edited chapter 0. Started Chapter 1.', 'BWV 565', 'Rewatched crailtap channel (Girl, Chocolate, etc)', 'BWV 565 p 7', \"Rewatched lurcNYC rehash of 90's cctv footage crossed with park skateboard computer imagined\", 'BWV 565 p 7', 'BWV 565 p 8', 'Neurology - notes chapter 2', 'lurkNYC -> watch tutorials for 15 minutes to an hour and is not written usually', 'Finished editing chapter 1. Started chapter 2.', 'BWV 565 p 8', '.75 miles', 'Push ups, hand stands, tricep/bicep, bosu ball squats, plank twists', 'BWV 565 p 8', '~5 miles on fast wheels', 'Yoga ball pichael and raises, pull ups. Stretched calfs, abductors, lunges, shoulders.', 'BWV 565 p 8', 'BWV 565', 'Finished edit chapter 2. Started chapter 3. Chapter 4 stil have to read 20 articles ~4-5 weeks', 'L downhill heel slides for fs 180', 'BWV 565 p. 1-9', 'Pushed L backwards (for R BS 180 landing), L manual, tail stops, sharp turns', 'Bulgarian squats, plank variations, hamstrings, calfs.', 'BWV 565 p. 9', 'L downhill heel slides for fs 180', 'BWV 565 p. 9', 'BWV 565 p. 9', 'Pushed R backwards (for L BS 180 landing), L manual, tail stops, sharp turns', 'Yoga ball pichael and bosu ball adductor squats. Stretched calfs, adductors, lunges, shoulders.', 'Right leg 180, 360 spins, L FS 180 into curb no board, stretch abductors', 'Rubbed legs and lower back until perspirating.', 'L downhill heelslides old concrete. Tried the new pavement section and had issues.', 'BWV 565 p. 9', 'Abductors. Spun around R in a circle for 15 minutes.', 'Abductors', 'BWV 565 p. 9', 'RL Ollie, FS 180, R BS 180, R struggle pop shuv', 'Abductors', '1 mile', 'BWV 565 p. 1-11', 'Abductors. Spun around R in a circle for 15 minutes.', 'BWV 565 9-10', 'Pushed LR backwards (BS 180 landing), manual, sharp turns', 'Spun R in circle', '2 miles', 'Abductors. Walking related.', 'BWV 565 9-10', 'BWV 565 8-10', 'Pop shuv it', 'Abductors. Spun around R in a circle for 15 minutes.', 'Pushed LR backwards (BS 180 landing), manual, sharp turns', '1.5 miles', 'Spun R in circle for 15 minutes arms T pose', 'BWV 565 8-10', 'BWV 565 8-10', 'R Pop shuv its', 'BWV 565 8-10', 'Spun R in circle for 15 minutes bent knees, arms/head down. Vastus medious and intermedious', 'Practiced L heelslides. OK at stopping but harder to slide and continue downhill.', 'Push-ups, triceps dips, bent rows', 'Concentrated on abductor and adductor vastus.', 'Spun R in circle for 15 minutes bent knees, arms/head down. Vastus medious and intermedious', '2.5 miles', 'BWV 565 9-11', 'Practiced L heelslides. Improved faster heelslides on fast pavement.', 'BWV 565 9-11', 'Foot/hand juggled rugby ball', 'Fast run .8 miles, slow .8, walk .4', 'Bosu ball adductor squats, phicheal, various leg raise crunches', 'Abductor, adductor, hamstring, vastus intermedialis, shoulders, back', 'BWV 565 11-12', 'Longer heelslides. Keep falling backwards on steeper incline.', 'BWV 565 11-12', 'BWV 565', 'Shoulders, hamstrings, abductors, calfs', 'Handful of hippie jumps, 5 miles', 'Rubbed feet, heel, calfs, tibia, quads', 'BWV 565', '1 miles', 'Abductors', 'L ollies worked on keeping left leg on tail. Handful of stationary L FS 180.', '3.8 miles', 'Spun R in circle for 15 minutes bent knees, arms/head down.', 'BWV 565 10-17', 'Heelslides, still falling backwards on long slides', 'BWV 565 10-12', 'BWV 565 12-13', 'Spun right in circle on left foot (R BS 180)', 'Cleaned board', 'BWV 565 12-13', 'L ollies tail foot. L FS 180s.', 'Cleaned longboard', 'BWV 565 12', '1 mile', '1 mile', 'Abductors', 'BWV 565 12', '1 mile', 'BWV 565 12-13', '1 mile', 'Abductors', 'Adductors', 'In office chair, one leg stomach height on bed, other leg raises for upper middle part vastus intermedialis. Also did bent knee drag from that position for the same muscle, groin, lower core. Quad feels good or it might be the higher ragweed this time of year.', 'BWV 565 12-13', 'L ollie and FS 180', 'Push-ups, handstands, triceps dips, bent rows, plank lateral shuffles, leg raises', 'BWV 565 12-13', 'Kicked rugby ball L foot', '5 miles', 'Vastus intermedialis, medialis, and hamstrings', 'Quad, groin, and hamstrings', 'BWV 565 14-16', 'Quad, groin, and hamstrings', 'Kicked rugby ball L foot', '5 miles', 'Quad, groin, and hamstrings', 'Quad, groin, and hamstrings', 'BWV 565 14-16', 'BWV 565 14-16', 'L rugby kicks', '2 miles', 'Abductors, side splits, hamstrings', 'BWV 565 14-16', 'L rugby kicks', '3.5 miles', 'Abductors, front/side lunges hamstrings', 'BWV 565 14-16', 'BWV 565 14-16', 'Spin in a circle on toes, abductors', '1 mile', 'Abductors, front/side lunges hamstrings', 'BWV 565 14-16', 'Abductors, front/side lunges hamstrings', 'BWV 565 14-16', 'Spin in a circle on toes, abductors', '1 mile', 'Abductors, front/side lunges hamstrings', 'BWV 565 14-16', 'Abductors, front/side lunges hamstrings']\n","['Notes', '', '', 'Switch .75 kickflip and switch old kickflip', 'L manual, R BS revert, hippie jumps, dry look at object/ollie, 4 pop shuv no land', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'After 2 hours after 4 beers', '', '', '', '', '', '', '', '', '', 'R pop shuv are getting consistent, hopefully the kickflip will be good in a month.', '', '', '', '', 'Lose balance left side on handstands and bad L jumps/balance', '', '', '', '', '', '', '', '', '', 'This is my second broken board and 5th board overall.', '', '', '', '', 'Larger wheels, trucks, and wider board and my L bicep femoris is sore.', '', '', '', '', '', '', '', '', 'Also read about dietary issues and insomnia', '', '', '', '', '', '', '', '', '', 'Carpet is pointless for landing but develops muscles. 7.75 board hard to land on.', 'Normally not recorded but cooking/washing takes 30-90 minutes everyday.', 'Saturated fat diet and dry skin bad for myeline.', '', '', '', 'PH: Last 45-60 days before needing to take 1-2 weeks off.', '', 'Otherwise energy bounces into your extended and loosely contracted legs', '', '', 'Measured and made two truck holes. I am very bored.', '', '', '', '', '', '', '', '', '', '', '', 'That wont induce paranoia and will cut down on wasteful government spending.', '', 'Coded moment function', '', 'Abdominal muscles already hurt', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Bending as dynamic stretch and juggling increases coordination', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'The other part is built up static electricity in the AC', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n","['ord_list', 83, 71, 83, 83, 83, 83, 76, 74, 71, 83, 76, 74, 71, 83, 87, 76, 83, 71, 83, 83, 74, 71, 83, 83, 74, 82, 82, 74, 83, 67, 83, 82, 71, 82, 74, 83, 67, 83, 83, 87, 76, 67, 83, 83, 74, 71, 82, 83, 71, 78, 83, 83, 87, 82, 71, 83, 83, 71, 87, 74, 83, 67, 83, 71, 82, 76, 82, 83, 76, 83, 71, 83, 87, 71, 83, 67, 82, 71, 83, 74, 83, 71, 83, 83, 74, 83, 74, 83, 83, 83, 67, 83, 71, 82, 66, 74, 71, 87, 71, 83, 71, 76, 83, 71, 83, 76, 83, 71, 83, 83, 83, 83, 83, 83, 83, 83, 82, 83, 83, 71, 83, 83, 83, 67, 83, 83, 83, 83, 67, 83, 82, 71, 83, 71, 87, 83, 74, 71, 74, 83, 76, 83, 74, 74, 83, 71, 76, 82, 76, 67, 83, 82, 74, 76, 83, 74, 71, 83, 76, 74, 87, 71, 83, 87, 74, 76, 83, 74, 87, 87, 87, 87, 74, 87, 71, 83, 71, 76, 83, 83, 83, 83, 83, 83, 71, 83, 71, 83, 74, 71, 83, 83, 83, 71, 83, 71, 83, 87, 83, 74, 76, 71, 83, 74, 83, 67, 83, 71, 71, 83, 83, 74, 83, 83, 71, 83, 71, 71, 71, 83, 83, 83, 71, 87, 83, 71, 83, 71, 87, 83, 83, 83, 83, 71, 87, 83, 76, 83, 83, 74, 76, 83, 83, 71, 83, 87, 83, 69, 83, 83, 68, 82, 83, 87, 71, 83, 71, 82, 83, 71, 87, 71, 87, 68, 68, 76, 83, 83, 82, 67, 71, 83, 83, 67, 78, 82, 71, 71, 83, 66, 66, 76, 83, 87, 87, 87, 69, 71, 87, 82, 83, 83, 69, 83, 78, 82, 83, 69, 83, 83, 83, 87, 71, 74, 82, 83, 71, 83, 87, 87, 87, 87, 82, 83, 71, 83, 83, 83, 67, 83, 83, 87, 71, 87, 83, 83, 71, 83, 87, 76, 83, 71, 71, 83, 71, 71, 87, 71, 71, 87, 87, 71, 76, 82, 83, 82, 83, 83, 71, 83, 83, 71, 71, 71, 83, 83, 82, 83, 83, 71, 83, 87, 71, 76, 82, 82, 71, 71, 72, 71, 71, 87, 83, 82, 67, 71, 82, 74, 71, 82, 71, 82, 87, 76, 74, 87, 76, 71, 76, 71, 71, 74, 87, 83, 71, 71, 83, 76, 82, 71, 83, 71, 83, 71, 83, 71, 71, 82, 83, 83, 71, 87, 67, 71, 76, 67, 71, 71, 83, 76, 71, 76, 67, 71, 76, 71, 71, 76, 67, 80, 83, 76, 71, 83, 83, 71, 83, 83, 87, 71, 83, 71, 76, 83, 87, 83, 71, 71, 83, 83, 76, 87, 83, 71, 71, 83, 71, 83, 76, 67, 83, 83, 87, 71, 76, 71, 87, 82, 67, 83, 71, 76, 71, 71, 83, 76, 83, 71, 87, 83, 83, 87, 83, 71, 76, 71, 71, 83, 83, 71, 83, 83, 71, 87, 87, 83, 71, 87, 71, 87, 83, 83, 83, 71, 83, 67, 71, 87, 76, 83, 83, 71, 83, 87, 76, 83, 83, 71, 71, 87, 76, 83, 71, 87, 76, 83, 71, 71, 83, 87, 83, 71, 83, 71, 83, 87, 83, 71, 83]\n"]}]},{"cell_type":"code","source":["## |a = 'abcdefg'\n","'''\n","for i in range(1,len(a)):\n","  print(a[:i])\n","  print(a[i])\n","  print('zzzzz')\n","\n","  print(a[-1])'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"nyadd2nCLeqQ","executionInfo":{"status":"ok","timestamp":1744989152983,"user_tz":300,"elapsed":19,"user":{"displayName":"David Leifer","userId":"06279506333224389759"}},"outputId":"6c2810b5-68e0-40fb-aa98-bf24388e05dc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nfor i in range(1,len(a)):\\n  print(a[:i])\\n  print(a[i])\\n  print('zzzzz')\\n\\n  print(a[-1])\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["abc = [\n","        [[0], ['a']],\n","        [[1], ['b']],\n","        [[2], ['c']],\n","        [[3], ['d']],\n","        [[4], ['e']],\n","        [[5], ['f']],\n","        [[6], ['g']],\n","        [[7], ['h']],\n","        [[8], ['i']],\n","        [[9], ['j']],\n","                      ]\n","# for i in range(10):\n","print(abc[1:2])"],"metadata":{"id":"SLyAYj_xtzdg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744137032714,"user_tz":300,"elapsed":6,"user":{"displayName":"David Leifer","userId":"06279506333224389759"}},"outputId":"3bb4744a-366f-4183-a0ca-7f695d31af9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[1], ['b']]]\n"]}]},{"cell_type":"code","source":["a = '1234'\n","b = '567'\n","c = '89'\n","\n","\n","if len(a) == 4:\n","  end_sub = a[2:]\n","  print(end_sub)\n","\n","if len(b) == 3:\n","  end_sub = b[:1]\n","  print(end_sub)\n","\n","if len(c) == 2:\n","  end_sub = c"],"metadata":{"id":"kti4uK-eVPeZ","executionInfo":{"status":"ok","timestamp":1744769169351,"user_tz":300,"elapsed":9,"user":{"displayName":"David Leifer","userId":"06279506333224389759"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1c1bc210-f9b0-4b10-b49e-62551d97b986"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["34\n","5\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"bNU5wcEd0AeC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6wounMFJaZT3"},"execution_count":null,"outputs":[]}]}