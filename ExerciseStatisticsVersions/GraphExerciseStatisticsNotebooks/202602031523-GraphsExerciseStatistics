{"cells":[{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":276,"status":"ok","timestamp":1770153525165,"user":{"displayName":"David Leifer","userId":"06279506333224389759"},"user_tz":360},"id":"IxtfVwAjhyaV"},"outputs":[],"source":["def todo(self):\n","  # List of primary issues\n","  # todo    description                                   hours   progress      Note\n","  #\n","  # todo0   A01.csv skate, long, downhill, juggling,      83      Completed\n","  #         running mean duration by category.\n","  #\n","  # todo1   A01.csv category by day of the week or        .25     DNF           Hours are spread throughout the day.\n","  #         time of day i.e. morning, afternoon, night\n","  #         or blocks of 3.\n","  #\n","  # todo2   A01.csv nltk the 'Explanation' and 'Notes'                          Word frequency might be useful to find specific muscles.\n","  #         sections? Manual descriptions are already                           Topic analysis is included in 'Activity'. Sentiment analysis is\n","  #         included in the write-up.                                           redundant since 'Notes' is informational and not opinion.\n","  #\n","  # todo3   Another tutorial chapter on merge sort.               TODO\n","  #         Compare with Python's built-in len(),\n","  #         sort(), and replace().\n","  #\n","  # todo4   The graphing part could be included in        .25     DNF           This is a good project to learn syntax and documentation since it's visual.\n","  #         Chapter 1 with pandas and SciPy.\n","  #\n","  # todo5   A01.csv longboard and running distance.       .5      DNF           Running occured around 5 times and longboarding was recorded with time.\n","  #\n","  #\n","  # todo6   P0P1B0B1.csv timeseries graphing.             45.5    Completed\n","  #\n","  # todo7   P1.csv manual vs observed prediction          46.5    Completed\n","  #         accuracy F1.\n","  #\n","  # todo8   B01.csv Pearson-Correlation and day-          2       Completed     Found that there was not correlation between parametric variables.\n","  #         delayed between calories, alcohol, exercise.                        An index similar to ENSO is redundant since there were no consecutive\n","  #                                                                             observations over 4 alcohol or excessive (calorie - calorie burned).\n","  #\n","  # todo9   A01P01B01 moving window spearman              .5      DNF           Would have to sort these for rank, which was completed in todo0.\n","  #         correlation between activity, duration,                             Square the difference between each numbers rank and sum all the numbers,\n","  #         time of day, pain, nutrients,                                       multiply by 6, divide by (number times (number squared minus one).\n","  #         calories, alcohol.                                                  1 - calculated number.\n","  #\n","  # todo10  tbd data filling and automatic predictions.   0       DNF\n","  #         idk if thats another chapter or avoided.\n","  #\n","  # todo11  Manual weather observations and PRISM data    0       DNF\n","  #         will be in a different GitHub to avoid\n","  #         confusion.\n","  #\n","  # Time spent at a computer programming\n","  # Total estimate  :\n","  # Total actual    :\n","  #\n","  # Purpose\n","  # The goal of writing this is to waste as much time as possible in between\n","  # exercise to avoid overtraining while retaining logical thought process\n","  # during long stretches of unemployment. These were written on a computer\n","  # with a 1.5-2 hour battery to restrict excessive\n","  # programming by limiting hardware access.\n","  #\n","  # Abstract\n","  # Python with C-like syntax is used for data manipulation and\n","  # graphing arrays are handled without dictionaries. The only\n","  # library used is Matplotlib for RGB graphing and to avoid writing a image or\n","  # video format that would likely spread misinformation. An implementation of\n","  # the merge sort algorithm was used to alphabetize exercise activity for\n","  # binning and graphing frequency by unique type. The built-in Python methods\n","  # for 'replace', 'split', 'len', and 'sort' were manually written for\n","  # learning purposes.\n","\n","  # Start date: 20250125\n","  # End date:\n","\n","  # Below is an exhaustive list of secondary issues.\n","\n","  # List of secondary issues\n","  # todo   description                                                  progress\n","  # todo0  rewrite parser for unicode csv str/int.\n","  # todo1  Stats class avoid NA, NAAN, -9999, etc.\n","  # todo2  refractor RGB_graphs.\n","  # todo3  monthly means on bar graphs.\n","  # todo4  organize merge_sort into another classe.                     Complete\n","  # todo5  modify merge sort to accept entire CSV.                      Class\n","  # todo6  Handle multi word activity descriptions consistently.        Class\n","  # todo7  switch the second capital letter to lower case if exists.    Class\n","  # todo8  unchain the four merge sort functions.                       Class\n","\n","  return\n","\n","# import sys\n","# for path in sys.path:\n","#   print(path)\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import exercise_module as eu\n","# eu.test_function()\n","# print(sys.version)\n","# 3.10.12 (main, Mar 22 2024, 16:50:05) [GCC 11.4.0]\n","# 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n","# 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n","\n","# In development.\n","###############################################\n","# Part D: Data visualization with a RGB graph #\n","###############################################\n","# Matplotlib for color because otherwise you would have\n","# to write hardware code to avoid using Python or C libraries.\n","class Graphs_rgb: # 'Graphs_rgb_dev()' in 'exercise_module.py'\n","  # Initialize the input variables\n","  def __init__(self, data):\n","    self.data = data\n","\n","  # A1.csv calories out per activity\n","  # TODO PLACE AT BOTTOM\n","  def rgb_timeseries_calories_out(self,data):\n","    # average daily\n","    # rolling 5 day average\n","    return\n","\n","  # Mean of all days using the daily mean by 3,5,7,...etc. before pain.\n","  def rgb_reverse_category(self,pain,not_pain,all_data):\n","\n","    x = [i[1] for i in pain]\n","    y1 = [j[0] for j in pain]\n","    y1n = [j[2] for j in pain]\n","\n","    y2 = [k[0] for k in not_pain]\n","    y2n = [k[2] for k in not_pain]\n","\n","    y3 = [l[0] for l in all_data]\n","    y3n = [l[2] for l in all_data]\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.scatter(x, y1, label='Pain', marker='+')\n","    plt.scatter(x, y2, label='Not Pain', marker='_')\n","    plt.scatter(x, y3, label='All Data', marker='.')\n","    plt.xlabel('Days in Reverse', fontsize=10)\n","    plt.ylabel('Mean Daily Activities', fontsize=10)\n","    plt.title('Activites Before Pain',fontsize=12)\n","    for iter, mean in enumerate(y1):\n","      if mean > 6.5:\n","        bx_pos = x[iter] - 1.5\n","        by_pos = mean - .1\n","      else:\n","        bx_pos = x[iter] + 1.5\n","        by_pos = mean + .1\n","      box_str = 'N. ' + str(y1n[iter])\n","      plt.text(\n","          bx_pos,\n","          by_pos,\n","          box_str,\n","          ha=\"center\",\n","          va=\"bottom\",\n","          size=8,\n","          bbox=dict(facecolor=\"white\", edgecolor=\"black\", boxstyle=\"round\", alpha=0.75),\n","      )\n","    plt.xticks(x)\n","    plt.ylim(5.1,6.75)\n","    plt.grid()\n","    plt.margins(y=0.01)\n","    plt.legend(loc='upper center')\n","    plt.show()\n","\n","    # plt.savefig('P1-0 Activity Frequency Mean July-October, 2024' + '.jpg')\n","    return\n","\n","  # Prediction vs observation and accuracy, precision, F1 score.\n","  # Input is prediction, observation, date, classification scores.\n","  def rgb_prediction_observation(self,prediction,observation,date,classification_list):\n","    print(observation)\n","    x = []\n","    y1 = []\n","    y2 = []\n","    y3 = []\n","    # Filter 'NA'. Similar to 'rgb_date_time()' but the function is used in\n","    # a loop instead of the entire list of columns and doesn't average every 5 days.\n","    for j in range(1,len(prediction)):\n","      if len(date[j]) == 3:\n","        month = date[j][0:1]\n","        day = date[j][1:]\n","      else:\n","        month = date[j][0:2]\n","        day = date[j][2:]\n","      split_date = month + '/' + day\n","      if prediction[j] == 'NA' or observation[j] == 'NA':\n","        y1.append(None)\n","        y2.append(None)\n","        y3.append(None)\n","        x.append(split_date)\n","      else:\n","        if prediction[j] == observation[j]:\n","          #print(prediction[j],observation[j],date[j])\n","          y1.append(int(prediction[j]))\n","          y2.append(None)\n","          y3.append(None)\n","          x.append(split_date)\n","        else : # prediction[j] != observation[j]\n","          y1.append(None)\n","          y2.append(int(prediction[j]))\n","          y3.append(int(observation[j]))\n","          x.append(split_date)\n","\n","    # Plots the three datasets on the same graph.\n","    plt.figure(figsize=(11, 5), dpi=400)\n","    plt.scatter(x, y1, label='Same', marker='.')\n","    plt.scatter(x, y2, label='Prediction', marker='o', s=100)\n","    plt.scatter(x, y3, label='Observation', marker='o')\n","    # The 'y' string removes '0' otherwise it runs into the xaxis.\n","    x_label = [i for i in x[::10]]\n","    y_label = [0,1,2,3,4,5]\n","    y_label_str = [str(y) for y in y_label]\n","    y_label_str[0] = ' '\n","    plt.xticks(x_label, labels=x_label, fontsize=17)\n","    plt.yticks(y_label, labels=y_label_str, fontsize=17)\n","    # Stamina text box and legend is positioned different since stamina was\n","    # normally high. 'bx_pos' is used for both.\n","    bx_pos = 77.5 # x[-5]\n","    if prediction[0] == 'Stm':\n","      by_pos = 1.91\n","      plt.ylabel('Stamina', fontsize=19)\n","      plt.legend(loc='lower left',fontsize=17,borderaxespad=1.2)\n","    else:\n","      by_pos = 4.5\n","      plt.ylabel('Pain', fontsize=19)\n","      plt.title(classification_list[0], fontsize=19)\n","      plt.legend(loc='upper center',fontsize=17,borderaxespad=1.2) # upper left except columns 8-11.\n","\n","    # Includes a manual prediction and accuracy scores within a text box.\n","    box_label = ['','','Accuracy: ', 'Precision: ', 'Recall: ', 'F1 Score: ']\n","    box_str = ''\n","    for k in range(2,len(classification_list)):\n","      box_subset_str = box_label[k] + str(round(classification_list[k],2))\n","      if k < len(classification_list) - 1:\n","        box_str += box_subset_str + '\\n'\n","      else:\n","        box_str += box_subset_str\n","    plt.text(\n","        bx_pos,\n","        by_pos,\n","        box_str,\n","        ha=\"left\",\n","        va=\"top\",\n","        size=17,\n","        bbox=dict(facecolor=\"white\", edgecolor=\"#d5d8de\", boxstyle=\"round,pad=0.5\", alpha=0.8))\n","    plt.grid()\n","    #plt.margins(y=0.1)\n","    return\n","\n","  # All the erroneous pain dates with each daily mean for 3,5,7..etc.\n","  def rgb_reverse_day_mean(self,input_data,P1_data):\n","    # x = [i[1] for i in input_data] # Day_ID\n","    # Return the date\n","    # Sort the 'input_data' chronologically and include the date.\n","    input_date = []\n","    input_data_id = sorted(input_data, key=lambda y:y[1])\n","    for j in range(len(input_data_id)):\n","      splice = input_data_id[j][1] + 1\n","      input_data_j = input_data_id[j]\n","      input_data_j.append(P1_data[1][splice])\n","      input_date.append(input_data_j)\n","    # Sorting 'input_date' based on the second value or the number of days in reverse.\n","    input_date_reverse = sorted(input_date, key=lambda x: x[2])\n","    y1 = []\n","    x1 = []\n","    reverse_days = [3,5,7,10,30]\n","    # 'y1' is a list of lists with the average activies per day as sorted by 'reverse_days'.\n","    for i in range(len(reverse_days)):\n","      y_input_day = []\n","      x_input_day = []\n","      for ii in range(len(input_date_reverse)):\n","        if input_date_reverse[ii][2] == reverse_days[i]:\n","          y_input_day.append(input_date_reverse[ii][4]) # input_date_reverse[ii]\n","          date = input_date_reverse[ii][5]\n","          if len(date) == 3:\n","            month = date[0:1]\n","            day = date[1:]\n","          else:\n","            month = date[0:2]\n","            day = date[2:]\n","          if day[0] == '0':\n","            day = day[1]\n","          split_date = input_date_reverse[ii][0] + '\\n' + month + '/' + day\n","          x_input_day.append(split_date)\n","      y1.append(y_input_day)\n","      x1.append(x_input_day)\n","\n","    plt.figure(figsize=(8,6))\n","    # y = [j[4] for j in input_data]\n","    # plt.scatter(x, y) # scatterplot version without sorting.\n","\n","    # 20250616-... Google Drive takes forever to update changes\n","    # to the spreadsheet, these are hardcoded.\n","    y1[1][7] = 4    # 20 /  5 instead of 21 / 5  = 4.2\n","    y1[2][5] = 4    # 28 /  7 instaed of 29 / 7  = 4.14\n","    y1[3][4] = 4.1  # 41 / 10 instead of 42 / 10 = 4.2\n","    y1[4][2] = 5.6  # 168/ 30 instead of 169/ 30 = 5.63\n","\n","    for sub in range(len(y1)):\n","      if reverse_days[sub] < 6:\n","        plt.plot(x1[sub],y1[sub])\n","      plt.scatter(x1[sub],y1[sub],label=reverse_days[sub])\n","    # plt.ylim(0,8.5)\n","    plt.text(\n","        6,\n","        4,\n","        box_str,\n","        ha=\"center\",\n","        va=\"bottom\",\n","        size=8,\n","        bbox=dict(facecolor=\"white\", edgecolor=\"black\", boxstyle=\"round\", alpha=0.75),\n","    )\n","\n","    plt.legend(title='Days Before\\n      Pain',title_fontsize=10,alignment='left',loc='upper left',fontsize=8,borderaxespad=1.2)\n","    plt.xlabel('Date (2024)', fontsize=10)\n","    plt.ylabel('Average per Day', fontsize=10)\n","    plt.title('Activities Before Pain')\n","    plt.grid()\n","    plt.margins(y=0.01)\n","    return\n","\n","  # Four utility functions daisy chained to rgb_timeseries_bar()\n","  # Minor todo: unchain them lol\n","  def rgb_timeseries_mean_p01(self,formatted_data_group):\n","    # Input is list (1-4) of lists (95) of each columns values without NA\n","    # i.e. [[dist1],[dist1],[dist1], etc]\n","    date_col_len = len(formatted_data_group[0])\n","    group_mean = []\n","    # Length of the column (95 without \"NA\" as filtered in rgb_date_time)\n","    for i in range(date_col_len):\n","      row_list = []\n","      # Length of columns to be summarized (1-4) 95 row_list values\n","      for j in range(len(formatted_data_group)):\n","        row_list.append(formatted_data_group[j][i])\n","      # Mean at each day for each group\n","      row_count = len(row_list)\n","      row_sum = sum(row_list)\n","      row_mean = row_sum / row_count\n","      group_mean.append(row_mean)\n","    return group_mean\n","\n","  def rgb_date_time(self,csv_groups,date_col):\n","    day_count = len(self.data[1])\n","    k = 0\n","    # Checks the n day average list.\n","    checker_list = []\n","    checker = []\n","    group_dist = []\n","    for i in csv_groups:\n","      dist0 = []\n","      dist1 = []\n","      dist2 = []\n","      dist1_sum = 0\n","      dist1_count = 0\n","      for j in range(day_count):\n","        if j == (day_count-1):\n","          break\n","        if i[j+1] == \"NA\":\n","          continue\n","        else:\n","          # Formatting the date, subtracts 'dist1_count' to format xaxis ticks.\n","          # year = 2024\n","          date_length = date_col[j+1-dist1_count]\n","          if len(date_length) < 4:\n","            month = date_length[:1]\n","            day = date_length[1:]\n","          else:\n","            month = date_length[:2]\n","            day = date_length[2:]\n","          date_format0 = month + \"/\" + day\n","          checker.append(int(i[j+1]))\n","          dist1_sum += float(i[j+1])\n","          dist1_count += 1\n","          date_format1 = month + \"/\" + day\n","          # Mean of every n days, change the n for desired values.\n","          if dist1_count == 5:\n","            dist1_average = round((dist1_sum / dist1_count))\n","            dist1.append(int(dist1_average))\n","            checker_list.append(checker)\n","            dist0.append(date_format0)\n","            dist2.append(date_format1)\n","            dist1_count = 0\n","            dist1_sum = 0\n","            checker = []\n","      group_dist.append([dist0,dist1,dist2])\n","      k += 1\n","    return group_dist\n","\n","  def rgb_P1_style(self,final_title,line):\n","    plt.yticks(range(1,6),fontsize=17)\n","    if final_title == 'Stamina':\n","      plt.ylabel(final_title + ' 5 Day Mean',fontsize=19)\n","    else:\n","      plt.title(final_title,fontsize=19)\n","      plt.ylabel(\"Pain\",fontsize=19)\n","      if line == 1:\n","        plt.legend(title='5 Day Mean', title_fontsize=19, fontsize=17)\n","    return\n","\n","  def rgb_B1_style(self,final_title,line):\n","    if final_title == 'Calories':\n","      plt.yticks(range(2000,3400,400),fontsize=17) # 1200,4500 for B1.csv\n","      plt.ylabel(\"Intake\",fontsize=19)\n","      plt.title(final_title,fontsize=19)\n","    elif final_title == 'Alcohol Servings':\n","      plt.yticks(range(0,6),fontsize=17) # 16 for P1.csv\n","      plt.title(\"Alcohol\",fontsize=19)\n","      plt.ylabel(\"Servings\",fontsize=19)\n","    elif final_title == 'Exercise':\n","      plt.yticks(range(0,3),fontsize=17)\n","      plt.title(final_title,fontsize=19)\n","      plt.ylabel(\"Calories Out\",fontsize=19)\n","      # plt.text(.1,.5, \"Calories Out \\n2 = 250+ \\n1 = 1-249\",\n","      #   bbox={'facecolor': 'white', 'alpha': .75, 'pad': 10})\n","    else:\n","      plt.yticks(range(1,6))\n","      if line == 1:\n","        plt.title(\"Nutrients\",fontsize=19)\n","      else:\n","        plt.title(final_title,fontsize-19)\n","      plt.ylabel(\"Intake\",fontsize=19)\n","      plt.yticks(fontsize=17)\n","      plt.legend(fontsize=17)\n","    return\n","\n","  # Bar plots for each column\n","  def rgb_timeseries_bar(self,title_full,start_val,P1_B1):\n","    for i in range(start_val,len(title_full)+start_val):\n","      formatted_csv_group = self.rgb_date_time([self.data[i]],self.data[1])\n","      fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(11, 5), dpi=400)\n","      ax.bar(formatted_csv_group[0][0], formatted_csv_group[0][1], width=0.8, align='edge', label='5 Day Mean')\n","      final_title = title_full[i-start_val]\n","      # Format the title, yticks, and ylabel\n","      if P1_B1 == 0:\n","        self.rgb_P1_style(final_title,0)\n","      elif P1_B1 == 1:\n","        self.rgb_B1_style(final_title,0)\n","      elif P1_B1 == 2:\n","        pass\n","        # self.rgb_A0_style(final_title,0)\n","      tick_positions = formatted_csv_group[0][2][::4]  # Get every n position\n","      plt.xticks(tick_positions, labels=tick_positions,\n","                 fontsize=17)\n","      plt.margins()\n","      # plt.legend(fontsize=17) # P1 specifies 5 day mean\n","      plt.grid()\n","      # break\n","      # plt.savefig(final_title + '.jpg')\n","    return\n","\n","  # Returns a date list without blanks\n","  def rgb_date_list(self):\n","    # date_literal is 0-30 days\n","    date_literal = []\n","    # Makes a list with only the dates\n","    for i in range(1,len(self.data[2])):\n","      if len(self.data[2][i]) > 0:\n","        date_literal.append(self.data[2][i])\n","    return date_literal\n","\n","  # Multiple lines same graphs.\n","  def rgb_timeseries_line(self,title_full,start_val,groups_num,title_label,P1_B1):\n","    data = self.data\n","    # secondary todo: name instead of number position\n","    j = 1\n","    # Adding multiple lines to a single plot by group with formatting\n","    for i in range(len(groups_num)):\n","      subset0 = groups_num[i:j][0]\n","      if subset0 == groups_num[-1]:\n","        break\n","      subset1 = groups_num[i+1:j+1][0]\n","      csv_groups = data[subset0:subset1]\n","      formatted_csv_group = self.rgb_date_time(csv_groups,self.data[1])\n","      # Format subplot\n","      fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(11, 5), dpi=400)\n","      # Get the formatted_csv_group second list of values in each group\n","      dist1_list = [dist1[1] for dist1 in formatted_csv_group]\n","      # First 3 columns in data are ID, while the title list isn't.\n","      # Subtract each subset by the start_val of the values (excluding date, id, etc)\n","      title_group = title_full[(subset0-start_val):(subset1-start_val)]\n","      # y = each dist1 in formatted_csv_group, x = every date value, x labels = every 5th date value\n","      for k in range(len(dist1_list)):\n","        ax.plot(formatted_csv_group[0][0], dist1_list[k], label=title_group[k], linewidth=4)\n","        # Format the title, yticks, and ylabel\n","        if P1_B1 == 0:\n","          self.rgb_P1_style(title_label[j-1],1)\n","        elif P1_B1 == 1:\n","          self.rgb_B1_style(title_label[j-1],1)\n","      # Chart formatting and save\n","      tick_positions = formatted_csv_group[0][2][::2]  # Get every n position\n","      plt.xticks(tick_positions, labels=tick_positions,\n","                 fontsize=17)\n","      plt.grid()\n","      plt.margins()\n","      #plt.savefig(title_label[j-1] + '.jpg')\n","      j += 1\n","    return\n","\n","  # Summarized with mean\n","  def rgb_timeseries_small(self,csv_groups_num,legend_label,ax):\n","    csv = self.data\n","    j = 1\n","    for i in range(len(csv_groups_num)):\n","      subset0 = csv_groups_num[i:j][0]\n","      if subset0 == csv_groups_num[-1]:\n","        break\n","      subset1 = csv_groups_num[i+1:j+1][0]\n","      csv_groups = csv[subset0:subset1]\n","      # Builds an array to skip NA and format the date\n","      # [[[dist0],[1],[2]],[[dist0],[1],[2]], etc]]]\n","      formatted_csv_group = self.rgb_date_time(csv_groups,self.data[1])\n","      # Get the formatted_csv_group second list of values in each group\n","      dist1_list = [dist1[1] for dist1 in formatted_csv_group]\n","      # Summarize each body part's group with mean\n","      dist1_group_mean = self.rgb_timeseries_mean_p01(dist1_list)\n","      # y = group mean, x = every date value, x labels = every 5th date value\n","      # Specified in rgb_date_time function\n","      ax.plot(formatted_csv_group[0][0], dist1_group_mean, label=legend_label[j-1], linewidth=4)\n","      # Chart formatting\n","      tick_positions = formatted_csv_group[0][2][::2]  # Get every n position\n","      plt.xticks(tick_positions, labels=tick_positions,\n","                  fontsize=17)\n","      j += 1\n","    return\n","\n","  # A01.csv frequency of merged 'Activity'.\n","  def rgb_timeseries_frequency(self,data):\n","    y = data[0][-11:]\n","    x = data[1][-11:]\n","    fig, ax = plt.subplots(figsize=(15, 17),dpi=400)\n","    bars = ax.barh(x,y,height=.5)\n","    for i in range(len(y)):\n","      plt.text(y[i]-2.5, x[i], str(y[i]), color='White', fontsize=17, ha='center', va='center', bbox={'facecolor': 'none','linewidth': 0})\n","    plt.title('Activity Frequency\\nJuly-October, 2024', pad=50, fontsize=19)\n","    # plt.title('Activity Frequency May, 2024', pad=50, fontsize=19)\n","    ax.tick_params(axis='both', labelsize=17)\n","    ax.tick_params(axis='y', pad=50)\n","    ax.xaxis.set_ticks_position('top')\n","    plt.yticks(x, labels=x, ha='center', fontsize=17)\n","    plt.grid(axis='x')\n","    plt.margins(y=0.01)\n","    # plt.savefig('A1-0 Activity Frequency July-October, 2024' + '.jpg')\n","    return\n","\n","  # A01.csv duration of merged 'Activity' hours.\n","  def rgb_timeseries_duration(self,data):\n","    y = [round(j) for j in data[0][-11:]]\n","    x = data[1][-11:]\n","    # x[5] = 'Guitar,\\nRest' # A0.csv\n","    fig, ax = plt.subplots(figsize=(15, 17),dpi=400)\n","    bars = ax.barh(x,y,height=.5)\n","    for i in range(len(y)):\n","      plt.text(y[i]-3, x[i], str(y[i]), color='White', fontsize=17, ha='center', va='center', bbox={'facecolor': 'C0','linewidth': 0})\n","    plt.title('Activity Duration\\nJuly-October, 2024', pad=50, fontsize=20)\n","    # plt.title('Activity Duration May, 2024', pad=50, fontsize=19)\n","    plt.xlabel('Hours', fontsize=17)\n","    ax.tick_params(axis='y', pad=50, labelsize=17)\n","    ax.tick_params(axis='x', labelsize=17)\n","    ax.xaxis.set_ticks_position('top')\n","    ax.xaxis.set_label_position('top')\n","    plt.yticks(x, labels=x, ha='center', fontsize=17)\n","    plt.grid(axis='x')\n","    plt.margins(y=0.01)\n","    # plt.savefig('A1-0 Activity Duration July-October, 2024' + '.jpg')\n","    return\n","\n","  # A01.csv mean of merged 'Activity' frequency / (sum of minutes)\n","  def rgb_timeseries_mean(self,data):\n","    y = [round(j,2) for j in data[0][-11:]]\n","    x = data[1][-11:]\n","    y.pop(0)\n","    x.pop(0)\n","    fig, ax = plt.subplots(figsize=(15, 17),dpi=400)\n","    bars = ax.barh(x,y,height=.5)\n","    for i in range(len(y)):\n","      plt.text(y[i]-.25, x[i], str(y[i]), color='White', fontsize=17, ha='center', va='center', bbox={'facecolor': 'C0','linewidth': 0})\n","    plt.title('Total Activity Average\\nJuly-October, 2024', pad=50, fontsize=20)\n","    plt.xlabel('Hours', fontsize=17)\n","    ax.tick_params(axis='y', pad=50, labelsize=17)\n","    ax.tick_params(axis='x', labelsize=17)\n","    ax.xaxis.set_ticks_position('top')\n","    ax.xaxis.set_label_position('top')\n","    plt.yticks(x, labels=x, ha='center', fontsize=17)\n","    plt.grid(axis='x')\n","    plt.margins(y=0.01)\n","    # plt.savefig('A1-0 Total Activity Average July-October, 2024' + '.jpg')\n","\n","  # A01.csv daily mean.\n","  def rgb_timeseries_daily_mean(self,data):\n","    y = [round(j) for j in data[0][-11:]]\n","    x = data[1][-11:]\n","    y.pop(0) # 'write'\n","    x.pop(0) # 'write'\n","    fig, ax = plt.subplots(figsize=(15, 17),dpi=400)\n","    bars = ax.barh(x,y,height=.5)\n","    for i in range(len(y)):\n","      plt.text(y[i]-1.5, x[i], str(y[i]), color='White', fontsize=17, ha='center', va='center', bbox={'facecolor': 'C0','linewidth': 0})\n","    plt.title('Daily Activity Average\\nJuly-October, 2024', pad=50, fontsize=20)\n","    plt.xlabel('Minutes', fontsize=17)\n","    ax.tick_params(axis='y', pad=50, labelsize=17)\n","    ax.tick_params(axis='x', labelsize=17)\n","    ax.xaxis.set_ticks_position('top')\n","    ax.xaxis.set_label_position('top')\n","    plt.yticks(x, labels=x, ha='center', fontsize=17)\n","    plt.grid(axis='x')\n","    plt.margins(y=0.01)\n","    # plt.savefig('A1-0 Daily Activity Average July-October, 2024' + '.jpg')\n","\n","\n","#################################################\n","# Part E: Part D visualization helper functions #\n","#################################################\n","# Merge sort is the fastest for worst case scenario sorting: N log(n)\n","# Implementation is from W3 and modified for AZ with ascii ord():\n","# https://www.w3schools.com/dsa/dsa_algo_mergesort.php\n","# Bubble sort is the fastest for almost sorted lists O(n)\n","# https://www.w3schools.com/dsa/dsa_timecomplexity_bblsort.php\n","# Python's built-in sort() function uses Tim Sort which uses a hybrid\n","# Insertion and Merge. Insertion is similar to Bubble with the same\n","# Time and Space complexity with worst case O(n^2) and best O(n).\n","class Graphs_sort:\n","  # Initialize the input variables\n","  def __init__(self, data):\n","    self.data = data\n","\n","  # Filters the verb endings using c_replace().\n","  def filter_stop(self,column):\n","    filtered_column = []\n","    for i in column:\n","      if 'Walked' in i:\n","        filtered_column.append(\"Walk\")\n","      elif 'Juggling' in i:\n","        filtered_column.append(\"Juggle\")\n","      elif 'Driving' in i:\n","        filtered_column.append(\"Drive\")\n","\n","      # english hard idk\n","      elif 'Reading' in i:\n","        filtered_column.append(\"Read\")\n","      elif 'Writing' in i:\n","        filtered_column.append(\"Write\")\n","      elif 'No juggling' in i:\n","        filtered_column.append(\"No juggling\")\n","      elif 'Running' in i:\n","        filtered_column.append(\"Run\")\n","      elif 'Hiking' in i:\n","        filtered_column.append(\"Hike\")\n","      elif 'Rested' in i:\n","        filtered_column.append(\"Rest\")\n","      elif 'Stretched' in i:\n","        filtered_column.append(\"Stretch\")\n","\n","      elif i == 'Lifts':\n","        # Could append since this is hard coded but I wanted to test.\n","        verb_less = i.replace(\"s\", \"\")\n","        # verb_less = self.c_replace(i, \"s\", \"\")\n","        filtered_column.append(verb_less)\n","      elif 'ing' in i:\n","        verb_less = i.replace(\"ing\", \"\")\n","        # verb_less = self.c_replace(i, \"ing\", \"\")\n","        filtered_column.append(verb_less)\n","      else:\n","        filtered_column.append(i)\n","    return filtered_column\n","\n","  # Calculates duration using end - start.\n","  def sort_time(self,activity,start,end):\n","    duration = ['Duration']\n","    for i in range(1,len(start)):\n","      # Checks to see if the Activity or Start column is empty.\n","      # if len(activity[i]) == 0 or len(start[i]) == 0:\n","      #  continue\n","      # Estimates sleep at 7 hours.\n","      dur = 0\n","      if 'Sleep' == activity[i]:\n","        dur = str(7*60)\n","      else:\n","        # Converts the '100' digits to '60' minutes in hours.\n","        # Gets the end hour.\n","        if len(end[i]) == 4:      # handles 1030 4 digits\n","          end_sub = end[i][:2]\n","        elif len(end[i]) == 3:    # handles 0930 3 digits\n","          end_sub = end[i][0]\n","        else:                     # handles 0030 2 digits\n","          end_sub = 0\n","        # Gets the start hour.\n","        if len(start[i]) == 4:    # handles 1030 4 digits\n","          start_sub = start[i][:2]\n","        elif len(start[i]) == 3:  # handles 0930 3 digits\n","          start_sub = start[i][0]\n","        else:                     # handles 0030 2 digits\n","          start_sub = 0\n","        # Subtracts 40 minutes since there are 60 in an hour not 100.\n","        if start_sub == end_sub:\n","          if int(end[i]) == int(start[i]):\n","            dur = str(5)\n","          else:\n","            dur = str(int(end[i]) - int(start[i]))\n","        else:\n","          # Turn over from one day to another.\n","          if int(end[i]) < int(start[i]):\n","            # First day's amount of hours (24 - the start time hour)\n","            first_day = 23 - int(start_sub)\n","            first_day_minutes = 60 - int(start[i][2:]) # the last two digits are the minutes\n","            # Second day's hours added to the first day's hours as 'dur' as minutes.\n","            if len(end[i]) == 4:\n","              second_day_minutes = end[i][2:]\n","            elif len(end[i]) == 3:\n","              second_day_minutes = end[i][1:]\n","            else: # There are no extra hours\n","              second_day_minutes = end[i]\n","            end_sub = end_sub + first_day\n","            hunid = (int(end_sub)) * 60 # hour difference converted to minutes\n","            dur = hunid + first_day_minutes + int(second_day_minutes)\n","          else:\n","            hunid = (int(end_sub) - int(start_sub)) * 40\n","            dur = str( ( int(end[i]) - int(start[i]) ) - hunid)\n","      duration.append(dur)\n","    return duration\n","\n","  # Merge sorts a list splice of strings from 'sort_ascii()' based on 'ord()'\n","  # and returns them to 'sort_ascii()'.\n","  def merge(self,left_in,right_in):\n","      result = []\n","      result_activity = []\n","      result_id = []\n","      result_dur = []\n","      i = j = 0\n","      while i < len(left_in[1]) and j < len(right_in[1]):\n","        left = left_in[0][i]\n","        right = right_in[0][j]\n","        left_activity = left_in[1][i]\n","        right_activity = right_in[1][j]\n","        left_id = left_in[2][i]\n","        right_id = right_in[2][j]\n","        left_dur = left_in[3][i]\n","        right_dur = right_in[3][j]\n","        if left < right: # or (left_activity_replace == right_activity and left < right) ?\n","          result.append(left)\n","          result_activity.append(left_activity)\n","          result_id.append(left_id)\n","          result_dur.append(left_dur)\n","          i += 1\n","        elif left > right: # or left_activity == right_activity ?\n","          result.append(right)\n","          result_activity.append(right_activity)\n","          result_id.append(right_id)\n","          result_dur.append(right_dur)\n","          j += 1\n","        else:\n","          if len(left_activity) > len(right_activity):\n","            length = len(right_activity)\n","          else: # same length?\n","            length = len(left_activity)\n","          # Find where the two words are different at k.\n","          for k in range(1,length):\n","            if left_activity[k] != right_activity[k]:\n","              break\n","          # Handles when the comparison first words are the same but\n","          # one of the comparisons have a space and second word.\n","          left_ord = ord(left_activity[k])\n","          right_ord = ord(right_activity[k])\n","          if left_activity[:length] == right_activity[:length]:\n","            if length < len(right_activity):\n","              if left_activity == right_activity[:length]:\n","                left_ord = -1\n","                # Comparison right_activity[length:] is longer and different.\n","                right_ord = ord(right_activity[length:][0])\n","          if left_activity[:k] == right_activity[:k] and left_ord < right_ord:\n","            result.append(left)\n","            result_activity.append(left_activity)\n","            result_id.append(left_id)\n","            result_dur.append(left_dur)\n","            i += 1\n","          else:\n","            result.append(right)\n","            result_activity.append(right_activity)\n","            result_id.append(right_id)\n","            result_dur.append(right_dur)\n","            j += 1\n","      result.extend(left_in[0][i:])\n","      result.extend(right_in[0][j:])\n","      result_activity.extend(left_in[1][i:])\n","      result_activity.extend(right_in[1][j:])\n","      result_id.extend(left_in[2][i:])\n","      result_id.extend(right_in[2][j:])\n","      result_dur.extend(left_in[3][i:])\n","      result_dur.extend(right_in[3][j:])\n","\n","      return [result,result_activity,result_id,result_dur]\n","\n","  # def sort_ascii(self,time_ID,ord_list,activity_filter,duration):\n","  # todo: avoid modification of the input variables with 'input[:]'\n","  def sort_ascii(self,ord_list,activity_filter,time_ID,duration):\n","    length = len(time_ID) - 1\n","    step = 1\n","    while step < length:\n","      for i in range(1, length, 2 * step):\n","        # Time vs space trade off: if you want less space calculate the duration\n","        # with another loop before sorting. Otherwise, the End and Start columns\n","        # are included in sorting and space is linear * number of columns (4).\n","        left = [ord_list[i:i + step],activity_filter[i:i + step], time_ID[i:i + step], duration[i:i + step]]\n","        right = [ord_list[i + step:i + 2 * step],\n","                 activity_filter[i + step:i + 2 * step], time_ID[i + step:i + 2 * step], duration[i + step:i + 2 * step]]\n","        merged = self.merge(left, right)\n","        # Place the merged array back into the original array\n","        for j in range(len(merged[0])):\n","          ord_list[i + j] = merged[0][j]\n","          activity_filter[i + j] = merged[1][j]\n","          time_ID[i + j] = merged[2][j]\n","          duration[i + j] = merged[3][j]\n","      step *= 2  # Double the sub-array length for the next iteration\n","    return [activity_filter,time_ID,duration]\n","\n","  # Returns the time_id and unique activity lists.\n","  def sort_unique_words(self,activity_col):\n","    # A0_length is 0-225\n","    activity_unique = []\n","    # Unique words in Activity\n","    for i in range(len(activity_col)):\n","      if activity_col[i] not in activity_unique:\n","        if len(activity_col[i]) == 0:\n","          continue\n","        else:\n","          activity_unique.append(activity_col[i])\n","    return activity_unique\n","\n","  # Returns the sorted list into AZ bins. C esque syntax.\n","  # Dimensions: 'sort_unique_words' by the number of occurances in 'sort_ascii'.\n","  def sort_unique_bin(self,sort_unique_words,sort_ascii):\n","\n","    # Once the word is different than the next word, bin the next\n","    # word (or words) since the list is already sorted.\n","\n","    # Empty 'unique_bin' is generated with int. Could use '0's but these\n","    # are 0,1,2,...n!\n","    unique_bin = [\n","        [[x],[x]] for x in range(len(sort_unique_words))\n","        ]\n","    count = 0\n","    for i in range(len(sort_ascii[0])):\n","      # Avoids checking 'sort_ascii' past the length of the list.\n","      # print(count, unique_bin[count], len(sort_unique_words))\n","      if i == (len(sort_ascii[0])-1):\n","        # If there are exactly one entry in the final 'sort_unique_words',\n","        # 'count' of type 'int' is placed as two lists into the identifier list.\n","        if unique_bin[-1][0][0].__class__ == int:\n","          unique_bin[-1] = [[sort_unique_words[count]]]\n","        break\n","      if sort_ascii[0][i] == sort_ascii[0][i+1]:\n","        # Words are the same, 'count' does not get incremented.\n","        if unique_bin[count][0][0].__class__ == str:\n","          # If first key or 'unique_bin[count]' is str, don't include it.\n","          unique_bin[count].append([sort_ascii[0][i],sort_ascii[1][i],sort_ascii[2][i]])\n","        else:\n","          unique_bin[count] = [\n","              [sort_unique_words[count]],\n","              [sort_ascii[0][i],sort_ascii[1][i],sort_ascii[2][i]]\n","              ]\n","      elif (sort_ascii[0][i-1] != sort_ascii[0][i]) and (sort_ascii[0][i] != sort_ascii[0][i+1]):\n","        # Previous word and next word are different.\n","        unique_bin[count] = [\n","            [sort_unique_words[count]],\n","            [sort_ascii[0][i],sort_ascii[1][i],sort_ascii[2][i]]\n","            ]\n","        count += 1\n","      elif (sort_ascii[0][i-1] == sort_ascii[0][i]) and (sort_ascii[0][i] != sort_ascii[0][i+1]):\n","        # Previous word is the same, next word is different.\n","        unique_bin[count].append([sort_ascii[0][i],sort_ascii[1][i],sort_ascii[2][i]])\n","        count += 1\n","      else:\n","        count += 1\n","    # Append last element of sorted list onto the bin list at end\n","    unique_bin[-1].append([sort_ascii[0][-1],\n","                           sort_ascii[1][-1],\n","                           sort_ascii[2][-1]])\n","    return unique_bin\n","\n","  # Orders 'int' or 'float' instead of strings.\n","  # secodnary todo use one list instead of several lists (also for sort_ascii())\n","  def merge_int(self, left, right):\n","    result_int = []\n","    result_activity = []\n","    i = j = 0\n","    while i < len(left[0]) and j < len(right[0]):\n","      if left[0][i] < right[0][j]:\n","        result_int.append(left[0][i])\n","        result_activity.append(left[1][i])\n","        i += 1\n","      else:\n","        result_int.append(right[0][j])\n","        result_activity.append(right[1][j])\n","        j += 1\n","    result_int.extend(left[0][i:])\n","    result_int.extend(right[0][j:])\n","    result_activity.extend(left[1][i:])\n","    result_activity.extend(right[1][j:])\n","    return [result_int, result_activity]\n","\n","  # Orders list of only 'int' or 'float'. From W3 schools.\n","  def merge_sort_int(self,int_in,activity_in):\n","    # The splice everything is used to avoid modification\n","    # of the input variables.\n","    array_int = int_in[:]\n","    activity = activity_in[:]\n","    step = 1  # Starting with sub-arrays of length 1\n","    length = len(array_int) - 1\n","    while step < length:\n","      for i in range(0, length, 2 * step):\n","        left = [array_int[i:i + step],activity[i:i + step]]\n","        right = [array_int[i + step:i + 2 * step],activity[i + step:i + 2 * step]]\n","        merged = self.merge_int(left, right)\n","        # Place the merged array back into the original array\n","        for j in range(len(merged[0])):\n","          array_int[i + j] = merged[0][j]\n","          activity[i + j] = merged[1][j]\n","      step *= 2  # Double the sub-array length for the next iteration\n","    return [array_int,activity]\n","\n","  # todo: use multiple variables\n","  # Merges entries if the first word in the string is the same. Uses C syntax.\n","  # If you're a stickler, replace 'for i in range()' with 'while iterator <= len(data)'\n","  def merge_similar_activities(self, sorted_list):\n","    # Specific formatting for this dataset.\n","    ######################################################################\n","    x = [i[0][0] for i in sorted_list[1:]] # or A0_sort_unique\n","    z = [len(k[1:]) for k in sorted_list[1:]] # frequency of each activity\n","    y = []\n","    # Calculate the hours for duration.\n","    for j in sorted_list[1:]:\n","      y_label = []\n","      for m in j[1:]:\n","        y_label.append(float(m[2]))\n","      y.append(round(sum(y_label) / 60, 4))\n","    # This is a bad method to avoid binning 'Skateboard paper' into\n","    # 'Skateboard' but I want to do the graphing.\n","    for ayy in range(len(x)):\n","      if x[ayy] == 'Skateboard paper':\n","        x[ayy] = 'paper Skateboard'\n","      if x[ayy] == 'Skateboard videos':\n","        x[ayy] = 'videos Skateboard'\n","\n","    ######################################################################\n","\n","    # Combining similar Activities. The dataset uses the same word plus\n","    # a number to denote multiple of the same activities on the same day.\n","    y_mean = round(sum(y) / float(len(y)), 4)\n","    count_0 = 0\n","    count_1 = 1\n","    root_bool = False\n","\n","    # Time complexity is the number of Activities.\n","    #for count_0 in range(len(y)):\n","    x[count_0] = x[count_0]\n","    y[count_0] = y[count_0]\n","    z[count_0] = z[count_0]\n","    spliced = []\n","    while True:\n","      if count_1 == len(y):\n","        x[count_0] = x[count_0]\n","        y[count_0] = y[count_0]\n","        z[count_0] = z[count_0]\n","        break\n","      x[count_1] = x[count_1]\n","      y[count_1] = y[count_1]\n","      z[count_1] = z[count_1]\n","      x_0 = x[count_0].split(' ') # c_split\n","      x_1 = x[count_1].split(' ') # c_split\n","      # Comparison operators to find the first instance of the word.\n","      # i.e. 'Eat' followed by 'Eat 0', 'Eat 1', etc.\n","      if x_0[0] == x_1[0]:\n","        if x_1[1] != 'outside':\n","          if ' ' not in x[count_0]:\n","            #if len(x_1) < 3:\n","            if root_bool == False:\n","              root_pos = count_0\n","              root_bool = True\n","      if root_bool == True:\n","        # Skip merging 'Skateboard paper' and 'Skateboard videos'.\n","        if y[root_pos] != y[count_0]:\n","          # 'y' value at 'count_0' is cumulative\n","          y[root_pos] += y[count_0]\n","          z[root_pos] += z[count_0]\n","        # The first 'y' value at 'root_pos' is the same as the total.\n","        x[count_0] = x[root_pos]\n","        y[count_0] = y[root_pos]\n","        z[count_0] = z[root_pos]\n","      else:\n","        x[count_0] = x[count_0]\n","        y[count_0] = y[count_0]\n","        z[count_0] = z[count_0]\n","      # This checks to see if the word and next word are different or the next\n","      # word is the same and is has three or more words, 'root_bool' is False.\n","      if x_0[0] != x_1[0] or len(x_1) >= 3:\n","        if root_bool == True:\n","          extra_numbers = (count_0+1) - (root_pos+1)\n","          # 'spliced' numbers list gets three values that slice the remaining\n","          # identical words in another loop.\n","          spliced = spliced + [[(count_0-extra_numbers),count_0, extra_numbers]]\n","          # The values at 'root_pos' get set to the current value at 'count_0'.\n","          x[count_0-extra_numbers] = x[count_0]\n","          y[count_0-extra_numbers] = y[count_0]\n","          z[count_0-extra_numbers] = z[count_0]\n","          root_bool = False\n","      count_0 += 1\n","      count_1 += 1\n","\n","    ######################################################################\n","    # Reorganizing the strings (bad method)\n","    # Also inserts '\\n' breaks if the string is longer than 10 characters\n","    # and more than one word.\n","    for bay in range(len(x)):\n","      if x[bay] == 'paper Skateboard':\n","        x[bay] = 'Skateboard paper'\n","      if x[bay] == 'videos Skateboard':\n","        x[bay] = 'Skateboard videos'\n","      string_count = 0\n","      for cay in x[bay]:\n","        if string_count > 9:\n","          string_break = x[bay].split(' ')\n","          string_word_count = len(string_break)\n","          if string_word_count > 1:\n","            string_word_mid = round(string_word_count / 2)\n","            string_0_half = \" \".join(string_break[:string_word_mid]) + '\\n'\n","            string_1_half = \" \".join(string_break[string_word_mid:])\n","            x[bay] = string_0_half + string_1_half\n","          break\n","        string_count += 1\n","    return [x,y,z,spliced]\n","\n","  # Uses 'merge_similar_activities' to splice the data.\n","  def merge_activities_splice(self,merged_activiites):\n","    # Time complexities is the number of repeated words that are being merged.\n","    x = []\n","    y = []\n","    z = []\n","    spler = []\n","    for k in range(len(merged_activiites[3])+1): # or while the length is less than the\n","      # The end splice.\n","      if k == len(merged_activiites[3]):\n","        # spler = spler + [[middle_0,0]] <- checks that the numbers are correct\n","        x = x + merged_activiites[0][middle_0:]\n","        y = y + merged_activiites[1][middle_0:]\n","        z = z + merged_activiites[2][middle_0:]\n","        break\n","      # The first splice.\n","      if x == []:\n","        # spler = [[0,merged_activiites[3][k][0]+1]]\n","        x = merged_activiites[0][0:merged_activiites[3][k][0]+1]\n","        y = merged_activiites[1][0:merged_activiites[3][k][0]+1]\n","        z = merged_activiites[2][0:merged_activiites[3][k][0]+1]\n","        # This part is carried into the next splice.\n","        middle_0 = merged_activiites[3][k][0]+1 + merged_activiites[3][k][2]\n","      else: # The middle splices.\n","        # spler = spler + [[middle_0, merged_activiites[3][k][0]+1]]\n","        x = x + merged_activiites[0][middle_0:merged_activiites[3][k][0]+1]\n","        y = y + merged_activiites[1][middle_0:merged_activiites[3][k][0]+1]\n","        z = z + merged_activiites[2][middle_0:merged_activiites[3][k][0]+1]\n","        # This part is carried into the next splice.\n","        middle_0 = merged_activiites[3][k][0]+1 + merged_activiites[3][k][2]\n","    return [x,y,z]\n","\n","  # Flips Horizontal list of lists to vertical. Similar to NumPy reshape().\n","  # Input test dimensions are seven days by eight categories.\n","  def direction_flipper(self, input):\n","    # Get some vert.\n","    count = 0\n","    count_ct = 0\n","    count_br = 0\n","    output = []\n","    activity_count = []\n","    while True:\n","      if count == len(input):\n","        # When 'count' reaches the length of the list of horizontal lists input,\n","        # append the vertical list.\n","        output.append(activity_count)\n","        # 'count' resets, 'count_ct' increases by one,\n","        # and a new nested list is declared.\n","        count = 0\n","        count_ct += 1\n","        activity_count = []\n","        if count_br == ((len(input[0])) * (len(input))):\n","          break\n","      # 'count_ct' stays the same each time and 'count' is incremented each\n","      # time to get the first value of each list.\n","      activity_count.append(input[count][count_ct])\n","      count += 1\n","      count_br += 1\n","    return output\n","\n","\n","  # Returns the abnormal pain entry.\n","  def erroneous_values(self,P1_vert_column):\n","    erroneous_values = [[P1_vert_column[0],'NA']]\n","    non_erroneous_values = [[P1_vert_column[0],'NA']]\n","    all_values = [[P1_vert_column[0],'NA']]\n","    for i in range(len(P1_vert_column)):\n","      if len(P1_vert_column[i]) > 1:\n","        continue\n","      # Append all values to calculate overall mean.\n","      all_values.append([P1_vert_column[i], i-1])\n","      # Not stamina and greater than 4 pain values and 'Day_ID' get sent\n","      # to the list of lists.\n","      if P1_vert_column[0] != 'Stm' and int(P1_vert_column[i]) > 3:\n","        erroneous_values.append([P1_vert_column[i], i-1])\n","      # If it is stamina, check for values 2 and less.\n","      elif P1_vert_column[0] == 'Stm' and int(P1_vert_column[i]) < 3:\n","        erroneous_values.append([P1_vert_column[i], i-1])\n","      else:\n","        # Otherwise it is a normal value.\n","        non_erroneous_values.append([P1_vert_column[i], i-1])\n","    return [erroneous_values,non_erroneous_values,all_values]\n","\n","  # Calculates the activity frequency for each day in ~ 500 iterations.\n","  # Returns 'Day_ID', 'start', and 'end' for splicing in 'activity_reverse()'.\n","  def activity_frequency_splice(self):\n","    # Skips the first ten values, 'Day_ID' is '10' in P1.csv and\n","    # 'start' is the 'Time_ID' in A1.tsv. The first 10 days are discarded\n","    # because they are why the information was collected.\n","    start = 53\n","    activity_frequency = []\n","    # The 'data' is from the function's class and needs a boilerplate\n","    # value appended to return the entire length of the 'data' list.\n","    data = self.data[0]\n","    data.append('100')\n","    for i in range(start,len(data)):\n","      # Checks 'Day_ID' P1.csv against 'Time_ID' from A1.tsv.\n","      # If 'Day_ID' is not '' or the values in A1.tsv,\n","      # they must by an integer (as long as the first value\n","      # header 'Day_ID' is ignored).\n","      if len(data[i]) > 0:\n","        day_id = int(data[i])\n","        activity_frequency.append([day_id-1, start, i])\n","        # 'start' is set to the 'i' or the iterator, which resumes checking\n","        # the length of A1.tsv.\n","        start = i\n","    # Deletes the null first value.\n","    del activity_frequency[0]\n","    return activity_frequency\n","\n","  # Function to find the number of activities [30,10,7,5,3] and\n","  # returns the ['Category', 'Day_ID', 'Number of Days', 'Activity Frequency', 'Frequency per Day']\n","  def activity_reverse(self,day_id,activity_frequency):\n","    counter = 0\n","    known_ID = int(day_id[2][1])\n","    activity_reverse = []\n","    for i in range(len(day_id)):\n","      if type(day_id[i][1]) == str:\n","        category = day_id[i][0] # the category\n","      elif type(day_id[i][1]) == int:\n","        if day_id[i][1] > 9:\n","          # The number of days in reverse.\n","          reverse_days = [3,5,7,10,30]\n","          # 'Day_ID' ignores the first 10 days by subtracting 10 from 'start' and 'end'.\n","          # activity_by_category = []\n","          for k in range(len(reverse_days)):\n","            # 'end' is set to the 'Day_ID' when calculating the first reverse day '3'\n","            # Otherwise, it is a subtracted 'known_ID' that is the previous 'start' value below.\n","            if reverse_days[k] == 3:\n","              end = day_id[i][1]\n","            else:\n","              end = known_ID\n","            start = day_id[i][1] - reverse_days[k]\n","            # print('start: ', start, ' = ', day_id[i][1], ' - ', reverse_days[k])\n","            # print('end: ', end)\n","            # Avoids calculating frequencies for the first 10 days.\n","            if start > 9:\n","              # The number of activities 'reverse_days[k]' from the pain observation.\n","              activity_total = 0\n","              for j in range(start-10,end-10):\n","                # Summation for the number of activities, 'reverse_days' (k) from pain observation.\n","                activity_difference = activity_frequency[j][2] - activity_frequency[j][1]\n","                # print('Activity calculating : ', activity_difference, ' = ', activity_frequency[j][2], ' - ', activity_frequency[j][1])\n","                activity_total += activity_difference\n","                counter += 1\n","              # 'known_ID' is used to avoid calculating frequencies that are already known\n","              # by using the previous start to avoid duplicated iterations.\n","              known_ID = start\n","              # If 'k' is greater than '0', the previous day range frequency difference\n","              # was already calcualted and is used to calculate the frequency, reducing\n","              # unnecessary iterations. If 'reverse_days[k]' is '5' the frequency is already\n","              # known for '3', the difference for days '4' and '5' are calculated and added\n","              # onto 'activity_previous'.\n","              if k > 0:\n","                activity_previous = activity_intermediate + activity_total\n","              else:\n","                activity_previous = activity_total\n","              # The previous total is held for the next iteration.\n","              # print('Activity Total = ', activity_previous)\n","              activity_intermediate = activity_previous\n","              # Calculate the 'Activity Frequency' and 'Days' to get the the average of each day.\n","              activity_day_mean = round(activity_previous / reverse_days[k],2)\n","              activity_reverse.append([category,day_id[i][1],reverse_days[k],activity_previous,activity_day_mean])\n","          # print(category,day_id[i][1],reverse_days[k],activity_previous)\n","          if i+1 == len(day_id):\n","            break\n","          # May or may not need this depending on the oscillatory mean of meridians along Neptune reflecting Sun farts.\n","          # known_ID = day_id[i+1][1]\n","          # Avoids when the only 'reverse_days[k]' is '30' and is the only\n","          # 'start' that's less than '9'. [3,5,7,10] have already been\n","          # calculated.\n","          #if known_ID == 'NA':\n","          #  known_ID = day_id[i+2][1]\n","          #print('known ID: ', known_ID)\n","    # print(counter) # ~472 iterations without skips or 250 with skipping already calculated differences.\n","    return activity_reverse\n","\n","  # Function to find the mean for each day group [3,5,7,10,30] mean.\n","  def activity_group_mean(self,input_data):\n","    group_day = [3,5,7,10,30]\n","    group_mean_list = []\n","\n","    for i in range(len(group_day)):\n","      group_sum = 0\n","      group_count = 0\n","      for j in range(len(input_data)):\n","        # When the 'group_day' is the same as the group day value in\n","        # the 'activity_frequency' as calculate by 'Day_ID', that '3' '5', etc\n","        # group sum and count is incremented by the frequency and count respectively.\n","        if group_day[i] == input_data[j][2]:\n","          group_sum += input_data[j][4]\n","          group_count += 1\n","      # Once all the '3', '5', or ... n is summized, that group day's mean is calculated.\n","      # Subtract one because there's an extra '0' value appended from 'activity_day_mean()'.\n","      group_mean = group_sum / (group_count - 1)\n","      group_mean_list.append([round(group_mean,2),group_day[i],group_count])\n","\n","    return group_mean_list\n","\n","#################################################\n","# Part F: Non-parametric Classification Metrics #\n","#################################################\n","# Inputs are observations and prediction columns.\n","# Assumes input has a header.\n","class classification_metrics:\n","  # true_positive  = true_positive   1  (true_positive 1 / true_positive 1 + fn4) or recall\n","  # true_negative  = true_negative   0  (true_negative 0 / true_negative 0 + false_positive 3)\n","  # false_positive = false_positive  3  predicted soreness, was not sore.\n","  #                                     false positive rate = false_positive 3 / false_positive 3 + true_negative 0\n","  # false_negative = false_negative  4  predicted not soreness, was sore\n","  # https://www.geeksforgeeks.org/metrics-for-machine-learning-model/#regression-evaluation-metrics\n","  # https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall\n","  '''\n","  print(P1_vert[0])\n","  print(P1_vert[1])\n","  print(P1_vert[2])\n","  # 0 6 7 8\n","  print(A1_vert[0]) # Day_ID\n","  print(A1_vert[6]) # Activity\n","  # print(A1_vert[7]) # Notes\n","  # print(A1_vert[8]) # Explaination\n","  '''\n","  def __init__(self,observations,predictions):\n","    # The input scale is 5-1 high pain to low pain (or stamina).\n","    # The original data was 1-5 high pain to low pain and was flipped\n","    # since it was confusing (except for stamina).\n","    self.observations = observations\n","    self.predictions = predictions\n","\n","  def binary_classification(self):\n","    # Returns 0 (True Negative) 1 (True Positive) if prediction\n","    # matches observation. False Positive when prediction was soreness\n","    # and observation was no soreness (3). False Negative when the prediction\n","    # was no soreness and there was soreness (4).\n","    # Also returns the count for the classifications.\n","    true_negative = 0\n","    true_positive = 0\n","    false_positive = 0\n","    false_negative = 0\n","    binary = []\n","    for i in range(1,len(self.observations)):\n","      if self.observations[i] == self.predictions[i]:\n","        result = 1\n","        true_positive += 1\n","      else:\n","        if self.predictions[i] == 'NA' or self.observations[i] == 'NA':\n","          result = 0\n","        # False positive predicted 4 or 5 (high soreness) and was 1,2,3.\n","        elif int(self.predictions[i]) > 3 and int(self.observations[i]) <= 3:\n","          result = 3\n","          false_positive += 1\n","        # False negative predicted 1,2,3 (low soreness) and was 4 or 5.\n","        elif int(self.predictions[i]) <= 3 and int(self.observations[i]) > 3:\n","          result = 4\n","          false_negative += 1\n","        else:\n","          result = 0\n","          true_negative += 1\n","      binary.append(result)\n","    return [binary,true_negative,true_positive,false_positive,false_negative]\n","\n","  def accuracy(self,binary):\n","    # Number of correct predictions / total, input is False/True 0/1.\n","    count = 0\n","    for i in binary:\n","      if i == 1:\n","        count += 1\n","    total = len(binary)\n","    result = count / total\n","    return result\n","\n","  def precision(self,true_positive,false_positive):\n","    # precision = true_positive 1 / (true_positive 1 + false_positive 3)\n","    result = true_positive / (true_positive + false_positive)\n","    return result\n","\n","  def recall(self,true_positive,false_negative):\n","    # (true_positive / true_positive + false_negative 4) or recall\n","    result = true_positive / (true_positive + false_negative)\n","    return result\n","\n","  def f1_score(self,true_positive,false_positive,false_negative):\n","    # 2 * (precision * recall) / (precision + recall)\n","    # (2 true_positive) / (2 true_positive + false_positive 3 + false_negative)\n","    result = (2 * true_positive) / ((2*true_positive) + false_positive + false_negative)\n","    return result\n","\n","  # After the previous n (10,7,5,3) days of activity frequency, use the non parametric\n","  # Wilcoxon's rank sum test to compare the two dependent or paired samples. The two\n","  # samples being compared are n days activity frequency with the entire dataset's activity\n","  # frequency. It is non-parametric because it is categorical or ordinal dataset and\n","  # not real world measurements, despite having over 30 observations.\n","  # https://www.stat.purdue.edu/~tqin/system101/method/method_wilcoxon_rank_sum_sas.htm\n","  # https://pmc.ncbi.nlm.nih.gov/articles/PMC4754273/\n","  def wilcoxon_rank_sum(self):\n","    return\n","\n","##############################################################################\n","# Part Z: Run the functions                                                  #\n","##############################################################################\n","\n","# Part A: The path of the CSV to be parsed\n","def CSV_running(path,unflipped_col):\n","  # Create the CSV_Parser class object and open the files\n","  parser = eu.CSV_Parser(path)\n","  read = parser.file_opener()\n","  # Index the comma position from the CSV and split the characters into their values\n","  comma_indexed = parser.comma_index(read, path, 0)\n","  # Get the width of columns of the commas\n","  comma_width = parser.comma_index(read, path, 1)\n","  # Sort the list into verticle columns\n","  # The P0 csv gets flipped, except for the Stm column\n","  # Divide by two - the list of comma places is doubled for the start/end value\n","  col_width = int(((comma_width - 1 ) / 2) - 1)\n","  vert = []\n","  for i in range(0,comma_width-1,2):\n","    value_list = parser.csv_value_list(comma_indexed, read, col_width, i)\n","    if unflipped_col == 0:\n","      vert.append(value_list)\n","    else:\n","      if value_list[0] in unflipped_col:\n","        vert.append(value_list)\n","      else:\n","        flip = parser.csv_flipper(value_list, col_width)\n","        vert.append(flip)\n","  return vert\n","\n","# Part D: Data visualization RGB\n","def P1_RGB_graph(P1_vert):\n","\n","  title_full = ['Stamina',\n","                'Feet','Ankle','Calves',\n","                'Knees','Quadriceps','Gluteus','Groin',\n","                'Abdominals','Lower Back',\n","                'Latissimus Dorsi','Trapezius','Shoulders',\n","                'Chest','Triceps','Biceps',\n","                'Neck','Head']\n","  P1_rgb = Graphs_rgb(P1_vert)\n","  P1_B1 = 0\n","  start_val = 3\n","  # Draws the bar charts\n","  # P1_rgb_bar = P1_rgb.rgb_timeseries_bar(title_full,start_val,P1_B1)\n","  # RGB Line Graphs by Group\n","  # Uses the position of each body part name in the title_full list\n","  P1_groups_num = [3,4,7,11,13,16,19,21]\n","  P1_title_label = ['Stamina','Lower Legs','Upper Legs','Core','Upper Back','Arms','Head']\n","  # P1_rgb_line = P1_rgb.rgb_timeseries_line(title_full,start_val,P1_groups_num,P1_title_label,P1_B1)\n","\n","  # Line graphs by upper/lower body group means\n","  def small():\n","    csv_groups_list = [[3,4],[4,7,11,13],[13,16,19,21]]\n","    legend_label = [['Stamina'],['Lower Legs','Upper Legs','Core'],['Upper Back','Arms','Head']]\n","    k0 = 0\n","    for csv_groups_num in csv_groups_list:\n","      fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(11, 5), dpi=400)\n","      P1_rgb_line_smallest = P1_rgb.rgb_timeseries_small(csv_groups_num,legend_label[k0],ax)\n","      # Plot formatting\n","      plt.margins()\n","      plt.grid()\n","      plt.yticks(range(1,6),fontsize=17)\n","      plt.xticks(fontsize=17)\n","      if sum(csv_groups_num) == sum(csv_groups_list[1]):\n","        ax.legend(title='5 Day Mean', title_fontsize=19, fontsize=17)\n","        plt.title(\"Lower Body\",fontsize=19)\n","        plt.ylabel(\"Pain\",fontsize=17)\n","        #plt.savefig(\"Lower Body Pain.jpg\")\n","      elif sum(csv_groups_num) == sum(csv_groups_list[2]):\n","        ax.legend(title='5 Day Mean', title_fontsize=19, fontsize=17)\n","        plt.title(\"Upper Body\",fontsize=19)\n","        plt.ylabel(\"Pain\",fontsize=17)\n","        #plt.savefig(\"Upper Body Pain.jpg\")\n","      else:\n","        plt.ylabel(\"Stamina 5 Day Mean\",fontsize=17)\n","        #plt.savefig(\"Stamina.jpg\")\n","      k0 += 1\n","\n","  # Smallest on one graph\n","  def smallest():\n","    csv_groups_list = [[3,4],[4,12],[13,21]]\n","    legend_label = [['Stamina'],['Lower Body'], ['Upper Body']]\n","    # csv_groups_list = [[4,21]]\n","    # legend_label = [['Pain']]\n","    fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(11, 5), dpi=400)\n","    k1 = 0\n","    for csv_groups_num in csv_groups_list:\n","      P1_rgb_line_smallest = P1_rgb.rgb_timeseries_small(csv_groups_num,legend_label[k1],ax)\n","      k1 += 1\n","    # Plot formatting\n","    plt.margins()\n","    plt.grid()\n","    plt.legend(title='5 Day Mean', title_fontsize=19, fontsize=17)\n","    plt.xticks(fontsize=17)\n","    plt.yticks(range(1,6),fontsize=17)\n","    plt.ylabel(\"Pain\",fontsize=17)\n","    # plt.savefig('P1_smallerest.jpg')\n","  # small()\n","  smallest()\n","\n","def B1_RGB_graph(B1_vert):\n","  title_full = ['Calories','Exercise',            # Group 0\n","              'Salt', 'Fat', 'Protein',           # Group 1\n","              'Carbohydrates', 'Alcohol Servings' # Group 3\n","              ]                                   # etc\n","  B1_rgb = Graphs_rgb(B1_vert)\n","  P1_B1 = 1\n","  start_val = 2\n","  # Part D RGB Graphs: B1.csv\n","  # B1_rgb_bar = B1_rgb.rgb_timeseries_bar(title_full,start_val,P1_B1)\n","  # RGB Line Graphs by Group for B0.csv\n","  # Uses the position of each title in the title_full list\n","  B1_groups_num = [2,3,4,8,9]\n","  B1_title_label = ['Calories','Exercise','Nutrients','Alcohol Servings']\n","  # Line graph is not appropriate for calories, exercise, and alcohol servings\n","  B1_rgb_line = B1_rgb.rgb_timeseries_line(title_full,start_val,B1_groups_num,B1_title_label,P1_B1)\n","\n","# A0_vert.tsv or A1_vert.tsv\n","def A1_RGB_graph(A1_vert):\n","  # Part D RGB Graphs: A0.tsv\n","  A1_sort = Graphs_sort(A1_vert)\n","  # Calculates the duration of each activity.\n","  A1_sort_duration = A1_sort.sort_time(A1_sort.data[6],A1_sort.data[4],A1_sort.data[5])\n","  # Removes endings for similar words such as: 'Walk', 'Walks', 'Walked', 'Walking'.\n","  A1_activity_filter = A1_sort.filter_stop(A1_sort.data[6])\n","\n","  # Time_ID, Date, Duration, Activity_filtered\n","  # Part of the duration times calories burned (used built in sorted)\n","  # not A1_sort.data[3][i], A1_sort.data[6][i]\n","  A1_sort_column = [A1_sort.data[1],A1_sort_duration,A1_activity_filter]\n","  # A1_sort_duration[i] * calories burned doing A1_sort.data[6][i]\n","  # Transpose\n","  A1_sort_rows = list(zip(*A1_sort_column))\n","  A1_sorted_rows = sorted(A1_sort_rows[1:], key=lambda row: row[2])\n","\n","  # Sorts the list using an implementation of merge sort.\n","  ord_list = ['ord_list'] + [ord(A1_activity_filter[x][0]) for x in range(1,len(A1_activity_filter))]\n","  A1_sort_merged = A1_sort.sort_ascii(ord_list,A1_activity_filter,A1_sort.data[1],A1_sort_duration)\n","  # Finds the unique occurances of each word in 'Activity'.\n","  A1_sort_unique = A1_sort.sort_unique_words(A1_sort_merged[0])\n","\n","  # Other part of the duration times calories burned\n","  # calories_out_min_path = \"/content/calories_burned_calculator.csv\"\n","  # calories_out_min = CSV_running(calories_out_min_path,0)\n","  # TODO merge sort by int\n","  calories_out_min = [0,0,0,4,0,0,0,0,8.6,3,7.5,10,10,0,9,0,15,18,7.2,5,6.1,5,7,7.2,0,0,0,3,3.8,0]\n","  A1_sorted_calories = []\n","  for i in range(len(A1_sorted_rows)):\n","    for j in range(len(A1_sort_unique[1:])):\n","      if A1_sorted_rows[i][2] == A1_sort_unique[1:][j]:\n","        # print(A1_sorted_rows[i][1],calories_out_min[j])\n","        calories_out = int(A1_sorted_rows[i][1]) * calories_out_min[j]\n","        A1_sorted_calories.append((int(A1_sorted_rows[i][0]),A1_sorted_rows[i][2],A1_sorted_rows[i][1],calories_out_min[j],calories_out))\n","  # Sort by the ID.\n","  A1_calories = sorted(A1_sorted_calories, key=lambda row: row[0])\n","  # Transpose back to columns.\n","  A1_calories_columns = [list(col) for col in zip(*A1_calories)]\n","  # ID, Activity, Duration, Calories Out per minute, Calories Out Activity, Date\n","  A1_calories_columns.append(A1_sort.data[3][1:])\n","\n","  # Bins the sorted list using the unique words.\n","  A1_sort_bin = A1_sort.sort_unique_bin(A1_sort_unique,A1_sort_merged)\n","  # Merges the bins based on if the first word in the string are the same.\n","  # i.e. 'Eat' <- 'Eat 1' <- 'Eat 2' <- 'Eat 3'\n","  A1_sort_similar = A1_sort.merge_similar_activities(A1_sort_bin)\n","  A1_sort_similar_splice = A1_sort.merge_activities_splice(A1_sort_similar)\n","  # Sort the values for each horizontal bar graph.\n","  A1_sort_similar_2 = A1_sort.merge_sort_int(A1_sort_similar_splice[2],A1_sort_similar_splice[0])\n","\n","  A1_sort_graph = Graphs_rgb(0)\n","  # A1_sort_graph.rgb_timeseries_frequency(A1_sort_similar_2)                     # ( 2, 0 )\n","  A1_sort_similar_1 = A1_sort.merge_sort_int(A1_sort_similar_splice[1],A1_sort_similar_splice[0])\n","  # A1_sort_graph.rgb_timeseries_duration(A1_sort_similar_1)                      # ( 0, 1 )\n","\n","  # Calories out by Activity\n","  A1_sort_graph.rgb_timeseries_calories_out(A1_calories_columns)\n","\n","  '''\n","  # Total activity mean.\n","  A1_sort_duration_mean = []\n","  for i in range(len(A1_sort_similar_splice[1])):\n","    duration_mean = A1_sort_similar_splice[1][i] / A1_sort_similar_splice[2][i]\n","    A1_sort_duration_mean.append(duration_mean)\n","  A1_sort_similar_3 = A1_sort.merge_sort_int(A1_sort_duration_mean,A1_sort_similar_splice[0])\n","  A1_sort_graph.rgb_timeseries_mean(A1_sort_similar_3)                          # ( 0,(1/2) )\n","\n","  # Daily mean.\n","  A1_sort_daily_mean = []\n","  for i in range(len(A1_sort_similar_splice[1])):\n","    duration_mean = (A1_sort_similar_splice[2][i] / 100) * 60\n","    A1_sort_daily_mean.append(duration_mean)\n","  A1_sort_similar_4 = A1_sort.merge_sort_int(A1_sort_daily_mean,A1_sort_similar_splice[0])\n","  # A1_sort_graph.rgb_timeseries_daily_mean(A1_sort_similar_4)                    # ( 0,((1/2) / 100) * 60)\n","  '''\n","\n","# Part F: Predictions vs Observed pain values using classification metrics.\n","def P1_Classification_RGB_graph(P1_vert,P1_vert_predictions):\n","\n","  # B1.csv - Nutrition - binary calories high and low -> above/below 2500\n","  # - Mean number of activities per day over 3-14 days\n","  #     - Exclude 09/09-09/13 since it was recorded with excessive detail.\n","  # - Not stretching in the one or two days afterward.\n","  # - Stretching too frequently in the one or two days afterward.\n","\n","  # Days of Interest :\n","  # Stamina for 08/27-0903 (value 4) except 08/29 (value 2) and abs (4) on 08/31.\n","      # Stamina for 09/13. Exclude 09/09-09/13 since it was recorded with excessive detail.\n","  # Stamina for 09/19-09/22 (value 4) except 09/21 (value 2).\n","  # The goal is to find an appropriate balance for exercise and not moving\n","  # by examining the frequency of Activities before these decreases.\n","\n","  # 'Day_ID' remove 49-53 for average graphing because they were recorded\n","  # differently and induce outliers. ['909','910','911','912','913']\n","  for P1 in range(len(P1_vert_predictions)):\n","    del P1_vert_predictions[P1][50:55]\n","    del P1_vert[P1][50:55]\n","  # Remove these dates: ['909','910','911','912','913']\n","  # in A1_vert[0], A1_vert[3]\n","  del A1_vert[0][271:326]\n","  del A1_vert[3][271:326]\n","  # Accuracy, Precision, Recall, F1\n","  # Uses F1 since RMSE is for regression prediction models. The pain scale\n","  # is numerical and is equivilent to nominal categories.\n","  title_full = ['','','',\n","  'Stamina',\n","  'Feet','Ankle','Calves',\n","  'Knees','Quadriceps','Gluteus','Groin',\n","  'Abdominals','Lower Back',\n","  'Latissimus Dorsi','Trapezius','Shoulders',\n","  'Chest','Triceps','Biceps',\n","  'Neck','Head']\n","  # todo classification_metrics\n","  # wilcoxon_rank_sum\n","  for i in range(3,len(P1_vert)):\n","    '''\n","    print('Pain Scale')\n","    print(P1_vert[i])\n","    print('Classification')\n","    class_met = classification_metrics(P1_vert[i],P1_vert_predictions[i])\n","    binary = class_met.binary_classification()\n","    print(binary[0])\n","    print()\n","    # i is the pain scale 'Day_ID' (1-101) for A1 (1-350ish) 'did'\n","    print(A1_vert[0]) # 'Day_ID\n","    print(A1_vert[1])\n","    print(A1_vert[6]) # 'Activity'\n","    print()\n","    '''\n","\n","    A1_graphs_sort = eu.Graphs_sort(A1_vert)\n","    A1_filter = A1_graphs_sort.filter_stop(A1_graphs_sort.data[6])\n","\n","    '''\n","    A1_graphs_sort = Graphs_sort(data)\n","    A1_graphs_sort.filter_stop(A1_graphs_sort.data[])\n","    acc_binary = class_met.accuracy(binary[0])\n","    # header : print(P1_vert[i][0])\n","    print('Accuracy', ' = ', acc_binary)\n","    precision_binary = class_met.precision(binary[2],binary[3]) # tsp fp\n","    print('Precision', ' = ', precision_binary)\n","    print('tsp', ' ', binary[2])\n","    print('fp', ' ', binary[3])\n","    print()\n","    recall_binary = class_met.recall(binary[2],binary[4]) # tsp fn\n","    print('Recall', ' = ', recall_binary)\n","    print('tsp', ' ', binary[2])\n","    print('fp', ' ', binary[3])\n","    print()\n","    f1_score_binary = class_met.f1_score(binary[2],binary[3],binary[4]) # tsp fp fn\n","    print('F1 Score', ' = ', f1_score_binary)\n","    print('tsp', ' ', binary[2])\n","    print('fp', ' ', binary[3])\n","    print('fn', ' ', binary[4])\n","    print()\n","    print()\n","    print()\n","    '''\n","\n","  return\n","\n","# Part E and F: P1.csv with part F accuracy metrics, activity reverse by group,\n","# and activity reverse by entry.\n","def A1_daily_RGB_graph(P1_vert,P1_vert_predictions,A1_vert):\n","  # Edited five days to remove overly detailed entries called 'A1-small.csv'\n","  # original is 'A1.csv' - ['909','910','911','912','913']\n","  '''\n","  A1_Graphs_sort = Graphs_sort(A1_vert)\n","  days_pain = []\n","  days_not_pain = []\n","  all_days = []\n","  # Loop returns the bodily part, number of days, and erroneous pain or\n","  # stamina values start and end from the erroneous observation minus\n","  # number of days. Also returns the days that were not a pain.\n","  for i in P1_vert[3:]:\n","    reverse_days = A1_Graphs_sort.erroneous_values(i)\n","    if len(reverse_days[0]) > 2:\n","      days_pain += reverse_days[0]\n","      days_not_pain += reverse_days[1]\n","      all_days += reverse_days[2]\n","\n","  # Returns the frequency of activities for every day (discards the first 10).\n","  A1_activity_splice = A1_Graphs_sort.activity_frequency_splice()\n","  A1_activity_pain = A1_Graphs_sort.activity_reverse(days_pain,A1_activity_splice)\n","  A1_activity_not_pain = A1_Graphs_sort.activity_reverse(days_not_pain,A1_activity_splice)\n","  A1_activity_all = A1_Graphs_sort.activity_reverse(all_days,A1_activity_splice)\n","\n","  # Calcualtes the mean for each reverse day means, then graph '3' on x with the sum\n","  # of the erroneous means divided by their count on the y against non pain\n","  # means and all the days mean. 'y' is labeled 'Activity Frequency'.\n","  A1_pain_group_mean = A1_Graphs_sort.activity_group_mean(A1_activity_pain)\n","  A1_not_pain_group_mean = A1_Graphs_sort.activity_group_mean(A1_activity_not_pain)\n","  A1_all_group_mean = A1_Graphs_sort.activity_group_mean(A1_activity_all)\n","\n","  # Graphing section.\n","  # Summarized pain, not pain, and all data means for 3,5,7,10,and 30 days before the event.\n","  A1_graph_rgb = Graphs_rgb([])\n","  A1_reverse_mean_category = A1_graph_rgb.rgb_reverse_category(A1_pain_group_mean,A1_not_pain_group_mean,A1_all_group_mean)\n","\n","  # Mean of every reverse splice for each day.\n","  A1_day_mean_category = A1_graph_rgb.rgb_reverse_day_mean(A1_activity_pain,P1_vert_predictions)\n","  '''\n","\n","  # Graphing the prediction and observation along with classification metrics.\n","  # Usage is from 'P1_Classification_RGB_graph()'\n","  title_full = [#'','','',\n","  'Stamina',\n","  'Feet','Ankle','Calves',\n","  'Knees','Quadriceps','Gluteus','Groin',\n","  'Abdominals','Lower Back',\n","  'Latissimus Dorsi','Trapezius','Shoulders',\n","  'Chest','Triceps','Biceps',\n","  'Neck','Head']\n","  for j in range(8,11\n","                 #,len(P1_vert_predictions)\n","                 ):\n","    class_met = classification_metrics(P1_vert[j],P1_vert_predictions[j])\n","    binary = class_met.binary_classification()\n","    acc_binary = class_met.accuracy(binary[0])\n","    precision_binary = class_met.precision(binary[2],binary[3]) # tsp fp\n","    recall_binary = class_met.recall(binary[2],binary[4]) # tsp fn\n","    f1_score_binary = class_met.f1_score(binary[2],binary[3],binary[4]) # tsp fp fn\n","    classification_list = [title_full[j-3],P1_vert[j][0],acc_binary,precision_binary,recall_binary,f1_score_binary]\n","    P1_graph_rgb = Graphs_rgb([])\n","    P1_graph_rgb.rgb_prediction_observation(P1_vert_predictions[j],P1_vert[j],P1_vert_predictions[1],classification_list)\n","  return\n","\n","# One month of May, 2024 observations\n","P0_path = \"/content/P0.csv\"\n","B0_path = \"/content/B0.csv\"\n","# A0 is a TSV because there are blank cells\n","A0_path = \"/content/A0.tsv\"\n","# P0_unflipped_col = ['ID','Date','Day','Stm']\n","# P0_vert = CSV_running(P0_path,P0_unflipped_col)\n","# B0_vert = CSV_running(B0_path,0)\n","# A0_vert = CSV_running(A0_path,0)\n","\n","# Four months of July-October observations\n","# P1.csv contains the pain scale and B1.csv contains the food records\n","P1_path = \"/content/P1-Observations-PaperFigures.csv\"\n","P1_path_predictions = \"/content/P1-Prediction-PaperFigures.csv\"\n","B1_path = \"/content/B1.csv\"\n","# A1 is a tsv because of blank cells\n","# A1_path = \"/content/A1.tsv\" # The full dataset.\n","A1_path = \"/content/A1-small.tsv\"\n","# List of columns to not be flipepd\n","# P1_unflipped_col = ['ID','Date','Day','Stm','Notes','Notes2']\n","# P1_vert = CSV_running(P1_path,P1_unflipped_col)\n","# P1_vert_predictions = CSV_running(P1_path_predictions,P1_unflipped_col)\n","# B1_vert = CSV_running(B1_path,0)\n","A1_vert = CSV_running(A1_path,0)\n","'''\n","results:\n","0) number of calories burned cateogries (B1 - 0,1,2) per day\n","  vs\n","  - length of activites (A1) per day.\n","  - categorize each activity as low, medium, high (0,1,2) intensity\n","  - use a calories burned website and multiply duration by the category\n","  to estimate calories burned.\n","  - compare with calories eaten (B1).\n","'''\n","# P1_RGB_graph(P0_vert)\n","# P1_RGB_graph(P1_vert)\n","# B1_RGB_graph(B0_vert)\n","# B1_RGB_graph(B1_vert)\n","# A1_RGB_graph(A0_vert)\n","A1_RGB_graph(A1_vert)\n","# P1_Classification_RGB_graph(P1_vert,P1_vert_predictions)\n","# A1_daily_RGB_graph(P1_vert,P1_vert_predictions,A1_vert)import io\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1749480169036,"user":{"displayName":"David Leifer","userId":"06279506333224389759"},"user_tz":300},"id":"nyadd2nCLeqQ","outputId":"a002dbcf-d792-4966-9e49-b2f262e8c999"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nfor i in range(1,len(a)):\\n  print(a[:i])\\n  print(a[i])\\n  print('zzzzz')\\n\\n  print(a[-1])\""]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["#|######|### |a = 'abcdefg'\n","'''\n","for i in range(1,len(a)):\n","  print(a[:i])\n","  print(a[i])\n","  print('zzzzz')\n","\n","  print(a[-1])'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1744137032714,"user":{"displayName":"David Leifer","userId":"06279506333224389759"},"user_tz":300},"id":"SLyAYj_xtzdg","outputId":"3bb4744a-366f-4183-a0ca-7f695d31af9e"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[[1], ['b']]]\n"]}],"source":["abc = [\n","        [[0], ['a']],\n","        [[1], ['b']],\n","        [[2], ['c']],\n","        [[3], ['d']],\n","        [[4], ['e']],\n","        [[5], ['f']],\n","        [[6], ['g']],\n","        [[7], ['h']],\n","        [[8], ['i']],\n","        [[9], ['j']],\n","                      ]\n","# for i in range(10):\n","print(abc[1:2])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1744769169351,"user":{"displayName":"David Leifer","userId":"06279506333224389759"},"user_tz":300},"id":"kti4uK-eVPeZ","outputId":"1c1bc210-f9b0-4b10-b49e-62551d97b986"},"outputs":[{"name":"stdout","output_type":"stream","text":["34\n","5\n"]}],"source":["a = '1234'\n","b = '567'\n","c = '89'\n","\n","\n","if len(a) == 4:\n","  end_sub = a[2:]\n","  print(end_sub)\n","\n","if len(b) == 3:\n","  end_sub = b[:1]\n","  print(end_sub)\n","\n","if len(c) == 2:\n","  end_sub = c"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bNU5wcEd0AeC"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6wounMFJaZT3"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1fHIWbrpk4NMYcse3Xdkb-S8XiS2pxzJm","timestamp":1721260308720},{"file_id":"1STKhtVVaknUZiohwWLdfyoZ3zg6Veimx","timestamp":1721260179755}],"authorship_tag":"ABX9TyO0svrCbbcnI+gvU4LP4Yd3"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}