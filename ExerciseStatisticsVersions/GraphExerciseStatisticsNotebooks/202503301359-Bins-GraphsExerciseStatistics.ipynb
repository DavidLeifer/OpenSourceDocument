{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1fHIWbrpk4NMYcse3Xdkb-S8XiS2pxzJm","timestamp":1721260308720},{"file_id":"1STKhtVVaknUZiohwWLdfyoZ3zg6Veimx","timestamp":1721260179755}],"authorship_tag":"ABX9TyPn9qOhGTd0aLDCmU/KYzie"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["def todo(self):\n","  # List of primary issues\n","  # todo    description                                   hours   progress      Note\n","  #\n","  # todo0   A01.csv skate, long, downhill, juggling,      39.5      Processing    rgb_unique_bin\n","  #         running mean duration by category.\n","  #\n","  # todo1   A01.csv category by day of the week or\n","  #         time of day i.e. morning, afternoon, night\n","  #         or blocks of 3.\n","  #\n","  # todo2   A0.csv nltk the 'Explanation' and 'Notes'\n","  #         sections? Manual descriptions are already\n","  #         included in the write-up.\n","  #\n","  # todo3   Another tutorial chapter on merge sort.\n","  #         Compare with Python's built-in len(),\n","  #         sort(), and replace().\n","  #\n","  # todo4   The graphing part could be included in\n","  #         Chapter 1 with pandas and SciPy.\n","  #\n","  # todo5   A01.csv longboard distance, running\n","  #         distance per day were not recorded well.\n","  #\n","  # todo6   P0P1B0B1.csv timeseries graphing.             45.5     Completed\n","  #\n","  # todo7   P1.csv manual vs observed prediction\n","  #         accuracy F1 or RMSE.\n","  #\n","  # todo8   B01.csv Pearson-Correlation and day-          2        Completed    Found that there was not correlation between parametric variables.\n","  #         delayed between calories, alcohol, exercise.                        An index similar to ENSO is redundant since there were no consecutive\n","  #                                                                             observations over 4 alcohol or excessive (calorie - calorie burned).\n","  #\n","  # todo9   A01P01B01 moving window spearman correlation           Processing   Would have to sort these for rank, which was started in todo0.\n","  #         between activity, duration, time of day, pain,\n","  #         nutrients, calories, alcohol.\n","  #\n","  # todo10  tbd data filling and automatic predictions.\n","  #         idk if thats another chapter or avoided.\n","  #\n","  # todo11  Manual weather observations and PRISM data\n","  #         will be in a different GitHub to avoid confusion.\n","  #\n","  # Time spent at a computer programming\n","  # Total estimate  :\n","  # Total actual    :\n","  #\n","  # Purpose\n","  # The goal of writing this is to waste as much time as possible in between\n","  # skateboarding, lifting, or exercise to avoid overtraining while retaining\n","  # logical thought process during long stretches of unemployment. These were\n","  # written on a computer with a 1.5-2 hour battery to restrict excessive\n","  # programming by limiting hardware access.\n","  #\n","  # Abstract\n","  # No library Python with C-like syntax is used for data manipulation and\n","  # graphing whereby arrays are handled without dictionaries. The only\n","  # library used is Matplotlib for RGB graphing and to avoid writing a image or\n","  # video format that would likely spread misinformation. An implementation of\n","  # the merge sort algorithm was used to alphabetize exercise activity for\n","  # binning and graphing frequency by unique type. The built-in Python methods\n","  # for 'replace', 'split', 'len', and 'sort' were manually written for\n","  # learning purposes.\n","\n","  # Start date: 20250125\n","  # End date:\n","\n","  # Below is an exhaustive list of secondary issues.\n","\n","  # List of secondary issues\n","  # todo   description                                                  progress\n","  # todo0  rewrite parser for unicode csv str/int.\n","  # todo1  Stats class avoid NA, NAAN, -9999, etc.\n","  # todo2  refractor RGB_graphs.\n","  # todo3  monthly means on bar graphs.\n","  # todo4  organize merge_sort into another classe.                     Complete\n","  # todo5  modify merge sort to accept entire CSV.                      Class\n","  # todo6  Handle multi word activity descriptions consistently.        Class\n","  # todo7  switch the second capital letter to lower case if exists.    Class\n","  # todo8  unchain the four merge sort functions.                       Class\n","\n","  return\n","\n","import sys\n","import matplotlib.pyplot as plt\n","print(sys.version)\n","# 3.10.12 (main, Mar 22 2024, 16:50:05) [GCC 11.4.0]\n","# 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n","\n","# Interesting method to print callable methods on an object.\n","# print(dir(left_right))\n","# Example is obj.__class__ that prints the data type without using the type(obj) method\n","# type_check = str(left_right.__class__)\n","\n","#####################################################################################\n","# Part A: CSV or TSV parser class to open the file and parse the values into a list #\n","#####################################################################################\n","class CSV_Parser:\n","  # Initialize the input variables\n","  def __init__(self, data_path):\n","    self.data_path = data_path\n","  def file_opener(self):\n","    with open(self.data_path, \"r\") as data_open:\n","      data_read = data_open.read()\n","      return data_read\n","  # Index the commas and line breaks\n","  def comma_index(self, open_file, path, column_len):\n","    data_comma_place = [0]\n","    column_pl_len = 0\n","    path_split = [ext for ext in path]\n","    path_ext = \"\".join(path_split[-3:])\n","    for i in range(len(open_file)):\n","      data1_col1 = open_file[i]\n","      if path_ext == \"csv\":\n","        if data1_col1 == \",\":\n","          data_comma_place.append(i)\n","          data_comma_place.append(i+1)\n","      elif path_ext == \"tsv\":\n","        if data1_col1 == \"\\t\":\n","          data_comma_place.append(i)\n","          data_comma_place.append(i+1)\n","      if data1_col1 == \"\\n\":\n","        data_comma_place.append(i)\n","        data_comma_place.append(i+1)\n","        # If you want to use the function to get the column width, set to 1\n","        if column_len == 1:\n","          col_width = len(data_comma_place)\n","          return col_width\n","          break\n","    last_val = data_comma_place[-1] + 2\n","    data_comma_place.append(last_val)\n","    return data_comma_place\n","  # Splitting the csv characters into list of words based on indexed comma position\n","  def csv_value_list(self, data_comma_out, open_file, col_width, col_head):\n","    j = col_head\n","    data_val_list = []\n","    for i in range(len(data_comma_out)):\n","      if j >= len(data_comma_out):\n","        break\n","      comma_strt = data_comma_out[j]\n","      j += 1\n","      comma_end = data_comma_out[j]\n","      #print(comma_strt, \"and \", comma_end)\n","      j += 1\n","      data_val_list.append(open_file[comma_strt:comma_end])\n","      j = j + (col_width * 2)\n","    return data_val_list\n","  # Flipping the columns from high to low for readability\n","  # If the original value was 5, set it to equal 0 (no pain)\n","  # If the original value was 0, set it to equal to 5 (high pain) etc.\n","  # If none of those things are true, append the string (for the column header)\n","  def csv_flipper(self, csv_list, col_width):\n","    csv_flipped = []\n","    for i in csv_list:\n","      if i == str(5):\n","        n = str(1)\n","        csv_flipped.append(n)\n","      elif i == str(4):\n","        n = str(2)\n","        csv_flipped.append(n)\n","      elif i == str(3):\n","        n = str(3)\n","        csv_flipped.append(n)\n","      elif i == str(2):\n","        n = str(4)\n","        csv_flipped.append(n)\n","      elif i == str(1):\n","        n = str(5)\n","        csv_flipped.append(n)\n","      else:\n","        csv_flipped.append(i)\n","    return csv_flipped\n","\n","######################################################\n","# Part B: Get descriptive statistics of each column. #\n","######################################################\n","class Statistics:\n","  # Returns a dictionary with the header and mean\n","  def mu(self, col_list):\n","    total = 0\n","    counter = 0\n","    # Column has to have a header\n","    for i in col_list[1:]:\n","      if i == \"NA\":\n","        continue\n","      total = total + float(i)\n","      counter += 1\n","    mean = total / counter\n","    header_mean = [col_list[0], mean]\n","    return header_mean\n","  # Returns the 2-4 moment of the distribution\n","  # Different than Google Sheets sample vs population\n","  def mnt(self, header, mean, col_list):\n","    col_1 = len(col_list) - 1\n","    stn = 0\n","    skew = 0\n","    kurt = 0\n","    counter = 0\n","    for i in col_list[1:]:\n","      if i == \"NA\":\n","        continue\n","      # secondary todo: doesn't work with decimals\n","      n1 = int(i) - mean\n","      n1_sqr = n1 ** 2\n","      n1_cube = n1 ** 3\n","      n1_quad = n1 ** 4\n","      stn = stn + n1_sqr\n","      skew = skew + n1_cube\n","      kurt = kurt + n1_quad\n","      counter += 1\n","    # Sample variance (n-1)\n","    # Population variance (n)\n","    counter = (counter - 1)\n","    stn_small_sqr = float(stn) / counter\n","    stn_small = stn_small_sqr ** .5\n","    skew_small_sqr = float(skew) / counter\n","    skew_small = skew_small_sqr / (stn_small ** 3)\n","    kurt_small_sqr = float(kurt) / counter\n","    kurt_small = kurt_small_sqr / (stn_small ** 4)\n","    return [header, stn_small, skew_small, kurt_small]\n","  # Covariance and correlation\n","  def covar(self, x_mean, y_mean, col_1_list, col_2_list):\n","    col_len = len(col_1_list) - 1\n","    covar = 0\n","    x1y1_sum = 0\n","    counter = 0\n","    for i in range(col_len):\n","      #if i == col_len-2:\n","      #  break\n","      if col_1_list[i+1] == \"NA\":\n","        continue\n","      if col_2_list[i+1] == \"NA\":\n","        continue\n","      # print(i+3,i)\n","      # print(\"x_mean: \", x_mean[1], \"x_value: \", col_1_list[i+1])\n","      x1 = float(col_1_list[i+1]) - x_mean[1]\n","      y1 = float(col_2_list[i+1]) - y_mean[1]\n","      x1y1 = x1 * y1\n","      x1y1_sum = x1y1_sum + x1y1\n","      counter += 1\n","    covar = x1y1_sum / counter\n","    return covar\n","  def cor(self, covar, col_1_stnd, col_2_stnd):\n","    stnd12 = col_1_stnd * col_2_stnd\n","    cor = covar / stnd12\n","    return cor\n","\n","######################################################\n","# Part C: Data visualization with a timeseries graph #\n","######################################################\n","# 'Graph' class accepts three variables: verticle arranged 'data',\n","# the date column'date_col_num', and the data column 'data_col_num'\n","class Graph:\n","  # Initialize the input variables\n","  def __init__(self, data, date_col_num, data_col_num):\n","    self.data = data\n","    self.date_col_num = date_col_num\n","    self.data_col_num = data_col_num\n","  def hi_lo(self, data_col_num):\n","    # The date and date column to be used\n","    data_col = self.data[data_col_num]\n","    data_col_len = len(data_col)\n","    # High and low of values\n","    hi_lo_count = 1\n","    hi = data_col[1]\n","    lo = data_col[1]\n","    for e in range(len(data_col[1:])):\n","      hi_lo_count += 1\n","      if hi_lo_count == (len(data_col[1:]) + 1):\n","        break\n","      if hi < data_col[hi_lo_count]:\n","        hi = data_col[hi_lo_count]\n","      if lo > data_col[hi_lo_count]:\n","        lo = data_col[hi_lo_count]\n","    return [hi, lo]\n","  def binned(self, hi_lo):\n","    # high value (5 in this case or hi_lo[0])\n","    # The date and date column to be used\n","    # TODO it works but is not resuable for other data ranges\n","    date_col = self.data[self.date_col_num]\n","    data_col = self.data[self.data_col_num]\n","    data_col_len = len(data_col)\n","\n","    fiver = []\n","    fourer = []\n","    threer = []\n","    twoer = []\n","    oner = []\n","    lol_stm_date = []\n","    counter = 1\n","    # Binned with date value\n","    for i in range(data_col_len):\n","      P0_column = self.data[self.data_col_num]\n","      if counter == len(self.data[0]):\n","        break\n","      # secondary todo:\n","      if P0_column[counter] == \"NA\":\n","        counter += 1\n","        continue\n","      if float(P0_column[counter]) == 5:\n","        fiver.append([date_col[counter], P0_column[counter]])\n","      elif float(P0_column[counter]) == 4:\n","        fourer.append([date_col[counter], P0_column[counter]])\n","      elif float(P0_column[counter]) == 3:\n","        threer.append([date_col[counter], P0_column[counter]])\n","      elif float(P0_column[counter]) == 2:\n","        twoer.append([date_col[counter], P0_column[counter]])\n","      elif float(P0_column[counter]) == 1:\n","        oner.append([date_col[counter], P0_column[counter]])\n","      counter += 1\n","    # Combining the binned data into one dictionary\n","    lol_date_stm = [fiver, fourer, threer, twoer, oner]\n","    return lol_date_stm\n","\n","  def time_series(self, date_hi_lo, lol_date_stm):\n","    date_col = self.data[self.date_col_num]\n","    # Base of the month, plus 00 i.e. 500\n","    date_base = int(date_hi_lo[1]) - 1\n","    # The number of spaces is the 'day' (date - month) - 'prev_day_space_int'\n","    # The difference between the values is multiplied by ' ' for each 5,4,3,2,1\n","    # Value with a '+' character marking the position\n","    spacer = []\n","    prev_day_space_int = 0\n","    for i in lol_date_stm:\n","      spacer_mid = []\n","      for ii in i:\n","        # ii is [date, value] in order\n","        # an if else statement\n","        # 7 starts at 22, 31 days\n","        # 8 30 days\n","        # 9 30 days\n","        # 10 31 days\n","        day = int(ii[0]) - date_base\n","        # print(day)\n","        day_count = day - prev_day_space_int\n","        day_space_str = ((day_count-1)*2) * \" \"\n","        spacer_mid.append(day_space_str)\n","        prev_day_space_int = day\n","      prev_day_space_int = 0\n","      spacer.append(spacer_mid)\n","    y_val = [y for y in range(len(spacer),0,-1)]\n","    return [spacer, y_val]\n","  def time_series_print(self,spacer,y_val):\n","    date_col = self.data[self.date_col_num]\n","    for j,k in zip(spacer,y_val):\n","      # y values\n","      print(k,end=\" \")\n","      for l in j:\n","        print(l,end=\"\")\n","        print(\"+\",end=\" \")\n","      print()\n","    # x values\n","    for m in range(len(date_col[0])-1):\n","      print(\"  \",end=\"\")\n","      for n in date_col[1:]:\n","        print(n[m], end=\" \")\n","      print()\n","    return\n","  def time_series_write(self,header,txt_out,spacer,y_val):\n","    # Open the output file location and write data to the txt\n","    date_col = self.data[self.date_col_num]\n","    file_output = open(txt_out, \"w\")\n","    file_output.write(header)\n","    file_output.write(\"\\n\")\n","    file_output.write(\"\\n\")\n","    # y values\n","    for j,k in zip(spacer,y_val):\n","      file_output.write(str(k) + \" \")\n","      for l in j:\n","        file_output.write(str(l))\n","        file_output.write(\"+\" + \" \")\n","      file_output.write(\"\\n\")\n","    # x values\n","    for m in range(len(date_col[0])-1):\n","      file_output.write(\"  \")\n","      for n in date_col[1:]:\n","        file_output.write(str(n[m]) + \" \")\n","      file_output.write(\"\\n\")\n","    file_output.close()\n","    return\n","\n","###############################################\n","# Part D: Data visualization with a RGB graph #\n","###############################################\n","# Matplotlib for color because otherwise you would have\n","# to write hardware code to avoid using Python or C libraries.\n","class Graphs_rgb:\n","  # Initialize the input variables\n","  def __init__(self, data):\n","    self.data = data\n","\n","  # Four utility functions daisy chained to rgb_timeseries_bar()\n","  # Minor todo: unchain them lol\n","  def rgb_timeseries_mean(self,formatted_data_group):\n","    # Input is list (1-4) of lists (95) of each columns values without NA\n","    # i.e. [[dist1],[dist1],[dist1], etc]\n","    date_col_len = len(formatted_data_group[0])\n","    group_mean = []\n","    # Length of the column (95 without \"NA\" as filtered in rgb_date_time)\n","    for i in range(date_col_len):\n","      row_list = []\n","      # Length of columns to be summarized (1-4) 95 row_list values\n","      for j in range(len(formatted_data_group)):\n","        row_list.append(formatted_data_group[j][i])\n","      # Mean at each day for each group\n","      row_count = len(row_list)\n","      row_sum = sum(row_list)\n","      row_mean = row_sum / row_count\n","      group_mean.append(row_mean)\n","    return group_mean\n","\n","  def rgb_date_time(self,csv_groups,date_col):\n","    day_count = len(self.data[1])\n","    k = 0\n","    group_dist = []\n","    for i in csv_groups:\n","      dist0 = []\n","      dist1 = []\n","      dist2 = []\n","      for j in range(day_count):\n","        if j == (day_count-1):\n","          break\n","        if i[j+1] == \"NA\":\n","          continue\n","        else:\n","          # Formatting the date\n","          # year = 2024\n","          date_length = date_col[j+1]\n","          if len(date_length) < 4:\n","            month = date_length[:1]\n","            day = date_length[1:]\n","          else:\n","            month = date_length[:2]\n","            day = date_length[2:]\n","          date_format0 = month + \"/\" + day\n","          dist0.append(date_format0)\n","          dist1.append(int(i[j+1]))\n","          date_format1 = month + \"/\" + day\n","          if int(day) % 5 == 0:\n","            dist2.append(date_format1)\n","          else:\n","            dist2.append(\" \")\n","            continue\n","      group_dist.append([dist0,dist1,dist2])\n","      k += 1\n","    return group_dist\n","\n","  def rgb_P1_style(self,final_title,line):\n","    plt.yticks(range(1,6))\n","    if final_title == 'Stamina':\n","      plt.ylabel(final_title)\n","    else:\n","      plt.title(final_title)\n","      plt.ylabel(\"Pain\")\n","      if line == 1:\n","        plt.legend()\n","    return\n","\n","  def rgb_B1_style(self,final_title,line):\n","    if final_title == 'Calories':\n","      plt.yticks(range(1200,4500,400))\n","      plt.ylabel(\"Intake\")\n","      plt.title(final_title)\n","    elif final_title == 'Alcohol Servings':\n","      plt.yticks(range(0,16))\n","      plt.title(\"Alcohol\")\n","      plt.ylabel(\"Servings\")\n","    elif final_title == 'Exercise':\n","      plt.yticks(range(0,3))\n","      plt.title(final_title)\n","      # plt.ylabel(\"Calories Out\")\n","      plt.text(.1,.5, \"Calories Out \\n2 = 250+ \\n1 = 1-249\",\n","         bbox={'facecolor': 'white', 'alpha': .75, 'pad': 10})\n","    else:\n","      plt.yticks(range(1,6))\n","      if line == 1:\n","        plt.title(\"Nutrients\")\n","      else:\n","        plt.title(final_title)\n","      plt.ylabel(\"Intake\")\n","      plt.legend()\n","    return\n","\n","  # Bar plots for each column\n","  def rgb_timeseries_bar(self,title_full,start_val,P1_B1):\n","    for i in range(start_val,len(title_full)+start_val):\n","      formatted_csv_group = self.rgb_date_time([self.data[i]],self.data[1])\n","      fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(15, 5))\n","      ax.bar(formatted_csv_group[0][0], formatted_csv_group[0][1], width=0.8, align='edge')\n","      final_title = title_full[i-start_val]\n","      # Format the title, yticks, and ylabel\n","      if P1_B1 == 0:\n","        self.rgb_P1_style(final_title,0)\n","      elif P1_B1 == 1:\n","        self.rgb_B1_style(final_title,0)\n","      elif P1_B1 == 2:\n","        pass\n","        # self.rgb_A0_style(final_title,0)\n","      plt.xticks(formatted_csv_group[0][0], labels=formatted_csv_group[0][2])\n","      plt.margins()\n","      plt.grid()\n","      # plt.savefig(final_title + '.jpg')\n","    return\n","\n","\n","  # Returns a date list without blanks\n","  def rgb_date_list(self):\n","    # date_literal is 0-30 days\n","    date_literal = []\n","    # Makes a list with only the dates\n","    for i in range(1,len(self.data[2])):\n","      if len(self.data[2][i]) > 0:\n","        date_literal.append(self.data[2][i])\n","    return date_literal\n","\n","  # Multiple lines same graphs.\n","  def rgb_timeseries_line(self,title_full,start_val,groups_num,title_label,P1_B1):\n","    data = self.data\n","    # secondary todo: name instead of number position\n","    j = 1\n","    # Adding multiple lines to a single plot by group with formatting\n","    for i in range(len(groups_num)):\n","      subset0 = groups_num[i:j][0]\n","      if subset0 == groups_num[-1]:\n","        break\n","      subset1 = groups_num[i+1:j+1][0]\n","      csv_groups = data[subset0:subset1]\n","      formatted_csv_group = self.rgb_date_time(csv_groups,self.data[1])\n","      # Format subplot\n","      fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(15, 5))\n","      # Get the formatted_csv_group second list of values in each group\n","      dist1_list = [dist1[1] for dist1 in formatted_csv_group]\n","      # First 3 columns in data are ID, while the title list isn't.\n","      # Subtract each subset by the start_val of the values (excluding date, id, etc)\n","      title_group = title_full[(subset0-start_val):(subset1-start_val)]\n","      # y = each dist1 in formatted_csv_group, x = every date value, x labels = every 5th date value\n","      for k in range(len(dist1_list)):\n","        ax.plot(formatted_csv_group[0][0], dist1_list[k], label=title_group[k], linewidth=4)\n","        # Format the title, yticks, and ylabel\n","        if P1_B1 == 0:\n","          self.rgb_P1_style(title_label[j-1],1)\n","        elif P1_B1 == 1:\n","          self.rgb_B1_style(title_label[j-1],1)\n","      # Chart formatting and save\n","      plt.xticks(formatted_csv_group[0][0], labels=formatted_csv_group[0][2])\n","      plt.grid()\n","      plt.margins()\n","      #plt.savefig(title_label[j-1] + '.jpg')\n","      j += 1\n","    return\n","\n","  # Summarized with mean\n","  def rgb_timeseries_small(self,csv_groups_num,legend_label,ax):\n","    csv = self.data\n","    j = 1\n","    for i in range(len(csv_groups_num)):\n","      subset0 = csv_groups_num[i:j][0]\n","      if subset0 == csv_groups_num[-1]:\n","        break\n","      subset1 = csv_groups_num[i+1:j+1][0]\n","      csv_groups = csv[subset0:subset1]\n","      # Builds an array to skip NA and format the date\n","      # [[[dist0],[1],[2]],[[dist0],[1],[2]], etc]]]\n","      formatted_csv_group = self.rgb_date_time(csv_groups,self.data[1])\n","      # Get the formatted_csv_group second list of values in each group\n","      dist1_list = [dist1[1] for dist1 in formatted_csv_group]\n","      # Summarize each body part's group with mean\n","      dist1_group_mean = self.rgb_timeseries_mean(dist1_list)\n","      # y = group mean, x = every date value, x labels = every 5th date value\n","      # Specified in rgb_date_time function\n","      ax.plot(formatted_csv_group[0][0], dist1_group_mean, label=legend_label[j-1], linewidth=4)\n","      plt.xticks(formatted_csv_group[0][0], labels=formatted_csv_group[0][2])\n","      j += 1\n","    return\n","\n","# Merge sort is the fastest for worst case scenario sorting: N log(n)\n","# Implementation is from W3 and modified for AZ with ascii ord():\n","# https://www.w3schools.com/dsa/dsa_algo_mergesort.php\n","class Graphs_sort:\n","  # Initialize the input variables\n","  def __init__(self, data):\n","    self.data = data\n","\n","  def class_questions(self):\n","    '''\n","    # The secondary todos would make good computer science class questions.\n","\n","    # Secondary todo 6: Modify merge sort to accept entire CSV as one list input.\n","    # The output is the AZ sorted CSV.\n","\n","    # Secondary todo 7: Handle multi word activity descriptions consistently.\n","    # if first word in two word string is the same as the comparison word, use\n","    # the letter of the second word in the two word string and compare with '0'.\n","\n","    # i.e.        'Guitar 2, Rest 1' and 'Guitar'\n","    # and         'Guitar'           and 'Guitar 2, Rest 1'\n","    #\n","    # should be:  'Guitar 2, Rest 1' and 'Guitar'\n","    # and         'Guitar 2, Rest 1' and 'Guitar'\n","\n","    # Secondary todo 7: switch the second capital letter to lower case if exists.\n","    # Capital then lower case ASCII order is probably a remnant of backward\n","    # compatibility issues with  early low bit hobby computers where every word\n","    # was capitalized and it would have been a pain to rewrite the OS since\n","    # there was no market in the 1970's.\n","    # i.e. 'RPI Firmware' and 'Read' since capital 'P' is a lower number\n","    # on the ASCII chart it would be ordered first regardless of the second word's\n","    # lowercase 'e' despite 'e' appearing before 'p' alphabetically. Write a\n","    # method to switch 'P' to lower case 'p' for ASCII number ordering and swapping\n","    # the two strings. Something along the lines of:\n","\n","    if i > 0 and i is upper case:\n","      # upper case 'P' to lower case 'p'\n","      # ord(i) <- the lower case letter\n","\n","    # Secondary todo 8: Un daisy chain the functions for reusability.\n","    # The functions merge, c_replace, filter_stop are daisy chained\n","    # and called in sort_ascii with filter_stop().\n","    '''\n","    return\n","\n","  # Merge calls ord_sum to calculate the ASCII of the string.\n","  # secondary todo: refractor to accept additional columns\n","  def merge(self,left_in,right_in):\n","      result = []\n","      result_activity = []\n","      result_id = []\n","      result_dur = []\n","      i = j = 0\n","      while i < len(left_in[1]) and j < len(right_in[1]):\n","        left = left_in[0][i]\n","        right = right_in[0][j]\n","        left_activity = left_in[1][i]\n","        right_activity = right_in[1][j]\n","        left_id = left_in[2][i]\n","        right_id = right_in[2][j]\n","        left_dur = left_in[3][i]\n","        right_dur = right_in[3][j]\n","        if left < right: # or (left_activity_replace == right_activity and left < right) ?\n","          result.append(left)\n","          result_activity.append(left_activity)\n","          result_id.append(left_id)\n","          result_dur.append(left_dur)\n","          i += 1\n","        elif left > right: # or left_activity == right_activity ?\n","          result.append(right)\n","          result_activity.append(right_activity)\n","          result_id.append(right_id)\n","          result_dur.append(right_dur)\n","          j += 1\n","        else:\n","          if len(left_activity) > len(right_activity):\n","            length = len(right_activity)\n","          else: # same length?\n","            length = len(left_activity)\n","          # Find where the two words are different at k.\n","          for k in range(1,length):\n","            if left_activity[k] != right_activity[k]:\n","              break\n","          # Handles when the comparison first words are the same but\n","          # one of the comparisons have a space and second word.\n","          left_ord = ord(left_activity[k])\n","          right_ord = ord(right_activity[k])\n","          if left_activity[:length] == right_activity[:length]:\n","            if length < len(right_activity):\n","              if left_activity == right_activity[:length]:\n","                left_ord = -1\n","                # Comparison right_activity[length:] is longer and different.\n","                right_ord = ord(right_activity[length:][0])\n","          if left_activity[:k] == right_activity[:k] and left_ord < right_ord:\n","            result.append(left)\n","            result_activity.append(left_activity)\n","            result_id.append(left_id)\n","            result_dur.append(left_dur)\n","            i += 1\n","          else:\n","            result.append(right)\n","            result_activity.append(right_activity)\n","            result_id.append(right_id)\n","            result_dur.append(right_dur)\n","            j += 1\n","      result.extend(left_in[0][i:])\n","      result.extend(right_in[0][j:])\n","      result_activity.extend(left_in[1][i:])\n","      result_activity.extend(right_in[1][j:])\n","      result_id.extend(left_in[2][i:])\n","      result_id.extend(right_in[2][j:])\n","      result_dur.extend(left_in[3][i:])\n","      result_dur.extend(right_in[3][j:])\n","\n","      return [result,result_activity,result_id,result_dur]\n","\n","  # Not used: Returns the length of the input similar to len(element).\n","  def c_len(self, input):\n","    if input.__class__ == str or input.__class__ == list:\n","      total = 0\n","      for i in input:\n","        total += 1\n","    elif input.__class__ == int or input.__class__ == float:\n","      print(\"TypeError: object of type 'int' has no len()\")\n","    return total\n","\n","  # Not used: Returns the split string at subword similar to word.split(sub_word).\n","  def c_split(self,word,sub_word):\n","    j = len(sub_word) # sub_word_len\n","    word_len = len(word)\n","    start = 0\n","    split_return = []\n","    # Testing each 'word' with the length of the 'sub_word'.\n","    for i in range(word_len):\n","      sub_word_test = word[i:j]\n","      # If they match, append the sub string of the 'word'.\n","      if sub_word_test == sub_word:\n","        split_return.append(word[start:(j - len(sub_word))])\n","        start = i + len(sub_word)\n","      if j == (word_len):\n","        j = word_len\n","      else:\n","        j += 1\n","    # Append the remaining string after the final 'sub_word'.\n","    split_return.append(word[start:])\n","    return split_return\n","\n","  # Not used: Input string and replace the word with the sub_word.\n","  # Similar to Python's string.replace(word,subword)\n","  def c_replace(self, string, word, sub_word):\n","    str_replace = \"\"\n","    word_len = len(word)\n","    str_len = len(string)\n","    count = 0\n","    for i in range(str_len-word_len+1):\n","      if string[count:word_len] == word:\n","        str_replace += sub_word\n","        count += len(word)\n","        word_len += len(word)\n","      else:\n","        str_replace += string[count]\n","        count += 1\n","        word_len += 1\n","    return str_replace\n","\n","  # Filters the verb endings using c_replace(). Similar to the previous one liner:\n","  # activity_arr = [filter_word if x == filter_word + 'ing' or x == filter_word + 'ed' else x for x in self.data[6]]\n","  def filter_stop(self,column):\n","    filtered_column = []\n","    for i in column:\n","      if 'Walked' in i:\n","        filtered_column.append(\"Walk\")\n","      elif 'Juggling' in i:\n","        filtered_column.append(\"Juggle\")\n","      elif 'Driving' in i:\n","        filtered_column.append(\"Drive\")\n","      elif i == 'Lifts':\n","        # Could append since this is hard coded but I wanted to test.\n","        verb_less = self.c_replace(i, \"s\", \"\")\n","        filtered_column.append(verb_less)\n","      elif 'ing' in i:\n","        verb_less = self.c_replace(i, \"ing\", \"\")\n","        filtered_column.append(verb_less)\n","      else:\n","        filtered_column.append(i)\n","    return filtered_column\n","\n","  # Calculates duration using end - start.\n","  def sort_time(self,activity,start,end):\n","    duration = ['Duration']\n","    for i in range(1,len(start)):\n","      # Checks to see if the Activity or Start column is empty.\n","      if len(activity[i]) == 0 or len(start[i]) == 0:\n","        continue\n","      # Estimates sleep at 7 hours.\n","      elif 'Sleep' == activity[i]:\n","        duration.append(str(7*60))\n","      else:\n","        # Gets the hour.\n","        if len(end[i]) > 3:\n","          end_sub = end[i][:2]\n","        else:\n","          end_sub = end[i][0]\n","        if len(start[i]) > 3:\n","          start_sub = start[i][:2]\n","        else:\n","          start_sub = start[i][0]\n","        # Subtracts 40 minutes since there are 60 in an hour not 100.\n","        if start_sub == end_sub:\n","          duration.append(str(int(end[i]) - int(start[i])))\n","        else:\n","          hunid = (int(end_sub) - int(start_sub)) * 40\n","          duration.append(str((int(end[i]) - int(start[i])) - hunid))\n","    return duration\n","\n","  # def sort_ascii(self,time_ID,ord_list,activity_filter,duration):\n","  def sort_ascii(self,ord_list,activity_filter,time_ID,duration):\n","    length = len(time_ID) - 1\n","    step = 1\n","    while step < length:\n","      for i in range(1, length, 2 * step):\n","        # Time vs space trade off: if you want less space calculate the duration\n","        # with another loop before sorting. Otherwise, the End and Start columns\n","        # are included in sorting and space is linear * number of columns (4).\n","        left = [ord_list[i:i + step],activity_filter[i:i + step], time_ID[i:i + step], duration[i:i + step]]\n","        right = [ord_list[i + step:i + 2 * step],\n","                 activity_filter[i + step:i + 2 * step], time_ID[i + step:i + 2 * step], duration[i + step:i + 2 * step]]\n","        merged = self.merge(left, right)\n","        # Place the merged array back into the original array\n","        for j in range(len(merged[0])):\n","          ord_list[i + j] = merged[0][j]\n","          activity_filter[i + j] = merged[1][j]\n","          time_ID[i + j] = merged[2][j]\n","          duration[i + j] = merged[3][j]\n","      step *= 2  # Double the sub-array length for the next iteration\n","    return [activity_filter,time_ID,duration]\n","\n","  # Returns the time_id and unique activity lists.\n","  def sort_unique_words(self,activity_col):\n","    # A0_length is 0-225\n","    activity_unique = []\n","    # Unique words in Activity\n","    for i in range(len(activity_col)):\n","      if activity_col[i] not in activity_unique:\n","        if len(activity_col[i]) == 0:\n","          continue\n","        else:\n","          activity_unique.append(activity_col[i])\n","    return activity_unique\n","\n","  # Returns the sorted list into AZ bins.\n","  # Dimensions: 'sort_unique_words' by the number of occurances in 'sort_ascii'.\n","  def sort_unique_bin(self,sort_unique_words,sort_ascii):\n","\n","    # Once the word is different than the next word, bin the next\n","    # word (or words) since the list is already sorted.\n","\n","    # Empty 'unique_bin' is generated with int. Could use '0's but these\n","    # are 0,1,2,...n!\n","    unique_bin = [\n","        [[x],[x]] for x in range(len(sort_unique_words))\n","        ]\n","    count = 0\n","    for i in range(len(sort_ascii[0])):\n","      # Avoids checking 'sort_ascii' past the length of the list.\n","      if i == (len(sort_ascii[0])-1):\n","        break\n","      if sort_ascii[0][i] == sort_ascii[0][i+1]:\n","        # Words are the same, 'count' does not get incremented.\n","        if unique_bin[count][0][0].__class__ == str:\n","          # If first key or 'unique_bin[count]' is str, don't include it.\n","          unique_bin[count].append([sort_ascii[0][i],sort_ascii[1][i],sort_ascii[2][i]])\n","        else:\n","          unique_bin[count] = [\n","              [sort_unique_words[count]],\n","              [sort_ascii[0][i],sort_ascii[1][i],sort_ascii[2][i]]\n","              ]\n","      elif (sort_ascii[0][i-1] != sort_ascii[0][i]) and (sort_ascii[0][i] != sort_ascii[0][i+1]):\n","        # Previous word and next word are different.\n","        unique_bin[count] = [\n","            [sort_unique_words[count]],\n","            [sort_ascii[0][i],sort_ascii[1][i],sort_ascii[2][i]]\n","            ]\n","        count += 1\n","      elif (sort_ascii[0][i-1] == sort_ascii[0][i]) and (sort_ascii[0][i] != sort_ascii[0][i+1]):\n","        # Previous word is the same, next word is different.\n","        unique_bin[count].append([sort_ascii[0][i],sort_ascii[1][i],sort_ascii[2][i]])\n","        count += 1\n","      else:\n","        count += 1\n","    # Append last element of sorted list onto the bin list at end\n","    unique_bin[-1].append([sort_ascii[0][-1],\n","                           sort_ascii[1][-1],\n","                           sort_ascii[2][-1]])\n","\n","    return unique_bin\n","\n","##############################################################################\n","# Part Z: Run the functions                                                  #\n","##############################################################################\n","\n","# Part A: The path of the CSV to be parsed\n","def CSV_running(path,unflipped_col):\n","  # Create the CSV_Parser class object and open the files\n","  parser = CSV_Parser(path)\n","  read = parser.file_opener()\n","  # Index the comma position from the CSV and split the characters into their values\n","  comma_indexed = parser.comma_index(read, path, 0)\n","  # Get the width of columns of the commas\n","  comma_width = parser.comma_index(read, path, 1)\n","  # Sort the list into verticle columns\n","  # The P0 csv gets flipped, except for the Stm column\n","  # Divide by two - the list of comma places is doubled for the start/end value\n","  col_width = int(((comma_width - 1 ) / 2) - 1)\n","  vert = []\n","  for i in range(0,comma_width-1,2):\n","    value_list = parser.csv_value_list(comma_indexed, read, col_width, i)\n","    if unflipped_col == 0:\n","      vert.append(value_list)\n","    else:\n","      if value_list[0] in unflipped_col:\n","        vert.append(value_list)\n","      else:\n","        flip = parser.csv_flipper(value_list, col_width)\n","        vert.append(flip)\n","  return vert\n","\n","# One month of May, 2024 observations\n","P0_path = \"/content/P0.csv\"\n","B0_path = \"/content/B0.csv\"\n","# A0 is a TSV because there are blank cells\n","A0_path = \"/content/A0.tsv\"\n","P0_unflipped_col = ['ID','Date','Day','Stm']\n","# P0_vert = CSV_running(P0_path,P0_unflipped_col)\n","# B0_vert = CSV_running(B0_path,0)\n","A0_vert = CSV_running(A0_path,0)\n","# Four months of July-October observations\n","# P1.csv contains the pain scale and B1.csv contains the food records\n","# P1_path = \"/content/P1-Observations-PaperFigures.csv\"\n","# B1_path = \"/content/B1.csv\"\n","# A1 is a tsv because of blank cells\n","A1_path = \"/content/A1.tsv\"\n","# List of columns to not be flipepd\n","# P1_unflipped_col = ['ID','Date','Day','Stm','Notes','Notes2']\n","# P1_vert = CSV_running(P1_path,P1_unflipped_col)\n","# B1_vert = CSV_running(B1_path,0)\n","# A1_vert = CSV_running(A1_path,0)\n","\n","# Part B: Get descriptive statistics\n","def stats_def(P1_vert,B1_vert):\n","  stats_class = Statistics()\n","  # The first three columns are skipped because they are ID, Date, and Day\n","  # These two loops calculate the means and moments\n","  P1_means_list = []\n","  P1_stnd_list = []\n","  B1_means_list = []\n","  B1_stnd_list = []\n","  # secondary todo: might make these functions\n","  for l in P1_vert[3:]:\n","    P1_means = stats_class.mu(l)\n","    P1_means_list.append(P1_means)\n","    P1_mnt2_4 = stats_class.mnt(P1_means[0],P1_means[1],l)\n","    P1_stnd_list.append(P1_mnt2_4[1])\n","  for m in B1_vert[2:]:\n","    B1_means = stats_class.mu(m)\n","    B1_means_list.append(B1_means)\n","    B1_mnt2_4 = stats_class.mnt(B1_means[0],B1_means[1],m)\n","    B1_stnd_list.append(B1_mnt2_4[1])\n","  # The nested loops calculates the covariance and correlations between B0 and P0\n","  for n in range(len(P1_vert[3:])):\n","    print(\"x: \", P1_vert[n+3][0])\n","    for o in range(len(B1_vert[2:])):\n","      print(\"    and \", B1_vert[o+2][0])\n","      P1B1_covar = stats_class.covar(P1_means_list[n],B1_means_list[o],P1_vert[n+3],B1_vert[o+2])\n","      P1B1_cor = stats_class.cor(P1B1_covar,P1_stnd_list[n],B1_stnd_list[o])\n","      print(P1B1_cor)\n","    print()\n","\n","# Part C: Data visualization ASCII\n","def P1_ASCII_graph(P1_vert):\n","  title_full = ['Stamina',\n","                'Feet','Ankle','Calves',\n","                'Knees','Quadriceps','Gluteus','Groin',\n","                'Abdominals','Lower Back',\n","                'Latissimus Dorsi','Trapezius','Shoulders',\n","                'Chest','Triceps','Biceps',\n","                'Neck','Head']\n","  graph_count = 3\n","  for p in P1_vert[3:]:\n","    #if graph_count == 4:\n","    #  break\n","    # Initialize graph class\n","    P1_graph = Graph(P1_vert, 1, graph_count)\n","    # ASCII Graphs\n","    # date_col_num = 1 # data_col_num = each successive column\n","    # this would be a loop over columns 3-20, 1st column is the date\n","    # print(p[0])\n","    # print()\n","    P1_hi_lo = P1_graph.hi_lo(graph_count)\n","    date_hi_lo = P1_graph.hi_lo(1)\n","    P1_binned = P1_graph.binned(P1_hi_lo)\n","    P1_time_series = P1_graph.time_series(date_hi_lo,P1_binned)\n","    # P1_graph.time_series_print(P1_time_series[0],P1_time_series[1])\n","    # P1_file_out = \"/content/P1_\" + p[0] + \".txt\"\n","    # P1_time_series_write = P1_graph.time_series_write(p[0],P1_file_out,P1_time_series[0],P1_time_series[1])\n","    # print(\"\\n\")\n","    graph_count += 1\n","\n","def B1_ASCII_graph(B1_vert):\n","  # Did not finish\n","  graph_count = 2\n","  for p in B1_vert[2:]:\n","    #if graph_count == 4:\n","    #  break\n","    # Initialize graph class\n","    B1_graph = Graph(B1_vert, 1, graph_count)\n","    # ASCII Graphs\n","    # date_col_num = 1 # data_col_num = each successive column\n","    # this would be a loop over columns 3-20, 1st column is the date\n","    # print(p[0])\n","    # print()\n","    B1_hi_lo = B1_graph.hi_lo(graph_count)\n","    date_hi_lo = B1_graph.hi_lo(1)\n","    B1_binned = B1_graph.binned(B1_hi_lo)\n","    B1_time_series = B1_graph.time_series(date_hi_lo,B1_binned)\n","    # B0_graph.time_series_print(B0_time_series[0],B0_time_series[1])\n","    # print()\n","    # B0_file_out = \"/content/B0_\" + p[0] + \".txt\"\n","    # B0_time_series_write = B0_graph.time_series_write(p[0],B0_file_out,B0_time_series[0],B0_time_series[1])\n","    # print(\"\\n\")\n","    graph_count += 1\n","\n","# Part D: Data visualization RGB\n","def P1_RGB_graph(P1_vert):\n","\n","  title_full = ['Stamina',\n","                'Feet','Ankle','Calves',\n","                'Knees','Quadriceps','Gluteus','Groin',\n","                'Abdominals','Lower Back',\n","                'Latissimus Dorsi','Trapezius','Shoulders',\n","                'Chest','Triceps','Biceps',\n","                'Neck','Head']\n","  P1_rgb = Graphs_rgb(P1_vert)\n","  P1_B1 = 0\n","  start_val = 3\n","  # Draws the bar charts\n","  P1_rgb_bar = P1_rgb.rgb_timeseries_bar(title_full,start_val,P1_B1)\n","  # RGB Line Graphs by Group\n","  # Uses the position of each body part name in the title_full list\n","  P1_groups_num = [3,4,7,11,13,16,19,21]\n","  P1_title_label = ['Stamina','Lower Legs','Upper Legs','Core','Upper Back','Arms','Head']\n","  # P1_rgb_line = P1_rgb.rgb_timeseries_line(title_full,start_val,P1_groups_num,P1_title_label,P1_B1)\n","\n","  # Line graphs by upper/lower body group means\n","  def small():\n","    csv_groups_list = [[3,4],[4,7,11,13],[13,16,19,21]]\n","    legend_label = [['Stamina'],['Lower Legs','Upper Legs','Core'],['Upper Back','Arms','Head']]\n","    k0 = 0\n","    for csv_groups_num in csv_groups_list:\n","      fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(15, 5))\n","      P1_rgb_line_smallest = P1_rgb.rgb_timeseries_small(csv_groups_num,legend_label[k0],ax)\n","      # Plot formatting\n","      plt.margins()\n","      plt.grid()\n","      plt.yticks(range(1,6))\n","      if sum(csv_groups_num) == sum(csv_groups_list[1]):\n","        ax.legend()\n","        plt.title(\"Lower Body\")\n","        plt.ylabel(\"Pain\")\n","        #plt.savefig(\"Lower Body Pain.jpg\")\n","      elif sum(csv_groups_num) == sum(csv_groups_list[2]):\n","        ax.legend()\n","        plt.title(\"Upper Body\")\n","        plt.ylabel(\"Pain\")\n","        #plt.savefig(\"Upper Body Pain.jpg\")\n","      else:\n","        plt.ylabel(\"Stamina\")\n","        #plt.savefig(\"Stamina.jpg\")\n","      k0 += 1\n","\n","  # Smallest on one graph\n","  def smallest():\n","    # csv_groups_list = [[3,4],[4,12],[13,21]]\n","    # legend_label = [['Stamina'],['Lower Body'], ['Upper Body']]\n","    csv_groups_list = [[4,21]]\n","    legend_label = [['Pain']]\n","    fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(15, 5))\n","    k1 = 0\n","    for csv_groups_num in csv_groups_list:\n","      P1_rgb_line_smallest = P1_rgb.rgb_timeseries_small(csv_groups_num,legend_label[k1],ax)\n","      k1 += 1\n","    # Plot formatting\n","    plt.margins()\n","    plt.grid()\n","    # plt.legend()\n","    plt.yticks(range(1,6))\n","    plt.ylabel(\"Pain\")\n","    plt.savefig('P1_smallerest.jpg')\n","  # small()\n","  # smallest()\n","\n","def B1_RGB_graph(B1_vert):\n","  title_full = ['Calories','Exercise',            # Group 0\n","              'Salt', 'Fat', 'Protein',           # Group 1\n","              'Carbohydrates', 'Alcohol Servings' # Group 3\n","              ]                                   # etc\n","  B1_rgb = Graphs_rgb(B1_vert)\n","  P1_B1 = 1\n","  start_val = 2\n","  # Part D RGB Graphs: B1.csv\n","  B1_rgb_bar = B1_rgb.rgb_timeseries_bar(title_full,start_val,P1_B1)\n","  # RGB Line Graphs by Group for B0.csv\n","  # Uses the position of each title in the title_full list\n","  B1_groups_num = [2,3,4,8,9]\n","  B1_title_label = ['Calories','Exercise','Nutrients','Alcohol Servings']\n","  # Line graph is not appropriate for calories, exercise, and alcohol servings\n","  #B1_rgb_line = B1_rgb.rgb_timeseries_line(title_full,start_val,B1_groups_num,B1_title_label,P1_B1)\n","\n","# A0_vert\n","def A0_RGB_graph(A0_vert):\n","  title_full = ['','',            # Group 0\n","              '', '',           # Group 1\n","              '', '','' # Group 3\n","              ]                                   # etc\n","  A0_sort = Graphs_sort(A0_vert)\n","  P1_B1 = 2\n","  start_val = 1\n","  # Part D RGB Graphs: A0.tsv\n","  # Calculates the duration of each activity.\n","  A0_sort_duration = A0_sort.sort_time(A0_sort.data[6],A0_sort.data[4],A0_sort.data[5])\n","  # Removes endings for similar words such as: 'Walk', 'Walks', 'Walked', 'Walking'.\n","  A0_activity_filter = A0_sort.filter_stop(A0_sort.data[6])\n","  # Merge sort the list using.\n","  ord_list = ['ord_list'] + [ord(A0_activity_filter[x][0]) for x in range(1,len(A0_activity_filter))]\n","  A0_sort_merged = A0_sort.sort_ascii(ord_list,A0_activity_filter,A0_sort.data[1],A0_sort_duration)\n","  # Finds the unique occurances of each word in 'Activity'.\n","  A0_sort_unique = A0_sort.sort_unique_words(A0_sort_merged[0])\n","  # Bins the sorted list using the unique words.\n","  A0_sort_bin = A0_sort.sort_unique_bin(A0_sort_unique,A0_sort_merged)\n","  # Graph occurrences of activity.\n","\n","  # Graph the duration of each activity.\n","\n","# A1_vert\n","def A1_RGB_graph(A0_vert):\n","  title_full = ['','',            # Group 0\n","              '', '',           # Group 1\n","              '', '','' # Group 3\n","              ]                                   # etc\n","  A1_rgb = Graphs_rgb(A1_vert)\n","  P1_B1 = 2\n","  start_val = 1\n","  # Part D RGB Graphs: A0.tsv\n","  filter_word = 'Walk'\n","  A1_rgb_unique_words = A1_rgb.rgb_unique_words(filter_word)\n","  A1_rgb_time = A1_rgb.rgb_time()\n","\n","  # A1_rgb_duration = A1_rgb.rgb_duration(A1_rgb_unique_words,A1_rgb_time)\n","  # A1_rgb.rgb_date_list()\n","  # RGB Line Graphs by Group for B0.csv\n","  # Uses the position of each title in the title_full list\n","  # B1_groups_num = [2,3,4,8,9]\n","  # B1_title_label = ['Calories','Exercise','Nutrients','Alcohol Servings']\n","  # Line graph is not appropriate for calories, exercise, and alcohol servings\n","  # B1_rgb_line = B1_rgb.rgb_timeseries_line(title_full,start_val,B1_groups_num,B1_title_label,P1_B1)\n","# P1_RGB_graph(P1_vert)\n","\n","# B1_RGB_graph(B1_vert)\n","A0_RGB_graph(A0_vert)\n","# A1_RGB_graph(A1_vert)"],"metadata":{"id":"IxtfVwAjhyaV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743360400735,"user_tz":300,"elapsed":140,"user":{"displayName":"David Leifer","userId":"06279506333224389759"}},"outputId":"206eb16d-5667-4a76-b9ea-b4db14da233e"},"execution_count":114,"outputs":[{"output_type":"stream","name":"stdout","text":["3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n","Activty Time_ID Duration\n","Animation 48 120\n","Animation 35 240\n","Cleaned room 101 120\n","Coffee 199 25\n","Coffee 187 90\n","Coffee 175 30\n","Coffee 160 30\n","Coffee 147 15\n","Coffee 134 30\n","Coffee 117 15\n","Computer 162 60\n","Cook 168 45\n","Cook 140 90\n","Cook 115 55\n","Cool down 90 35\n","Core 32 15\n","Drive 97 60\n","Dynamic warmup 88 15\n","Dynamic warmup 51 10\n","Dynamic warmup 20 15\n","Dynamic warmup 5 15\n","Dynamic warmup 1 15\n","Eat 208 30\n","Eat 203 30\n","Eat 201 30\n","Eat 198 5\n","Eat 195 60\n","Eat 191 90\n","Eat 186 60\n","Eat 183 60\n","Eat 181 75\n","Eat 174 60\n","Eat 171 30\n","Eat 170 30\n","Eat 169 30\n","Eat 163 15\n","Eat 159 5\n","Eat 156 60\n","Eat 148 15\n","Eat 143 60\n","Eat 110 45\n","Eat 92 70\n","Eat 0 116 15\n","Eat 1 128 30\n","Guitar 188 30\n","Guitar 176 105\n","Guitar 161 60\n","Guitar 155 45\n","Guitar 135 30\n","Guitar 113 15\n","Guitar 112 30\n","Guitar 104 120\n","Guitar 84 30\n","Guitar 83 30\n","Guitar 76 30\n","Guitar 71 90\n","Guitar 68 60\n","Guitar 65 60\n","Guitar 59 45\n","Guitar 49 40\n","Guitar 43 30\n","Guitar 40 60\n","Guitar 36 45\n","Guitar 26 60\n","Guitar 19 30\n","Guitar 13 60\n","Guitar 11 60\n","Guitar 4 150\n","Guitar 0 223 30\n","Guitar 0 219 30\n","Guitar 0 202 60\n","Guitar 0 118 50\n","Guitar 1 226 90\n","Guitar 1 222 30\n","Guitar 1 217 30\n","Guitar 1 214 75\n","Guitar 1 207 30\n","Guitar 2, Rest 1 196 240\n","Guitar 2, Rest 1 184 240\n","Guitar 2, Rest 1 172 30\n","Guitar 2, Rest 1 157 60\n","Guitar 2, Rest 1 145 30\n","Guitar 2, Rest 1 131 210\n","Juggle 225 60\n","Juggle 215 60\n","Juggle 213 60\n","Juggle 211 30\n","Juggle 205 60\n","Juggle 192 15\n","Juggle 138 15\n","Juggle 30 25\n","Lift 180 15\n","Lift 166 15\n","Lift 7 15\n","Longboard 221 75\n","Longboard 216 30\n","Longboard 151 45\n","Longboard 0 167 45\n","Lotion 34 120\n","Mobility 165 15\n","Mobility 105 30\n","Mobility 103 60\n","Mobility 102 5\n","Mobility 99 60\n","Mobility 98 60\n","Mobility 89 45\n","Mobility 78 45\n","Mobility 74 15\n","Mobility 70 30\n","Mobility 63 15\n","Mobility 62 15\n","Mobility 61 30\n","Mobility 47 15\n","Mobility 0.0 122 5\n","Mobility 0.1 149 30\n","Mobility 1.0 123 10\n","Mobility 2.0 124 15\n","Mobility 3.0 125 10\n","Mobility 3.1 153 15\n","Ollie Notes 82 60\n","Plyometrics 224 120\n","Plyometrics 220 120\n","Plyometrics 52 30\n","Plyometrics 37 45\n","Plyometrics 2 15\n","Plyometrics 0 121 60\n","Plyometrics 1 179 15\n","Pre exercise test 189 15\n","Pre exercise test 178 15\n","Pre exercise test 137 15\n","Pre exercise test 120 40\n","RPI Firmware 12 90\n","Read 200 180\n","Read 194 60\n","Read 142 210\n","Read 129 60\n","Read 111 30\n","Read 100 180\n","Read 96 240\n","Read 94 240\n","Read 86 90\n","Read 85 90\n","Read 81 120\n","Read 73 60\n","Read 64 210\n","Read 44 60\n","Read 41 270\n","Relax 177 15\n","Relax 164 5\n","Relax 150 30\n","Relax 136 15\n","Relax 119 15\n","Rest 95 630\n","Rest 93 210\n","Rest 80 195\n","Rest 58 90\n","Rest 56 45\n","Rest 55 704\n","Rest 39 300\n","Rest 29 285\n","Rest 25 300\n","Rest 18 105\n","Rest 16 195\n","Rest 0 204 135\n","Rest 0 193 165\n","Rest 0 182 270\n","Rest 0 152 150\n","Rest 0 141 180\n","Rest 0 126 120\n","Rest 1 206 75\n","Rest 2 209 240\n","Rugby 67 90\n","Run 218 60\n","Run 38 30\n","Run, walk 54 50\n","Run, walk 23 30\n","Shower 91 15\n","Skateboard basement 212 120\n","Skateboard basement 139 60\n","Skateboard basement 107 30\n","Skateboard basement 87 30\n","Skateboard basement 60 315\n","Skateboard basement 22 30\n","Skateboard basement 6 30\n","Skateboard outside 190 60\n","Skateboard outside 109 15\n","Skateboard outside 108 15\n","Skateboard outside 79 90\n","Skateboard outside 45 60\n","Sleep 210 420\n","Sleep 197 420\n","Sleep 185 420\n","Sleep 173 420\n","Sleep 158 420\n","Sleep 154 420\n","Sleep 146 420\n","Sleep 133 420\n","Stretch 69 30\n","Stretch 57 15\n","Stretch 50 10\n","Stretch 42 15\n","Stretch 31 20\n","Stretch 27 60\n","Stretch 24 15\n","Stretch 17 15\n","Stretch 0 90\n","Tests 144 15\n","Tests 130 30\n","Upperbody 53 20\n","Walk 132 30\n","Walk 127 45\n","Walk 114 60\n","Walk 106 30\n","Walk 77 30\n","Walk 75 30\n","Walk 72 30\n","Walk 66 30\n","Walk 46 30\n","Walk 33 30\n","Walk 28 60\n","Walk 10 30\n","Walk 3 45\n","Wrench 21 15\n","Wrench 15 60\n","Wrench 14 60\n","Wrench 9 120\n","Wrench 8 120\n","228\n","0 ['Activty', 'Time_ID', 'Duration']\n","1 ['Animation', '48', '120']\n","2 ['Animation', '35', '240']\n","3 ['Cleaned room', '101', '120']\n","4 ['Coffee', '199', '25']\n","5 ['Coffee', '187', '90']\n","6 ['Coffee', '175', '30']\n","7 ['Coffee', '160', '30']\n","8 ['Coffee', '147', '15']\n","9 ['Coffee', '134', '30']\n","10 ['Coffee', '117', '15']\n","11 ['Computer', '162', '60']\n","12 ['Cook', '168', '45']\n","13 ['Cook', '140', '90']\n","14 ['Cook', '115', '55']\n","15 ['Cool down', '90', '35']\n","16 ['Core', '32', '15']\n","17 ['Drive', '97', '60']\n","18 ['Dynamic warmup', '88', '15']\n","19 ['Dynamic warmup', '51', '10']\n","20 ['Dynamic warmup', '20', '15']\n","21 ['Dynamic warmup', '5', '15']\n","22 ['Dynamic warmup', '1', '15']\n","23 ['Eat', '208', '30']\n","24 ['Eat', '203', '30']\n","25 ['Eat', '201', '30']\n","26 ['Eat', '198', '5']\n","27 ['Eat', '195', '60']\n","28 ['Eat', '191', '90']\n","29 ['Eat', '186', '60']\n","30 ['Eat', '183', '60']\n","31 ['Eat', '181', '75']\n","32 ['Eat', '174', '60']\n","33 ['Eat', '171', '30']\n","34 ['Eat', '170', '30']\n","35 ['Eat', '169', '30']\n","36 ['Eat', '163', '15']\n","37 ['Eat', '159', '5']\n","38 ['Eat', '156', '60']\n","39 ['Eat', '148', '15']\n","40 ['Eat', '143', '60']\n","41 ['Eat', '110', '45']\n","42 ['Eat', '92', '70']\n","43 ['Eat 0', '116', '15']\n","44 ['Eat 1', '128', '30']\n","45 ['Guitar', '188', '30']\n","46 ['Guitar', '176', '105']\n","47 ['Guitar', '161', '60']\n","48 ['Guitar', '155', '45']\n","49 ['Guitar', '135', '30']\n","50 ['Guitar', '113', '15']\n","51 ['Guitar', '112', '30']\n","52 ['Guitar', '104', '120']\n","53 ['Guitar', '84', '30']\n","54 ['Guitar', '83', '30']\n","55 ['Guitar', '76', '30']\n","56 ['Guitar', '71', '90']\n","57 ['Guitar', '68', '60']\n","58 ['Guitar', '65', '60']\n","59 ['Guitar', '59', '45']\n","60 ['Guitar', '49', '40']\n","61 ['Guitar', '43', '30']\n","62 ['Guitar', '40', '60']\n","63 ['Guitar', '36', '45']\n","64 ['Guitar', '26', '60']\n","65 ['Guitar', '19', '30']\n","66 ['Guitar', '13', '60']\n","67 ['Guitar', '11', '60']\n","68 ['Guitar', '4', '150']\n","69 ['Guitar 0', '223', '30']\n","70 ['Guitar 0', '219', '30']\n","71 ['Guitar 0', '202', '60']\n","72 ['Guitar 0', '118', '50']\n","73 ['Guitar 1', '226', '90']\n","74 ['Guitar 1', '222', '30']\n","75 ['Guitar 1', '217', '30']\n","76 ['Guitar 1', '214', '75']\n","77 ['Guitar 1', '207', '30']\n","78 ['Guitar 2, Rest 1', '196', '240']\n","79 ['Guitar 2, Rest 1', '184', '240']\n","80 ['Guitar 2, Rest 1', '172', '30']\n","81 ['Guitar 2, Rest 1', '157', '60']\n","82 ['Guitar 2, Rest 1', '145', '30']\n","83 ['Guitar 2, Rest 1', '131', '210']\n","84 ['Juggle', '225', '60']\n","85 ['Juggle', '215', '60']\n","86 ['Juggle', '213', '60']\n","87 ['Juggle', '211', '30']\n","88 ['Juggle', '205', '60']\n","89 ['Juggle', '192', '15']\n","90 ['Juggle', '138', '15']\n","91 ['Juggle', '30', '25']\n","92 ['Lift', '180', '15']\n","93 ['Lift', '166', '15']\n","94 ['Lift', '7', '15']\n","95 ['Longboard', '221', '75']\n","96 ['Longboard', '216', '30']\n","97 ['Longboard', '151', '45']\n","98 ['Longboard 0', '167', '45']\n","99 ['Lotion', '34', '120']\n","100 ['Mobility', '165', '15']\n","101 ['Mobility', '105', '30']\n","102 ['Mobility', '103', '60']\n","103 ['Mobility', '102', '5']\n","104 ['Mobility', '99', '60']\n","105 ['Mobility', '98', '60']\n","106 ['Mobility', '89', '45']\n","107 ['Mobility', '78', '45']\n","108 ['Mobility', '74', '15']\n","109 ['Mobility', '70', '30']\n","110 ['Mobility', '63', '15']\n","111 ['Mobility', '62', '15']\n","112 ['Mobility', '61', '30']\n","113 ['Mobility', '47', '15']\n","114 ['Mobility 0.0', '122', '5']\n","115 ['Mobility 0.1', '149', '30']\n","116 ['Mobility 1.0', '123', '10']\n","117 ['Mobility 2.0', '124', '15']\n","118 ['Mobility 3.0', '125', '10']\n","119 ['Mobility 3.1', '153', '15']\n","120 ['Ollie Notes', '82', '60']\n","121 ['Plyometrics', '224', '120']\n","122 ['Plyometrics', '220', '120']\n","123 ['Plyometrics', '52', '30']\n","124 ['Plyometrics', '37', '45']\n","125 ['Plyometrics', '2', '15']\n","126 ['Plyometrics 0', '121', '60']\n","127 ['Plyometrics 1', '179', '15']\n","128 ['Pre exercise test', '189', '15']\n","129 ['Pre exercise test', '178', '15']\n","130 ['Pre exercise test', '137', '15']\n","131 ['Pre exercise test', '120', '40']\n","132 ['RPI Firmware', '12', '90']\n","133 ['Read', '200', '180']\n","134 ['Read', '194', '60']\n","135 ['Read', '142', '210']\n","136 ['Read', '129', '60']\n","137 ['Read', '111', '30']\n","138 ['Read', '100', '180']\n","139 ['Read', '96', '240']\n","140 ['Read', '94', '240']\n","141 ['Read', '86', '90']\n","142 ['Read', '85', '90']\n","143 ['Read', '81', '120']\n","144 ['Read', '73', '60']\n","145 ['Read', '64', '210']\n","146 ['Read', '44', '60']\n","147 ['Read', '41', '270']\n","148 ['Relax', '177', '15']\n","149 ['Relax', '164', '5']\n","150 ['Relax', '150', '30']\n","151 ['Relax', '136', '15']\n","152 ['Relax', '119', '15']\n","153 ['Rest', '95', '630']\n","154 ['Rest', '93', '210']\n","155 ['Rest', '80', '195']\n","156 ['Rest', '58', '90']\n","157 ['Rest', '56', '45']\n","158 ['Rest', '55', '704']\n","159 ['Rest', '39', '300']\n","160 ['Rest', '29', '285']\n","161 ['Rest', '25', '300']\n","162 ['Rest', '18', '105']\n","163 ['Rest', '16', '195']\n","164 ['Rest 0', '204', '135']\n","165 ['Rest 0', '193', '165']\n","166 ['Rest 0', '182', '270']\n","167 ['Rest 0', '152', '150']\n","168 ['Rest 0', '141', '180']\n","169 ['Rest 0', '126', '120']\n","170 ['Rest 1', '206', '75']\n","171 ['Rest 2', '209', '240']\n","172 ['Rugby', '67', '90']\n","173 ['Run', '218', '60']\n","174 ['Run', '38', '30']\n","175 ['Run, walk', '54', '50']\n","176 ['Run, walk', '23', '30']\n","177 ['Shower', '91', '15']\n","178 ['Skateboard basement', '212', '120']\n","179 ['Skateboard basement', '139', '60']\n","180 ['Skateboard basement', '107', '30']\n","181 ['Skateboard basement', '87', '30']\n","182 ['Skateboard basement', '60', '315']\n","183 ['Skateboard basement', '22', '30']\n","184 ['Skateboard basement', '6', '30']\n","185 ['Skateboard outside', '190', '60']\n","186 ['Skateboard outside', '109', '15']\n","187 ['Skateboard outside', '108', '15']\n","188 ['Skateboard outside', '79', '90']\n","189 ['Skateboard outside', '45', '60']\n","190 ['Sleep', '210', '420']\n","191 ['Sleep', '197', '420']\n","192 ['Sleep', '185', '420']\n","193 ['Sleep', '173', '420']\n","194 ['Sleep', '158', '420']\n","195 ['Sleep', '154', '420']\n","196 ['Sleep', '146', '420']\n","197 ['Sleep', '133', '420']\n","198 ['Stretch', '69', '30']\n","199 ['Stretch', '57', '15']\n","200 ['Stretch', '50', '10']\n","201 ['Stretch', '42', '15']\n","202 ['Stretch', '31', '20']\n","203 ['Stretch', '27', '60']\n","204 ['Stretch', '24', '15']\n","205 ['Stretch', '17', '15']\n","206 ['Stretch', '0', '90']\n","207 ['Tests', '144', '15']\n","208 ['Tests', '130', '30']\n","209 ['Upperbody', '53', '20']\n","210 ['Walk', '132', '30']\n","211 ['Walk', '127', '45']\n","212 ['Walk', '114', '60']\n","213 ['Walk', '106', '30']\n","214 ['Walk', '77', '30']\n","215 ['Walk', '75', '30']\n","216 ['Walk', '72', '30']\n","217 ['Walk', '66', '30']\n","218 ['Walk', '46', '30']\n","219 ['Walk', '33', '30']\n","220 ['Walk', '28', '60']\n","221 ['Walk', '10', '30']\n","222 ['Walk', '3', '45']\n","223 ['Wrench', '21', '15']\n","224 ['Wrench', '15', '60']\n","225 ['Wrench', '14', '60']\n","226 ['Wrench', '9', '120']\n","227 ['Wrench', '8', '120']\n"]}]},{"cell_type":"code","source":["3 # a = [1, 2, 3, 4, 5]\n","# print(a[1:])\n"],"metadata":{"id":"6pQFdEQFs7F9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742421488052,"user_tz":300,"elapsed":13,"user":{"displayName":"David Leifer","userId":"06279506333224389759"}},"outputId":"348f8d71-72dd-452f-f590-aeb470682e13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4\n","4\n"]}]},{"cell_type":"code","source":["a = 'abcdefg'\n","for i in range(1,len(a)):\n","print(a[:i])\n","print(a[i])\n","print('zzzzz')\n","\n","print(a[-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nyadd2nCLeqQ","executionInfo":{"status":"ok","timestamp":1742741830911,"user_tz":300,"elapsed":9,"user":{"displayName":"David Leifer","userId":"06279506333224389759"}},"outputId":"56b66fbe-922a-4d93-f653-15d5d6016280"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["a\n","b\n","zzzzz\n","ab\n","c\n","zzzzz\n","abc\n","d\n","zzzzz\n","abcd\n","e\n","zzzzz\n","abcde\n","f\n","zzzzz\n","abcdef\n","g\n","zzzzz\n","g\n"]}]},{"cell_type":"code","source":["abc = [\n","        [[0], ['a']],\n","        [[1], ['b']],\n","        [[2], ['c']],\n","        [[3], ['d']],\n","        [[4], ['e']],\n","        [[5], ['f']],\n","        [[6], ['g']],\n","        [[7], ['h']],\n","        [[8], ['i']],\n","        [[9], ['j']],\n","                      ]\n","for i in range(10):\n","  print(abc[i][0])"],"metadata":{"id":"SLyAYj_xtzdg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743284417480,"user_tz":300,"elapsed":34,"user":{"displayName":"David Leifer","userId":"06279506333224389759"}},"outputId":"4d67175c-5e33-49cd-ee87-cfc708217517"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0]\n","[1]\n","[2]\n","[3]\n","[4]\n","[5]\n","[6]\n","[7]\n","[8]\n","[9]\n"]}]},{"cell_type":"code","source":["unique_bin = []\n","count = 0\n","\n","try:\n","  print(unique_bin[count][0])\n","except IndexError as e:\n","  print(e)"],"metadata":{"id":"hZ37WDSXunLW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743343743146,"user_tz":300,"elapsed":13,"user":{"displayName":"David Leifer","userId":"06279506333224389759"}},"outputId":"9a6f2336-368d-4bfe-af34-78b5ec9e8aeb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["list index out of range\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"kti4uK-eVPeZ"},"execution_count":null,"outputs":[]}]}