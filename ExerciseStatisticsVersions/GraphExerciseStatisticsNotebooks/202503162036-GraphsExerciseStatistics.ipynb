{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1fHIWbrpk4NMYcse3Xdkb-S8XiS2pxzJm","timestamp":1721260308720},{"file_id":"1STKhtVVaknUZiohwWLdfyoZ3zg6Veimx","timestamp":1721260179755}],"authorship_tag":"ABX9TyPlHgu1FCm2ibFQi29O0fnc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["def todo(self):\n","  # List of primary issues\n","  # todo    description                                   hours   progress      Note\n","  #\n","  # todo0   A0/1.csv skate, long, downhill, juggling,     17.5      Processing    rgb_unique_bin\n","  #         running mean duration by category.\n","  # todo1   A0/1.csv longboard distance, running\n","  #         distance per day, overall mean\n","  # todo2   P0.csv observations, mean in each,            20      Halted        At ARIMA\n","  #         category ARIMA (after todo3).\n","  # todo3   p1 observations, manual prediction, mean in   23.5    Halted        At ARIMA\n","  #         each category, ARIMA.\n","  # todo4   B01.csv calories, salt, fat, protein, carb,   2       Halted        At ARIMA\n","  #         alc each day, overall mean, ARIMA (todo3).\n","  # todo5   B01 weather summary, OS PRISM, exercise\n","  #         intensity.\n","  # todo6   B01 (moving window) correlation between\n","  #         exercise and ___.\n","  #         (calorie, real weather, alcohol)\n","  # todo7   B01 moving window spearman correlation\n","  #         between todo0 duration and pain, calories\n","  #         burned. duration and nutrients. real and\n","  #         summary weather. pain and real weather.\n","  #         pain and summary weather. ???.\n","  # todo8   tbd data filling idk if thats another\n","  #         chapter or avoided.\n","  #\n","  # Time spent at a computer programming\n","  # Total estimate  :\n","  # Total actual    :\n","  #\n","  # Summary\n","  # asdfasdf\n","  #\n","  # Start date: 20250125\n","  # End date:\n","\n","  # There is an exhaustive list of excessive secondary issues.\n","\n","  # List of secondary issues\n","  # todo   description                              progress\n","  # todo0  rewrite parser for unicode csv str/int\n","  # todo1  Stats class avoid NA, NAAN, -9999\n","  # todo2  refractor RGB_graphs\n","  # todo3  monthly means on bar graphs\n","  # todo4  unchain the four merge sort functions\n","  # todo5  modify merge sort to accept entire CSV\n","  # todo6  organize rgb_graphs into additional classes\n","  return\n","\n","import sys\n","import matplotlib.pyplot as plt\n","print(sys.version)\n","# 3.10.12 (main, Mar 22 2024, 16:50:05) [GCC 11.4.0]\n","# 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n","\n","#####################################################################################\n","# Part A: CSV or TSV parser class to open the file and parse the values into a list #\n","#####################################################################################\n","class CSV_Parser:\n","  # Initialize the input variables\n","  def __init__(self, data_path):\n","    self.data_path = data_path\n","  def file_opener(self):\n","    with open(self.data_path, \"r\") as data_open:\n","      data_read = data_open.read()\n","      return data_read\n","  # Index the commas and line breaks\n","  def comma_index(self, open_file, path, column_len):\n","    data_comma_place = [0]\n","    column_pl_len = 0\n","    path_split = [ext for ext in path]\n","    path_ext = \"\".join(path_split[-3:])\n","    for i in range(len(open_file)):\n","      data1_col1 = open_file[i]\n","      if path_ext == \"csv\":\n","        if data1_col1 == \",\":\n","          data_comma_place.append(i)\n","          data_comma_place.append(i+1)\n","      elif path_ext == \"tsv\":\n","        if data1_col1 == \"\\t\":\n","          data_comma_place.append(i)\n","          data_comma_place.append(i+1)\n","      if data1_col1 == \"\\n\":\n","        data_comma_place.append(i)\n","        data_comma_place.append(i+1)\n","        # If you want to use the function to get the column width, set to 1\n","        if column_len == 1:\n","          col_width = len(data_comma_place)\n","          return col_width\n","          break\n","    last_val = data_comma_place[-1] + 2\n","    data_comma_place.append(last_val)\n","    return data_comma_place\n","  # Splitting the csv characters into list of words based on indexed comma position\n","  def csv_value_list(self, data_comma_out, open_file, col_width, col_head):\n","    j = col_head\n","    data_val_list = []\n","    for i in range(len(data_comma_out)):\n","      if j >= len(data_comma_out):\n","        break\n","      comma_strt = data_comma_out[j]\n","      j += 1\n","      comma_end = data_comma_out[j]\n","      #print(comma_strt, \"and \", comma_end)\n","      j += 1\n","      data_val_list.append(open_file[comma_strt:comma_end])\n","      j = j + (col_width * 2)\n","    return data_val_list\n","  # Flipping the columns from high to low for readability\n","  # If the original value was 5, set it to equal 0 (no pain)\n","  # If the original value was 0, set it to equal to 5 (high pain) etc.\n","  # If none of those things are true, append the string (for the column header)\n","  def csv_flipper(self, csv_list, col_width):\n","    csv_flipped = []\n","    for i in csv_list:\n","      if i == str(5):\n","        n = str(1)\n","        csv_flipped.append(n)\n","      elif i == str(4):\n","        n = str(2)\n","        csv_flipped.append(n)\n","      elif i == str(3):\n","        n = str(3)\n","        csv_flipped.append(n)\n","      elif i == str(2):\n","        n = str(4)\n","        csv_flipped.append(n)\n","      elif i == str(1):\n","        n = str(5)\n","        csv_flipped.append(n)\n","      else:\n","        csv_flipped.append(i)\n","    return csv_flipped\n","\n","######################################################\n","# Part B: Get descriptive statistics of each column. #\n","######################################################\n","class Statistics:\n","  # Returns a dictionary with the header and mean\n","  def mu(self, col_list):\n","    total = 0\n","    counter = 0\n","    # Column has to have a header\n","    for i in col_list[1:]:\n","      if i == \"NA\":\n","        continue\n","      total = total + float(i)\n","      counter += 1\n","    mean = total / counter\n","    header_mean = [col_list[0], mean]\n","    return header_mean\n","  # Returns the 2-4 moment of the distribution\n","  # Different than Google Sheets sample vs population\n","  def mnt(self, header, mean, col_list):\n","    col_1 = len(col_list) - 1\n","    stn = 0\n","    skew = 0\n","    kurt = 0\n","    counter = 0\n","    for i in col_list[1:]:\n","      if i == \"NA\":\n","        continue\n","      # secondary todo: doesn't work with decimals\n","      n1 = int(i) - mean\n","      n1_sqr = n1 ** 2\n","      n1_cube = n1 ** 3\n","      n1_quad = n1 ** 4\n","      stn = stn + n1_sqr\n","      skew = skew + n1_cube\n","      kurt = kurt + n1_quad\n","      counter += 1\n","    # Sample variance (n-1)\n","    # Population variance (n)\n","    counter = (counter - 1)\n","    stn_small_sqr = float(stn) / counter\n","    stn_small = stn_small_sqr ** .5\n","    skew_small_sqr = float(skew) / counter\n","    skew_small = skew_small_sqr / (stn_small ** 3)\n","    kurt_small_sqr = float(kurt) / counter\n","    kurt_small = kurt_small_sqr / (stn_small ** 4)\n","    return [header, stn_small, skew_small, kurt_small]\n","  # Covariance and correlation\n","  def covar(self, x_mean, y_mean, col_1_list, col_2_list):\n","    col_len = len(col_1_list) - 1\n","    covar = 0\n","    x1y1_sum = 0\n","    counter = 0\n","    for i in range(col_len):\n","      #if i == col_len-2:\n","      #  break\n","      if col_1_list[i+1] == \"NA\":\n","        continue\n","      if col_2_list[i+1] == \"NA\":\n","        continue\n","      # print(i+3,i)\n","      # print(\"x_mean: \", x_mean[1], \"x_value: \", col_1_list[i+1])\n","      x1 = float(col_1_list[i+1]) - x_mean[1]\n","      y1 = float(col_2_list[i+1]) - y_mean[1]\n","      x1y1 = x1 * y1\n","      x1y1_sum = x1y1_sum + x1y1\n","      counter += 1\n","    covar = x1y1_sum / counter\n","    return covar\n","  def cor(self, covar, col_1_stnd, col_2_stnd):\n","    stnd12 = col_1_stnd * col_2_stnd\n","    cor = covar / stnd12\n","    return cor\n","\n","######################################################\n","# Part C: Data visualization with a timeseries graph #\n","######################################################\n","# 'Graph' class accepts three variables: verticle arranged 'data',\n","# the date column'date_col_num', and the data column 'data_col_num'\n","class Graph:\n","  # Initialize the input variables\n","  def __init__(self, data, date_col_num, data_col_num):\n","    self.data = data\n","    self.date_col_num = date_col_num\n","    self.data_col_num = data_col_num\n","  def hi_lo(self, data_col_num):\n","    # The date and date column to be used\n","    data_col = self.data[data_col_num]\n","    data_col_len = len(data_col)\n","    # High and low of values\n","    hi_lo_count = 1\n","    hi = data_col[1]\n","    lo = data_col[1]\n","    for e in range(len(data_col[1:])):\n","      hi_lo_count += 1\n","      if hi_lo_count == (len(data_col[1:]) + 1):\n","        break\n","      if hi < data_col[hi_lo_count]:\n","        hi = data_col[hi_lo_count]\n","      if lo > data_col[hi_lo_count]:\n","        lo = data_col[hi_lo_count]\n","    return [hi, lo]\n","  def binned(self, hi_lo):\n","    # high value (5 in this case or hi_lo[0])\n","    # The date and date column to be used\n","    # TODO it works but is not resuable for other data ranges\n","    date_col = self.data[self.date_col_num]\n","    data_col = self.data[self.data_col_num]\n","    data_col_len = len(data_col)\n","\n","    fiver = []\n","    fourer = []\n","    threer = []\n","    twoer = []\n","    oner = []\n","    lol_stm_date = []\n","    counter = 1\n","    # Binned with date value\n","    for i in range(data_col_len):\n","      P0_column = self.data[self.data_col_num]\n","      if counter == len(self.data[0]):\n","        break\n","      # secondary todo:\n","      if P0_column[counter] == \"NA\":\n","        counter += 1\n","        continue\n","      if float(P0_column[counter]) == 5:\n","        fiver.append([date_col[counter], P0_column[counter]])\n","      elif float(P0_column[counter]) == 4:\n","        fourer.append([date_col[counter], P0_column[counter]])\n","      elif float(P0_column[counter]) == 3:\n","        threer.append([date_col[counter], P0_column[counter]])\n","      elif float(P0_column[counter]) == 2:\n","        twoer.append([date_col[counter], P0_column[counter]])\n","      elif float(P0_column[counter]) == 1:\n","        oner.append([date_col[counter], P0_column[counter]])\n","      counter += 1\n","    # Combining the binned data into one dictionary\n","    lol_date_stm = [fiver, fourer, threer, twoer, oner]\n","    return lol_date_stm\n","\n","  def time_series(self, date_hi_lo, lol_date_stm):\n","    date_col = self.data[self.date_col_num]\n","    # Base of the month, plus 00 i.e. 500\n","    date_base = int(date_hi_lo[1]) - 1\n","    # The number of spaces is the 'day' (date - month) - 'prev_day_space_int'\n","    # The difference between the values is multiplied by ' ' for each 5,4,3,2,1\n","    # Value with a '+' character marking the position\n","    spacer = []\n","    prev_day_space_int = 0\n","    for i in lol_date_stm:\n","      spacer_mid = []\n","      for ii in i:\n","        # ii is [date, value] in order\n","        # an if else statement\n","        # 7 starts at 22, 31 days\n","        # 8 30 days\n","        # 9 30 days\n","        # 10 31 days\n","        day = int(ii[0]) - date_base\n","        # print(day)\n","        day_count = day - prev_day_space_int\n","        day_space_str = ((day_count-1)*2) * \" \"\n","        spacer_mid.append(day_space_str)\n","        prev_day_space_int = day\n","      prev_day_space_int = 0\n","      spacer.append(spacer_mid)\n","    y_val = [y for y in range(len(spacer),0,-1)]\n","    return [spacer, y_val]\n","  def time_series_print(self,spacer,y_val):\n","    date_col = self.data[self.date_col_num]\n","    for j,k in zip(spacer,y_val):\n","      # y values\n","      print(k,end=\" \")\n","      for l in j:\n","        print(l,end=\"\")\n","        print(\"+\",end=\" \")\n","      print()\n","    # x values\n","    for m in range(len(date_col[0])-1):\n","      print(\"  \",end=\"\")\n","      for n in date_col[1:]:\n","        print(n[m], end=\" \")\n","      print()\n","    return\n","  def time_series_write(self,header,txt_out,spacer,y_val):\n","    # Open the output file location and write data to the txt\n","    date_col = self.data[self.date_col_num]\n","    file_output = open(txt_out, \"w\")\n","    file_output.write(header)\n","    file_output.write(\"\\n\")\n","    file_output.write(\"\\n\")\n","    # y values\n","    for j,k in zip(spacer,y_val):\n","      file_output.write(str(k) + \" \")\n","      for l in j:\n","        file_output.write(str(l))\n","        file_output.write(\"+\" + \" \")\n","      file_output.write(\"\\n\")\n","    # x values\n","    for m in range(len(date_col[0])-1):\n","      file_output.write(\"  \")\n","      for n in date_col[1:]:\n","        file_output.write(str(n[m]) + \" \")\n","      file_output.write(\"\\n\")\n","    file_output.close()\n","    return\n","\n","###############################################\n","# Part D: Data visualization with a RGB graph #\n","###############################################\n","# Matplotlib for color because otherwise you would have\n","# to write hardware code to avoid using Python or C libraries.\n","class Graphs_rgb:\n","  # Initialize the input variables\n","  def __init__(self, data):\n","    self.data = data\n","\n","  # Four utility functions daisy chained to rgb_timeseries_bar()\n","  # Minor todo: unchain them lol\n","  def rgb_timeseries_mean(self,formatted_data_group):\n","    # Input is list (1-4) of lists (95) of each columns values without NA\n","    # i.e. [[dist1],[dist1],[dist1], etc]\n","    date_col_len = len(formatted_data_group[0])\n","    group_mean = []\n","    # Length of the column (95 without \"NA\" as filtered in rgb_date_time)\n","    for i in range(date_col_len):\n","      row_list = []\n","      # Length of columns to be summarized (1-4) 95 row_list values\n","      for j in range(len(formatted_data_group)):\n","        row_list.append(formatted_data_group[j][i])\n","      # Mean at each day for each group\n","      row_count = len(row_list)\n","      row_sum = sum(row_list)\n","      row_mean = row_sum / row_count\n","      group_mean.append(row_mean)\n","    return group_mean\n","\n","  def rgb_date_time(self,csv_groups,date_col):\n","    day_count = len(self.data[1])\n","    k = 0\n","    group_dist = []\n","    for i in csv_groups:\n","      dist0 = []\n","      dist1 = []\n","      dist2 = []\n","      for j in range(day_count):\n","        if j == (day_count-1):\n","          break\n","        if i[j+1] == \"NA\":\n","          continue\n","        else:\n","          # Formatting the date\n","          # year = 2024\n","          date_length = date_col[j+1]\n","          if len(date_length) < 4:\n","            month = date_length[:1]\n","            day = date_length[1:]\n","          else:\n","            month = date_length[:2]\n","            day = date_length[2:]\n","          date_format0 = month + \"/\" + day\n","          dist0.append(date_format0)\n","          dist1.append(int(i[j+1]))\n","          date_format1 = month + \"/\" + day\n","          if int(day) % 5 == 0:\n","            dist2.append(date_format1)\n","          else:\n","            dist2.append(\" \")\n","            continue\n","      group_dist.append([dist0,dist1,dist2])\n","      k += 1\n","    return group_dist\n","\n","  def rgb_P1_style(self,final_title,line):\n","    plt.yticks(range(1,6))\n","    if final_title == 'Stamina':\n","      plt.ylabel(final_title)\n","    else:\n","      plt.title(final_title)\n","      plt.ylabel(\"Pain\")\n","      if line == 1:\n","        plt.legend()\n","    return\n","\n","  def rgb_B1_style(self,final_title,line):\n","    if final_title == 'Calories':\n","      plt.yticks(range(1200,4500,400))\n","      plt.ylabel(\"Intake\")\n","      plt.title(final_title)\n","    elif final_title == 'Alcohol Servings':\n","      plt.yticks(range(0,16))\n","      plt.title(\"Alcohol\")\n","      plt.ylabel(\"Servings\")\n","    elif final_title == 'Exercise':\n","      plt.yticks(range(0,3))\n","      plt.title(final_title)\n","      # plt.ylabel(\"Calories Out\")\n","      plt.text(.1,.5, \"Calories Out \\n2 = 250+ \\n1 = 1-249\",\n","         bbox={'facecolor': 'white', 'alpha': .75, 'pad': 10})\n","    else:\n","      plt.yticks(range(1,6))\n","      if line == 1:\n","        plt.title(\"Nutrients\")\n","      else:\n","        plt.title(final_title)\n","      plt.ylabel(\"Intake\")\n","      plt.legend()\n","    return\n","\n","  # Bar plots for each column\n","  def rgb_timeseries_bar(self,title_full,start_val,P1_B1):\n","    for i in range(start_val,len(title_full)+start_val):\n","      formatted_csv_group = self.rgb_date_time([self.data[i]],self.data[1])\n","      fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(15, 5))\n","      ax.bar(formatted_csv_group[0][0], formatted_csv_group[0][1], width=0.8, align='edge')\n","      final_title = title_full[i-start_val]\n","      # Format the title, yticks, and ylabel\n","      if P1_B1 == 0:\n","        self.rgb_P1_style(final_title,0)\n","      elif P1_B1 == 1:\n","        self.rgb_B1_style(final_title,0)\n","      elif P1_B1 == 2:\n","        pass\n","        # self.rgb_A0_style(final_title,0)\n","      plt.xticks(formatted_csv_group[0][0], labels=formatted_csv_group[0][2])\n","      plt.margins()\n","      plt.grid()\n","      # plt.savefig(final_title + '.jpg')\n","    return\n","\n","  # Merge sort is the fastest for worst case scenario sorting: N log(n)\n","  # Implementation is from W3 and modified for AZ with ascii ord():\n","  # https://www.w3schools.com/dsa/dsa_algo_mergesort.php\n","\n","  # The functions ord_sum, merge, c_replace, filter_stop are daisy chained\n","  # and called in sort_ascii with filter_stop(). The input is the activity column.\n","  # secondary todo: unchain them lol and organize\n","  def ord_sum(self, left_right):\n","    total = 0\n","    # print(dir(left_right))\n","    type_check = str(left_right.__class__)\n","    if \"'str'\" in type_check:\n","      # todo improve number of letters\n","      total = ord(left_right[0])\n","    else:\n","      total = left_right\n","    return total\n","\n","  # Merge calls ord_sum to calculate the ASCII of the string.\n","  # secondary todo: refractor to accept additional columns\n","  def merge(self,left_in,right_in):\n","      result = []\n","      result_activity = []\n","      result_id = []\n","      result_dur = []\n","      i = j = 0\n","      while i < len(left_in[0]) and j < len(right_in[0]):\n","        left = self.ord_sum(left_in[1][i])\n","        right = self.ord_sum(right_in[1][j])\n","        left_activity = left_in[1][i]\n","        right_activity = right_in[1][j]\n","        left_id = left_in[2][i]\n","        right_id = right_in[2][j]\n","        left_dur = left_in[3][i]\n","        right_dur = right_in[3][j]\n","        if left < right:\n","          result.append(left)\n","          result_activity.append(left_activity)\n","          result_id.append(left_id)\n","          result_dur.append(left_dur)\n","          i += 1\n","        else:\n","          result.append(right)\n","          result_activity.append(right_activity)\n","          result_id.append(right_id)\n","          result_dur.append(right_dur)\n","          j += 1\n","\n","      result.extend(left_in[0][i:])\n","      result.extend(right_in[0][j:])\n","      result_activity.extend(left_in[1][i:])\n","      result_activity.extend(right_in[1][j:])\n","      result_id.extend(left_in[2][i:])\n","      result_id.extend(right_in[2][j:])\n","      result_dur.extend(left_in[3][i:])\n","      result_dur.extend(right_in[3][j:])\n","\n","      return [result,result_activity,result_id,result_dur]\n","\n","  def c_replace(self, string, word, sub_word):\n","    # Input string and replace the word with the sub_word. Similar to Python's:\n","    # string.replace(word,subword)\n","    str_replace = \"\"\n","    word_len = len(word)\n","    str_len = len(string)\n","    count = 0\n","    for i in range(str_len-word_len+1):\n","      if string[count:word_len] == word:\n","        str_replace += sub_word\n","        count += len(word)\n","        word_len += len(word)\n","      else:\n","        str_replace += string[count]\n","        count += 1\n","        word_len += 1\n","    return str_replace\n","\n","  def filter_stop(self,column):\n","    # Filters the verb endings using c_replace(). Similar to the previous one liner:\n","    # activity_arr = [filter_word if x == filter_word + 'ing' or x == filter_word + 'ed' else x for x in self.data[6]]\n","    filtered_column = []\n","    for i in column:\n","      if 'Walked' in i:\n","        filtered_column.append(\"Walk\")\n","      elif 'Juggling' in i:\n","        filtered_column.append(\"Juggle\")\n","      elif 'Driving' in i:\n","        filtered_column.append(\"Drive\")\n","      elif i == 'Lifts':\n","        # Could append since this is hard coded but I wanted to test.\n","        verb_less = self.c_replace(i, \"s\", \"\")\n","        filtered_column.append(verb_less)\n","      elif 'ing' in i:\n","        verb_less = self.c_replace(i, \"ing\", \"\")\n","        filtered_column.append(verb_less)\n","      else:\n","        filtered_column.append(i)\n","    return filtered_column\n","\n","  # Calculates duration using end - start.\n","  def rgb_time(self,activity,start,end):\n","    duration = ['Duration']\n","    for i in range(1,len(start)):\n","      # Checks to see if the Activity or Start column is empty.\n","      if len(activity[i]) == 0 or len(start[i]) == 0:\n","        continue\n","      # Estimates sleep at 7 hours.\n","      elif 'Sleep' == activity[i]:\n","        duration.append(str(7*60))\n","      else:\n","        # Gets the hour.\n","        if len(end[i]) > 3:\n","          end_sub = end[i][:2]\n","        else:\n","          end_sub = end[i][0]\n","        if len(start[i]) > 3:\n","          start_sub = start[i][:2]\n","        else:\n","          start_sub = start[i][0]\n","        # Subtracts 40 minutes since there are 60 in an hour not 100.\n","        if start_sub == end_sub:\n","          duration.append(str(int(end[i]) - int(start[i])))\n","        else:\n","          hunid = (int(end_sub) - int(start_sub)) * 40\n","          duration.append(str((int(end[i]) - int(start[i])) - hunid))\n","    return duration\n","\n","  def sort_ascii(self,time_ID,activity,duration):\n","    length = len(time_ID) - 1\n","    step = 1\n","    ord_list = [x for x in range(length+1)]\n","    activity_filter = self.filter_stop(activity)\n","    while step < length:\n","      for i in range(1, length, 2 * step):\n","        # Time vs space trade off: if you want less space calculate the duration\n","        # with another loop before sorting. Otherwise, the End and Start columns\n","        # are included in sorting and space is linear * number of columns (4).\n","        left = [ord_list[i:i + step], activity_filter[i:i + step], time_ID[i:i + step], duration[i:i + step]]\n","        right = [ord_list[i + step:i + 2 * step], activity_filter[i + step:i + 2 * step], time_ID[i + step:i + 2 * step], duration[i + step:i + 2 * step]]\n","        merged = self.merge(left, right)\n","        # Place the merged array back into the original array\n","        for j in range(len(merged[0])):\n","          ord_list[i + j] = merged[0][j]\n","          activity_filter[i + j] = merged[1][j]\n","          time_ID[i + j] = merged[2][j]\n","          duration[i + j] = merged[3][j]\n","      step *= 2  # Double the sub-array length for the next iteration\n","    return [activity_filter,time_ID,duration]\n","\n","  # Returns the time_id and unique activity lists\n","  def rgb_unique_words(self,activity_col):\n","    # A0_length is 0-225\n","    activity_unique = []\n","    # Unique words in Activity\n","    for i in range(len(activity_col)):\n","      if activity_col[i] not in activity_unique:\n","        if len(activity_col[i]) == 0:\n","          continue\n","        else:\n","          activity_unique.append(activity_col[i])\n","    return activity_unique\n","\n","  def rgb_unique_bin(self,activity_unique,sort_ascii):\n","    # todo bin each value with the output from rgb_unique_words\n","\n","    return\n","\n","  # Returns a date list without blanks\n","  def rgb_date_list(self):\n","    # date_literal is 0-30 days\n","    date_literal = []\n","    # Makes a list with only the dates\n","    for i in range(1,len(self.data[2])):\n","      if len(self.data[2][i]) > 0:\n","        date_literal.append(self.data[2][i])\n","    return date_literal\n","\n","  # Multiple lines same graphs.\n","  def rgb_timeseries_line(self,title_full,start_val,groups_num,title_label,P1_B1):\n","    data = self.data\n","    # secondary todo: name instead of number position\n","    j = 1\n","    # Adding multiple lines to a single plot by group with formatting\n","    for i in range(len(groups_num)):\n","      subset0 = groups_num[i:j][0]\n","      if subset0 == groups_num[-1]:\n","        break\n","      subset1 = groups_num[i+1:j+1][0]\n","      csv_groups = data[subset0:subset1]\n","      formatted_csv_group = self.rgb_date_time(csv_groups,self.data[1])\n","      # Format subplot\n","      fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(15, 5))\n","      # Get the formatted_csv_group second list of values in each group\n","      dist1_list = [dist1[1] for dist1 in formatted_csv_group]\n","      # First 3 columns in data are ID, while the title list isn't.\n","      # Subtract each subset by the start_val of the values (excluding date, id, etc)\n","      title_group = title_full[(subset0-start_val):(subset1-start_val)]\n","      # y = each dist1 in formatted_csv_group, x = every date value, x labels = every 5th date value\n","      for k in range(len(dist1_list)):\n","        ax.plot(formatted_csv_group[0][0], dist1_list[k], label=title_group[k], linewidth=4)\n","        # Format the title, yticks, and ylabel\n","        if P1_B1 == 0:\n","          self.rgb_P1_style(title_label[j-1],1)\n","        elif P1_B1 == 1:\n","          self.rgb_B1_style(title_label[j-1],1)\n","      # Chart formatting and save\n","      plt.xticks(formatted_csv_group[0][0], labels=formatted_csv_group[0][2])\n","      plt.grid()\n","      plt.margins()\n","      #plt.savefig(title_label[j-1] + '.jpg')\n","      j += 1\n","    return\n","\n","  # Summarized with mean\n","  def rgb_timeseries_small(self,csv_groups_num,legend_label,ax):\n","    csv = self.data\n","    j = 1\n","    for i in range(len(csv_groups_num)):\n","      subset0 = csv_groups_num[i:j][0]\n","      if subset0 == csv_groups_num[-1]:\n","        break\n","      subset1 = csv_groups_num[i+1:j+1][0]\n","      csv_groups = csv[subset0:subset1]\n","      # Builds an array to skip NA and format the date\n","      # [[[dist0],[1],[2]],[[dist0],[1],[2]], etc]]]\n","      formatted_csv_group = self.rgb_date_time(csv_groups,self.data[1])\n","      # Get the formatted_csv_group second list of values in each group\n","      dist1_list = [dist1[1] for dist1 in formatted_csv_group]\n","      # Summarize each body part's group with mean\n","      dist1_group_mean = self.rgb_timeseries_mean(dist1_list)\n","      # y = group mean, x = every date value, x labels = every 5th date value\n","      # Specified in rgb_date_time function\n","      ax.plot(formatted_csv_group[0][0], dist1_group_mean, label=legend_label[j-1], linewidth=4)\n","      plt.xticks(formatted_csv_group[0][0], labels=formatted_csv_group[0][2])\n","      j += 1\n","    return\n","\n","##############################################################################\n","# Part Z: Run the functions                                                  #\n","##############################################################################\n","\n","# Part A: The path of the CSV to be parsed\n","def CSV_running(path,unflipped_col):\n","  # Create the CSV_Parser class object and open the files\n","  parser = CSV_Parser(path)\n","  read = parser.file_opener()\n","  # Index the comma position from the CSV and split the characters into their values\n","  comma_indexed = parser.comma_index(read, path, 0)\n","  # Get the width of columns of the commas\n","  comma_width = parser.comma_index(read, path, 1)\n","  # Sort the list into verticle columns\n","  # The P0 csv gets flipped, except for the Stm column\n","  # Divide by two - the list of comma places is doubled for the start/end value\n","  col_width = int(((comma_width - 1 ) / 2) - 1)\n","  vert = []\n","  for i in range(0,comma_width-1,2):\n","    value_list = parser.csv_value_list(comma_indexed, read, col_width, i)\n","    if unflipped_col == 0:\n","      vert.append(value_list)\n","    else:\n","      if value_list[0] in unflipped_col:\n","        vert.append(value_list)\n","      else:\n","        flip = parser.csv_flipper(value_list, col_width)\n","        vert.append(flip)\n","  return vert\n","\n","# One month of May, 2024 observations\n","P0_path = \"/content/P0.csv\"\n","B0_path = \"/content/B0.csv\"\n","# A0 is a TSV because there are blank cells\n","A0_path = \"/content/A0.tsv\"\n","P0_unflipped_col = ['ID','Date','Day','Stm']\n","# P0_vert = CSV_running(P0_path,P0_unflipped_col)\n","# B0_vert = CSV_running(B0_path,0)\n","A0_vert = CSV_running(A0_path,0)\n","# Four months of July-October observations\n","# P1.csv contains the pain scale and B1.csv contains the food records\n","# P1_path = \"/content/P1-Observations-PaperFigures.csv\"\n","# B1_path = \"/content/B1.csv\"\n","# A1 is a tsv because of blank cells\n","A1_path = \"/content/A1.tsv\"\n","# List of columns to not be flipepd\n","# P1_unflipped_col = ['ID','Date','Day','Stm','Notes','Notes2']\n","# P1_vert = CSV_running(P1_path,P1_unflipped_col)\n","# B1_vert = CSV_running(B1_path,0)\n","# A1_vert = CSV_running(A1_path,0)\n","\n","# Part B: Get descriptive statistics\n","def stats_def(P1_vert,B1_vert):\n","  stats_class = Statistics()\n","  # The first three columns are skipped because they are ID, Date, and Day\n","  # These two loops calculate the means and moments\n","  P1_means_list = []\n","  P1_stnd_list = []\n","  B1_means_list = []\n","  B1_stnd_list = []\n","  # secondary todo: might make these functions\n","  for l in P1_vert[3:]:\n","    P1_means = stats_class.mu(l)\n","    P1_means_list.append(P1_means)\n","    P1_mnt2_4 = stats_class.mnt(P1_means[0],P1_means[1],l)\n","    P1_stnd_list.append(P1_mnt2_4[1])\n","  for m in B1_vert[2:]:\n","    B1_means = stats_class.mu(m)\n","    B1_means_list.append(B1_means)\n","    B1_mnt2_4 = stats_class.mnt(B1_means[0],B1_means[1],m)\n","    B1_stnd_list.append(B1_mnt2_4[1])\n","  # The nested loops calculates the covariance and correlations between B0 and P0\n","  for n in range(len(P1_vert[3:])):\n","    print(\"x: \", P1_vert[n+3][0])\n","    for o in range(len(B1_vert[2:])):\n","      print(\"    and \", B1_vert[o+2][0])\n","      P1B1_covar = stats_class.covar(P1_means_list[n],B1_means_list[o],P1_vert[n+3],B1_vert[o+2])\n","      P1B1_cor = stats_class.cor(P1B1_covar,P1_stnd_list[n],B1_stnd_list[o])\n","      print(P1B1_cor)\n","    print()\n","\n","# Part C: Data visualization ASCII\n","def P1_ASCII_graph(P1_vert):\n","  title_full = ['Stamina',\n","                'Feet','Ankle','Calves',\n","                'Knees','Quadriceps','Gluteus','Groin',\n","                'Abdominals','Lower Back',\n","                'Latissimus Dorsi','Trapezius','Shoulders',\n","                'Chest','Triceps','Biceps',\n","                'Neck','Head']\n","  graph_count = 3\n","  for p in P1_vert[3:]:\n","    #if graph_count == 4:\n","    #  break\n","    # Initialize graph class\n","    P1_graph = Graph(P1_vert, 1, graph_count)\n","    # ASCII Graphs\n","    # date_col_num = 1 # data_col_num = each successive column\n","    # this would be a loop over columns 3-20, 1st column is the date\n","    # print(p[0])\n","    # print()\n","    P1_hi_lo = P1_graph.hi_lo(graph_count)\n","    date_hi_lo = P1_graph.hi_lo(1)\n","    P1_binned = P1_graph.binned(P1_hi_lo)\n","    P1_time_series = P1_graph.time_series(date_hi_lo,P1_binned)\n","    # P1_graph.time_series_print(P1_time_series[0],P1_time_series[1])\n","    # P1_file_out = \"/content/P1_\" + p[0] + \".txt\"\n","    # P1_time_series_write = P1_graph.time_series_write(p[0],P1_file_out,P1_time_series[0],P1_time_series[1])\n","    # print(\"\\n\")\n","    graph_count += 1\n","\n","def B1_ASCII_graph(B1_vert):\n","  # Did not finish\n","  graph_count = 2\n","  for p in B1_vert[2:]:\n","    #if graph_count == 4:\n","    #  break\n","    # Initialize graph class\n","    B1_graph = Graph(B1_vert, 1, graph_count)\n","    # ASCII Graphs\n","    # date_col_num = 1 # data_col_num = each successive column\n","    # this would be a loop over columns 3-20, 1st column is the date\n","    # print(p[0])\n","    # print()\n","    B1_hi_lo = B1_graph.hi_lo(graph_count)\n","    date_hi_lo = B1_graph.hi_lo(1)\n","    B1_binned = B1_graph.binned(B1_hi_lo)\n","    B1_time_series = B1_graph.time_series(date_hi_lo,B1_binned)\n","    # B0_graph.time_series_print(B0_time_series[0],B0_time_series[1])\n","    # print()\n","    # B0_file_out = \"/content/B0_\" + p[0] + \".txt\"\n","    # B0_time_series_write = B0_graph.time_series_write(p[0],B0_file_out,B0_time_series[0],B0_time_series[1])\n","    # print(\"\\n\")\n","    graph_count += 1\n","\n","# Part D: Data visualization RGB\n","def P1_RGB_graph(P1_vert):\n","\n","  title_full = ['Stamina',\n","                'Feet','Ankle','Calves',\n","                'Knees','Quadriceps','Gluteus','Groin',\n","                'Abdominals','Lower Back',\n","                'Latissimus Dorsi','Trapezius','Shoulders',\n","                'Chest','Triceps','Biceps',\n","                'Neck','Head']\n","  P1_rgb = Graphs_rgb(P1_vert)\n","  P1_B1 = 0\n","  start_val = 3\n","  # Draws the bar charts\n","  P1_rgb_bar = P1_rgb.rgb_timeseries_bar(title_full,start_val,P1_B1)\n","  # RGB Line Graphs by Group\n","  # Uses the position of each body part name in the title_full list\n","  P1_groups_num = [3,4,7,11,13,16,19,21]\n","  P1_title_label = ['Stamina','Lower Legs','Upper Legs','Core','Upper Back','Arms','Head']\n","  # P1_rgb_line = P1_rgb.rgb_timeseries_line(title_full,start_val,P1_groups_num,P1_title_label,P1_B1)\n","\n","  # Line graphs by upper/lower body group means\n","  def small():\n","    csv_groups_list = [[3,4],[4,7,11,13],[13,16,19,21]]\n","    legend_label = [['Stamina'],['Lower Legs','Upper Legs','Core'],['Upper Back','Arms','Head']]\n","    k0 = 0\n","    for csv_groups_num in csv_groups_list:\n","      fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(15, 5))\n","      P1_rgb_line_smallest = P1_rgb.rgb_timeseries_small(csv_groups_num,legend_label[k0],ax)\n","      # Plot formatting\n","      plt.margins()\n","      plt.grid()\n","      plt.yticks(range(1,6))\n","      if sum(csv_groups_num) == sum(csv_groups_list[1]):\n","        ax.legend()\n","        plt.title(\"Lower Body\")\n","        plt.ylabel(\"Pain\")\n","        #plt.savefig(\"Lower Body Pain.jpg\")\n","      elif sum(csv_groups_num) == sum(csv_groups_list[2]):\n","        ax.legend()\n","        plt.title(\"Upper Body\")\n","        plt.ylabel(\"Pain\")\n","        #plt.savefig(\"Upper Body Pain.jpg\")\n","      else:\n","        plt.ylabel(\"Stamina\")\n","        #plt.savefig(\"Stamina.jpg\")\n","      k0 += 1\n","\n","  # Smallest on one graph\n","  def smallest():\n","    # csv_groups_list = [[3,4],[4,12],[13,21]]\n","    # legend_label = [['Stamina'],['Lower Body'], ['Upper Body']]\n","    csv_groups_list = [[4,21]]\n","    legend_label = [['Pain']]\n","    fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(15, 5))\n","    k1 = 0\n","    for csv_groups_num in csv_groups_list:\n","      P1_rgb_line_smallest = P1_rgb.rgb_timeseries_small(csv_groups_num,legend_label[k1],ax)\n","      k1 += 1\n","    # Plot formatting\n","    plt.margins()\n","    plt.grid()\n","    # plt.legend()\n","    plt.yticks(range(1,6))\n","    plt.ylabel(\"Pain\")\n","    plt.savefig('P1_smallerest.jpg')\n","  # small()\n","  # smallest()\n","\n","def B1_RGB_graph(B1_vert):\n","  title_full = ['Calories','Exercise',            # Group 0\n","              'Salt', 'Fat', 'Protein',           # Group 1\n","              'Carbohydrates', 'Alcohol Servings' # Group 3\n","              ]                                   # etc\n","  B1_rgb = Graphs_rgb(B1_vert)\n","  P1_B1 = 1\n","  start_val = 2\n","  # Part D RGB Graphs: B1.csv\n","  B1_rgb_bar = B1_rgb.rgb_timeseries_bar(title_full,start_val,P1_B1)\n","  # RGB Line Graphs by Group for B0.csv\n","  # Uses the position of each title in the title_full list\n","  B1_groups_num = [2,3,4,8,9]\n","  B1_title_label = ['Calories','Exercise','Nutrients','Alcohol Servings']\n","  # Line graph is not appropriate for calories, exercise, and alcohol servings\n","  #B1_rgb_line = B1_rgb.rgb_timeseries_line(title_full,start_val,B1_groups_num,B1_title_label,P1_B1)\n","\n","# A0_vert\n","def A0_RGB_graph(A0_vert):\n","  title_full = ['','',            # Group 0\n","              '', '',           # Group 1\n","              '', '','' # Group 3\n","              ]                                   # etc\n","  A0_rgb = Graphs_rgb(A0_vert)\n","  P1_B1 = 2\n","  start_val = 1\n","  # Part D RGB Graphs: A0.tsv\n","  AO_rgb_duration = A0_rgb.rgb_time(A0_rgb.data[6],A0_rgb.data[4],A0_rgb.data[5])\n","  A0_rgb_sorted = A0_rgb.sort_ascii(A0_rgb.data[1],A0_rgb.data[6],AO_rgb_duration)\n","  #for i in range(len(A0_rgb_sorted[0])):\n","  #  print(A0_rgb_sorted[0][i])\n","\n","  A0_rgb_unique = A0_rgb.rgb_unique_words(A0_rgb_sorted[0])\n","  A0_rgb_bin = A0_rgb.rgb_unique_bin(A0_rgb_unique,A0_rgb_sorted)\n","\n","  #A0_rgb_time = A0_rgb.rgb_time()\n","  #A0_rgb_duration = A0_rgb.rgb_duration(A0_rgb_unique_words,A0_rgb_time)\n","  # A0_rgb.rgb_date_list()\n","  # RGB Line Graphs by Group for B0.csv\n","  # Uses the position of each title in the title_full list\n","  # B1_groups_num = [2,3,4,8,9]\n","  # B1_title_label = ['Calories','Exercise','Nutrients','Alcohol Servings']\n","  # Line graph is not appropriate for calories, exercise, and alcohol servings\n","  # B1_rgb_line = B1_rgb.rgb_timeseries_line(title_full,start_val,B1_groups_num,B1_title_label,P1_B1)\n","\n","# A0_vert\n","def A1_RGB_graph(A0_vert):\n","  title_full = ['','',            # Group 0\n","              '', '',           # Group 1\n","              '', '','' # Group 3\n","              ]                                   # etc\n","  A1_rgb = Graphs_rgb(A1_vert)\n","  P1_B1 = 2\n","  start_val = 1\n","  # Part D RGB Graphs: A0.tsv\n","  filter_word = 'Walk'\n","  A1_rgb_unique_words = A1_rgb.rgb_unique_words(filter_word)\n","  A1_rgb_time = A1_rgb.rgb_time()\n","  A1_rgb_duration = A1_rgb.rgb_duration(A1_rgb_unique_words,A1_rgb_time)\n","  # A1_rgb.rgb_date_list()\n","  # RGB Line Graphs by Group for B0.csv\n","  # Uses the position of each title in the title_full list\n","  # B1_groups_num = [2,3,4,8,9]\n","  # B1_title_label = ['Calories','Exercise','Nutrients','Alcohol Servings']\n","  # Line graph is not appropriate for calories, exercise, and alcohol servings\n","  # B1_rgb_line = B1_rgb.rgb_timeseries_line(title_full,start_val,B1_groups_num,B1_title_label,P1_B1)\n","# P1_RGB_graph(P1_vert)\n","\n","# B1_RGB_graph(B1_vert)\n","A0_RGB_graph(A0_vert)\n","# A1_RGB_graph(A1_vert)"],"metadata":{"id":"IxtfVwAjhyaV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742175368605,"user_tz":300,"elapsed":53,"user":{"displayName":"David Leifer","userId":"06279506333224389759"}},"outputId":"3c66a447-de06-4b13-a63a-5584009d4c9d"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n","[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227]\n"]}]},{"cell_type":"code","source":["['Activty', 'Stretch', 'Dynamic warmup', 'Plyometrics', 'Walk', 'Guitar', 'Dynamic warmup', 'Skateboard basement', 'Lifts', 'Wrench']\n"],"metadata":{"id":"J3V5fDnEs67p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6pQFdEQFs7F9"},"execution_count":null,"outputs":[]}]}
