{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1fHIWbrpk4NMYcse3Xdkb-S8XiS2pxzJm","timestamp":1721260308720},{"file_id":"1STKhtVVaknUZiohwWLdfyoZ3zg6Veimx","timestamp":1721260179755}],"authorship_tag":"ABX9TyMi+c5WIe/d9HJh+QVbLVTM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["def todo(self):\n","  # List of primary issues\n","  # todo    description                                   hours   progress      Note\n","  #\n","  # todo0   A01.csv skate, long, downhill, juggling,      61      Processing    Average length of each activity?\n","  #         running mean duration by category.\n","  #\n","  # todo1   A01.csv category by day of the week or        .25     DNF.          Hours are spread throughout the day.\n","  #         time of day i.e. morning, afternoon, night\n","  #         or blocks of 3.\n","  #\n","  # todo2   A01.csv nltk the 'Explanation' and 'Notes'                          Word frequency might be useful to find specific muscles.\n","  #         sections? Manual descriptions are already                           Topic analysis is included in 'Activity'. Sentiment analysis is\n","  #         included in the write-up.                                           redundant since 'Notes' is informational and not autobiographical.\n","  #\n","  # todo3   Another tutorial chapter on merge sort.\n","  #         Compare with Python's built-in len(),\n","  #         sort(), and replace().\n","  #\n","  # todo4   The graphing part could be included in        .25     DNF.          This is a good project to learn syntax and documentation since it's visual.\n","  #         Chapter 1 with pandas and SciPy.\n","  #\n","  # todo5   A01.csv longboard and running distance.       .5      DNF.          Running occured around 5 times and longboarding was recorded with time.\n","  #\n","  #\n","  # todo6   P0P1B0B1.csv timeseries graphing.             45.5    Completed\n","  #\n","  # todo7   P1.csv manual vs observed prediction\n","  #         accuracy F1 or RMSE.\n","  #\n","  # todo8   B01.csv Pearson-Correlation and day-          2       Completed     Found that there was not correlation between parametric variables.\n","  #         delayed between calories, alcohol, exercise.                        An index similar to ENSO is redundant since there were no consecutive\n","  #                                                                             observations over 4 alcohol or excessive (calorie - calorie burned).\n","  #\n","  # todo9   A01P01B01 moving window spearman correlation                        Would have to sort these for rank, which was completed in todo0.\n","  #         between activity, duration, time of day, pain,\n","  #         nutrients, calories, alcohol.\n","  #\n","  # todo10  tbd data filling and automatic predictions.\n","  #         idk if thats another chapter or avoided.\n","  #\n","  # todo11  Manual weather observations and PRISM data\n","  #         will be in a different GitHub to avoid confusion.\n","  #\n","  # Time spent at a computer programming\n","  # Total estimate  :\n","  # Total actual    :\n","  #\n","  # Purpose\n","  # The goal of writing this is to waste as much time as possible in between\n","  # skateboarding, lifting, or exercise to avoid overtraining while retaining\n","  # logical thought process during long stretches of unemployment. These were\n","  # written on a computer with a 1.5-2 hour battery to restrict excessive\n","  # programming by limiting hardware access.\n","  #\n","  # Abstract\n","  # No library Python with C-like syntax is used for data manipulation and\n","  # graphing whereby arrays are handled without dictionaries. The only\n","  # library used is Matplotlib for RGB graphing and to avoid writing a image or\n","  # video format that would likely spread misinformation. An implementation of\n","  # the merge sort algorithm was used to alphabetize exercise activity for\n","  # binning and graphing frequency by unique type. The built-in Python methods\n","  # for 'replace', 'split', 'len', and 'sort' were manually written for\n","  # learning purposes.\n","\n","  # Start date: 20250125\n","  # End date:\n","\n","  # Below is an exhaustive list of secondary issues.\n","\n","  # List of secondary issues\n","  # todo   description                                                  progress\n","  # todo0  rewrite parser for unicode csv str/int.\n","  # todo1  Stats class avoid NA, NAAN, -9999, etc.\n","  # todo2  refractor RGB_graphs.\n","  # todo3  monthly means on bar graphs.\n","  # todo4  organize merge_sort into another classe.                     Complete\n","  # todo5  modify merge sort to accept entire CSV.                      Class\n","  # todo6  Handle multi word activity descriptions consistently.        Class\n","  # todo7  switch the second capital letter to lower case if exists.    Class\n","  # todo8  unchain the four merge sort functions.                       Class\n","\n","  return\n","\n","import sys\n","import matplotlib.pyplot as plt\n","print(sys.version)\n","# 3.10.12 (main, Mar 22 2024, 16:50:05) [GCC 11.4.0]\n","# 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n","\n","# Interesting method to print callable methods on an object.\n","# print(dir(left_right))\n","# Example is obj.__class__ that prints the data type without using the type(obj) method\n","# type_check = str(left_right.__class__)\n","\n","#####################################################################################\n","# Part A: CSV or TSV parser class to open the file and parse the values into a list #\n","#####################################################################################\n","class CSV_Parser:\n","  # Initialize the input variables\n","  def __init__(self, data_path):\n","    self.data_path = data_path\n","  def file_opener(self):\n","    with open(self.data_path, \"r\") as data_open:\n","      data_read = data_open.read()\n","      return data_read\n","  # Index the commas and line breaks\n","  def comma_index(self, open_file, path, column_len):\n","    data_comma_place = [0]\n","    column_pl_len = 0\n","    path_split = [ext for ext in path]\n","    path_ext = \"\".join(path_split[-3:])\n","    for i in range(len(open_file)):\n","      data1_col1 = open_file[i]\n","      if path_ext == \"csv\":\n","        if data1_col1 == \",\":\n","          data_comma_place.append(i)\n","          data_comma_place.append(i+1)\n","      elif path_ext == \"tsv\":\n","        if data1_col1 == \"\\t\":\n","          data_comma_place.append(i)\n","          data_comma_place.append(i+1)\n","      if data1_col1 == \"\\n\":\n","        data_comma_place.append(i)\n","        data_comma_place.append(i+1)\n","        # If you want to use the function to get the column width, set to 1\n","        if column_len == 1:\n","          col_width = len(data_comma_place)\n","          return col_width\n","          break\n","    last_val = data_comma_place[-1] + 2\n","    data_comma_place.append(last_val)\n","    return data_comma_place\n","  # Splitting the csv characters into list of words based on indexed comma position\n","  def csv_value_list(self, data_comma_out, open_file, col_width, col_head):\n","    j = col_head\n","    data_val_list = []\n","    for i in range(len(data_comma_out)):\n","      if j >= len(data_comma_out):\n","        break\n","      comma_strt = data_comma_out[j]\n","      j += 1\n","      comma_end = data_comma_out[j]\n","      #print(comma_strt, \"and \", comma_end)\n","      j += 1\n","      data_val_list.append(open_file[comma_strt:comma_end])\n","      j = j + (col_width * 2)\n","    return data_val_list\n","  # Flipping the columns from high to low for readability\n","  # If the original value was 5, set it to equal 0 (no pain)\n","  # If the original value was 0, set it to equal to 5 (high pain) etc.\n","  # If none of those things are true, append the string (for the column header)\n","  def csv_flipper(self, csv_list, col_width):\n","    csv_flipped = []\n","    for i in csv_list:\n","      if i == str(5):\n","        n = str(1)\n","        csv_flipped.append(n)\n","      elif i == str(4):\n","        n = str(2)\n","        csv_flipped.append(n)\n","      elif i == str(3):\n","        n = str(3)\n","        csv_flipped.append(n)\n","      elif i == str(2):\n","        n = str(4)\n","        csv_flipped.append(n)\n","      elif i == str(1):\n","        n = str(5)\n","        csv_flipped.append(n)\n","      else:\n","        csv_flipped.append(i)\n","    return csv_flipped\n","\n","######################################################\n","# Part B: Get descriptive statistics of each column. #\n","######################################################\n","class Statistics:\n","  # Returns a dictionary with the header and mean\n","  def mu(self, col_list):\n","    total = 0\n","    counter = 0\n","    # Column has to have a header\n","    for i in col_list[1:]:\n","      if i == \"NA\":\n","        continue\n","      total = total + float(i)\n","      counter += 1\n","    mean = total / counter\n","    header_mean = [col_list[0], mean]\n","    return header_mean\n","  # Returns the 2-4 moment of the distribution\n","  # Different than Google Sheets sample vs population\n","  def mnt(self, header, mean, col_list):\n","    col_1 = len(col_list) - 1\n","    stn = 0\n","    skew = 0\n","    kurt = 0\n","    counter = 0\n","    for i in col_list[1:]:\n","      if i == \"NA\":\n","        continue\n","      # secondary todo: doesn't work with decimals\n","      n1 = int(i) - mean\n","      n1_sqr = n1 ** 2\n","      n1_cube = n1 ** 3\n","      n1_quad = n1 ** 4\n","      stn = stn + n1_sqr\n","      skew = skew + n1_cube\n","      kurt = kurt + n1_quad\n","      counter += 1\n","    # Sample variance (n-1)\n","    # Population variance (n)\n","    counter = (counter - 1)\n","    stn_small_sqr = float(stn) / counter\n","    stn_small = stn_small_sqr ** .5\n","    skew_small_sqr = float(skew) / counter\n","    skew_small = skew_small_sqr / (stn_small ** 3)\n","    kurt_small_sqr = float(kurt) / counter\n","    kurt_small = kurt_small_sqr / (stn_small ** 4)\n","    return [header, stn_small, skew_small, kurt_small]\n","  # Covariance and correlation\n","  def covar(self, x_mean, y_mean, col_1_list, col_2_list):\n","    col_len = len(col_1_list) - 1\n","    covar = 0\n","    x1y1_sum = 0\n","    counter = 0\n","    for i in range(col_len):\n","      #if i == col_len-2:\n","      #  break\n","      if col_1_list[i+1] == \"NA\":\n","        continue\n","      if col_2_list[i+1] == \"NA\":\n","        continue\n","      # print(i+3,i)\n","      # print(\"x_mean: \", x_mean[1], \"x_value: \", col_1_list[i+1])\n","      x1 = float(col_1_list[i+1]) - x_mean[1]\n","      y1 = float(col_2_list[i+1]) - y_mean[1]\n","      x1y1 = x1 * y1\n","      x1y1_sum = x1y1_sum + x1y1\n","      counter += 1\n","    covar = x1y1_sum / counter\n","    return covar\n","  def cor(self, covar, col_1_stnd, col_2_stnd):\n","    stnd12 = col_1_stnd * col_2_stnd\n","    cor = covar / stnd12\n","    return cor\n","\n","######################################################\n","# Part C: Data visualization with a timeseries graph #\n","######################################################\n","# 'Graph' class accepts three variables: verticle arranged 'data',\n","# the date column'date_col_num', and the data column 'data_col_num'\n","class Graph:\n","  # Initialize the input variables\n","  def __init__(self, data, date_col_num, data_col_num):\n","    self.data = data\n","    self.date_col_num = date_col_num\n","    self.data_col_num = data_col_num\n","  def hi_lo(self, data_col_num):\n","    # The date and date column to be used\n","    data_col = self.data[data_col_num]\n","    data_col_len = len(data_col)\n","    # High and low of values\n","    hi_lo_count = 1\n","    hi = data_col[1]\n","    lo = data_col[1]\n","    for e in range(len(data_col[1:])):\n","      hi_lo_count += 1\n","      if hi_lo_count == (len(data_col[1:]) + 1):\n","        break\n","      if hi < data_col[hi_lo_count]:\n","        hi = data_col[hi_lo_count]\n","      if lo > data_col[hi_lo_count]:\n","        lo = data_col[hi_lo_count]\n","    return [hi, lo]\n","  def binned(self, hi_lo):\n","    # high value (5 in this case or hi_lo[0])\n","    # The date and date column to be used\n","    # TODO it works but is not resuable for other data ranges\n","    date_col = self.data[self.date_col_num]\n","    data_col = self.data[self.data_col_num]\n","    data_col_len = len(data_col)\n","\n","    fiver = []\n","    fourer = []\n","    threer = []\n","    twoer = []\n","    oner = []\n","    lol_stm_date = []\n","    counter = 1\n","    # Binned with date value\n","    for i in range(data_col_len):\n","      P0_column = self.data[self.data_col_num]\n","      if counter == len(self.data[0]):\n","        break\n","      # secondary todo:\n","      if P0_column[counter] == \"NA\":\n","        counter += 1\n","        continue\n","      if float(P0_column[counter]) == 5:\n","        fiver.append([date_col[counter], P0_column[counter]])\n","      elif float(P0_column[counter]) == 4:\n","        fourer.append([date_col[counter], P0_column[counter]])\n","      elif float(P0_column[counter]) == 3:\n","        threer.append([date_col[counter], P0_column[counter]])\n","      elif float(P0_column[counter]) == 2:\n","        twoer.append([date_col[counter], P0_column[counter]])\n","      elif float(P0_column[counter]) == 1:\n","        oner.append([date_col[counter], P0_column[counter]])\n","      counter += 1\n","    # Combining the binned data into one dictionary\n","    lol_date_stm = [fiver, fourer, threer, twoer, oner]\n","    return lol_date_stm\n","\n","  def time_series(self, date_hi_lo, lol_date_stm):\n","    date_col = self.data[self.date_col_num]\n","    # Base of the month, plus 00 i.e. 500\n","    date_base = int(date_hi_lo[1]) - 1\n","    # The number of spaces is the 'day' (date - month) - 'prev_day_space_int'\n","    # The difference between the values is multiplied by ' ' for each 5,4,3,2,1\n","    # Value with a '+' character marking the position\n","    spacer = []\n","    prev_day_space_int = 0\n","    for i in lol_date_stm:\n","      spacer_mid = []\n","      for ii in i:\n","        # ii is [date, value] in order\n","        # an if else statement\n","        # 7 starts at 22, 31 days\n","        # 8 30 days\n","        # 9 30 days\n","        # 10 31 days\n","        day = int(ii[0]) - date_base\n","        # print(day)\n","        day_count = day - prev_day_space_int\n","        day_space_str = ((day_count-1)*2) * \" \"\n","        spacer_mid.append(day_space_str)\n","        prev_day_space_int = day\n","      prev_day_space_int = 0\n","      spacer.append(spacer_mid)\n","    y_val = [y for y in range(len(spacer),0,-1)]\n","    return [spacer, y_val]\n","  def time_series_print(self,spacer,y_val):\n","    date_col = self.data[self.date_col_num]\n","    for j,k in zip(spacer,y_val):\n","      # y values\n","      print(k,end=\" \")\n","      for l in j:\n","        print(l,end=\"\")\n","        print(\"+\",end=\" \")\n","      print()\n","    # x values\n","    for m in range(len(date_col[0])-1):\n","      print(\"  \",end=\"\")\n","      for n in date_col[1:]:\n","        print(n[m], end=\" \")\n","      print()\n","    return\n","  def time_series_write(self,header,txt_out,spacer,y_val):\n","    # Open the output file location and write data to the txt\n","    date_col = self.data[self.date_col_num]\n","    file_output = open(txt_out, \"w\")\n","    file_output.write(header)\n","    file_output.write(\"\\n\")\n","    file_output.write(\"\\n\")\n","    # y values\n","    for j,k in zip(spacer,y_val):\n","      file_output.write(str(k) + \" \")\n","      for l in j:\n","        file_output.write(str(l))\n","        file_output.write(\"+\" + \" \")\n","      file_output.write(\"\\n\")\n","    # x values\n","    for m in range(len(date_col[0])-1):\n","      file_output.write(\"  \")\n","      for n in date_col[1:]:\n","        file_output.write(str(n[m]) + \" \")\n","      file_output.write(\"\\n\")\n","    file_output.close()\n","    return\n","\n","###############################################\n","# Part D: Data visualization with a RGB graph #\n","###############################################\n","# Matplotlib for color because otherwise you would have\n","# to write hardware code to avoid using Python or C libraries.\n","class Graphs_rgb:\n","  # Initialize the input variables\n","  def __init__(self, data):\n","    self.data = data\n","\n","  # Four utility functions daisy chained to rgb_timeseries_bar()\n","  # Minor todo: unchain them lol\n","  def rgb_timeseries_mean(self,formatted_data_group):\n","    # Input is list (1-4) of lists (95) of each columns values without NA\n","    # i.e. [[dist1],[dist1],[dist1], etc]\n","    date_col_len = len(formatted_data_group[0])\n","    group_mean = []\n","    # Length of the column (95 without \"NA\" as filtered in rgb_date_time)\n","    for i in range(date_col_len):\n","      row_list = []\n","      # Length of columns to be summarized (1-4) 95 row_list values\n","      for j in range(len(formatted_data_group)):\n","        row_list.append(formatted_data_group[j][i])\n","      # Mean at each day for each group\n","      row_count = len(row_list)\n","      row_sum = sum(row_list)\n","      row_mean = row_sum / row_count\n","      group_mean.append(row_mean)\n","    return group_mean\n","\n","  def rgb_date_time(self,csv_groups,date_col):\n","    day_count = len(self.data[1])\n","    k = 0\n","    group_dist = []\n","    for i in csv_groups:\n","      dist0 = []\n","      dist1 = []\n","      dist2 = []\n","      for j in range(day_count):\n","        if j == (day_count-1):\n","          break\n","        if i[j+1] == \"NA\":\n","          continue\n","        else:\n","          # Formatting the date\n","          # year = 2024\n","          date_length = date_col[j+1]\n","          if len(date_length) < 4:\n","            month = date_length[:1]\n","            day = date_length[1:]\n","          else:\n","            month = date_length[:2]\n","            day = date_length[2:]\n","          date_format0 = month + \"/\" + day\n","          dist0.append(date_format0)\n","          dist1.append(int(i[j+1]))\n","          date_format1 = month + \"/\" + day\n","          if int(day) % 5 == 0:\n","            dist2.append(date_format1)\n","          else:\n","            dist2.append(\" \")\n","            continue\n","      group_dist.append([dist0,dist1,dist2])\n","      k += 1\n","    return group_dist\n","\n","  def rgb_P1_style(self,final_title,line):\n","    plt.yticks(range(1,6))\n","    if final_title == 'Stamina':\n","      plt.ylabel(final_title)\n","    else:\n","      plt.title(final_title)\n","      plt.ylabel(\"Pain\")\n","      if line == 1:\n","        plt.legend()\n","    return\n","\n","  def rgb_B1_style(self,final_title,line):\n","    if final_title == 'Calories':\n","      plt.yticks(range(1200,4500,400))\n","      plt.ylabel(\"Intake\")\n","      plt.title(final_title)\n","    elif final_title == 'Alcohol Servings':\n","      plt.yticks(range(0,16))\n","      plt.title(\"Alcohol\")\n","      plt.ylabel(\"Servings\")\n","    elif final_title == 'Exercise':\n","      plt.yticks(range(0,3))\n","      plt.title(final_title)\n","      # plt.ylabel(\"Calories Out\")\n","      plt.text(.1,.5, \"Calories Out \\n2 = 250+ \\n1 = 1-249\",\n","         bbox={'facecolor': 'white', 'alpha': .75, 'pad': 10})\n","    else:\n","      plt.yticks(range(1,6))\n","      if line == 1:\n","        plt.title(\"Nutrients\")\n","      else:\n","        plt.title(final_title)\n","      plt.ylabel(\"Intake\")\n","      plt.legend()\n","    return\n","\n","  # Bar plots for each column\n","  def rgb_timeseries_bar(self,title_full,start_val,P1_B1):\n","    for i in range(start_val,len(title_full)+start_val):\n","      formatted_csv_group = self.rgb_date_time([self.data[i]],self.data[1])\n","      fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(15, 5))\n","      ax.bar(formatted_csv_group[0][0], formatted_csv_group[0][1], width=0.8, align='edge')\n","      final_title = title_full[i-start_val]\n","      # Format the title, yticks, and ylabel\n","      if P1_B1 == 0:\n","        self.rgb_P1_style(final_title,0)\n","      elif P1_B1 == 1:\n","        self.rgb_B1_style(final_title,0)\n","      elif P1_B1 == 2:\n","        pass\n","        # self.rgb_A0_style(final_title,0)\n","      plt.xticks(formatted_csv_group[0][0], labels=formatted_csv_group[0][2])\n","      plt.margins()\n","      plt.grid()\n","      # plt.savefig(final_title + '.jpg')\n","    return\n","\n","\n","  # Returns a date list without blanks\n","  def rgb_date_list(self):\n","    # date_literal is 0-30 days\n","    date_literal = []\n","    # Makes a list with only the dates\n","    for i in range(1,len(self.data[2])):\n","      if len(self.data[2][i]) > 0:\n","        date_literal.append(self.data[2][i])\n","    return date_literal\n","\n","  # Multiple lines same graphs.\n","  def rgb_timeseries_line(self,title_full,start_val,groups_num,title_label,P1_B1):\n","    data = self.data\n","    # secondary todo: name instead of number position\n","    j = 1\n","    # Adding multiple lines to a single plot by group with formatting\n","    for i in range(len(groups_num)):\n","      subset0 = groups_num[i:j][0]\n","      if subset0 == groups_num[-1]:\n","        break\n","      subset1 = groups_num[i+1:j+1][0]\n","      csv_groups = data[subset0:subset1]\n","      formatted_csv_group = self.rgb_date_time(csv_groups,self.data[1])\n","      # Format subplot\n","      fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(15, 5))\n","      # Get the formatted_csv_group second list of values in each group\n","      dist1_list = [dist1[1] for dist1 in formatted_csv_group]\n","      # First 3 columns in data are ID, while the title list isn't.\n","      # Subtract each subset by the start_val of the values (excluding date, id, etc)\n","      title_group = title_full[(subset0-start_val):(subset1-start_val)]\n","      # y = each dist1 in formatted_csv_group, x = every date value, x labels = every 5th date value\n","      for k in range(len(dist1_list)):\n","        ax.plot(formatted_csv_group[0][0], dist1_list[k], label=title_group[k], linewidth=4)\n","        # Format the title, yticks, and ylabel\n","        if P1_B1 == 0:\n","          self.rgb_P1_style(title_label[j-1],1)\n","        elif P1_B1 == 1:\n","          self.rgb_B1_style(title_label[j-1],1)\n","      # Chart formatting and save\n","      plt.xticks(formatted_csv_group[0][0], labels=formatted_csv_group[0][2])\n","      plt.grid()\n","      plt.margins()\n","      #plt.savefig(title_label[j-1] + '.jpg')\n","      j += 1\n","    return\n","\n","  # Summarized with mean\n","  def rgb_timeseries_small(self,csv_groups_num,legend_label,ax):\n","    csv = self.data\n","    j = 1\n","    for i in range(len(csv_groups_num)):\n","      subset0 = csv_groups_num[i:j][0]\n","      if subset0 == csv_groups_num[-1]:\n","        break\n","      subset1 = csv_groups_num[i+1:j+1][0]\n","      csv_groups = csv[subset0:subset1]\n","      # Builds an array to skip NA and format the date\n","      # [[[dist0],[1],[2]],[[dist0],[1],[2]], etc]]]\n","      formatted_csv_group = self.rgb_date_time(csv_groups,self.data[1])\n","      # Get the formatted_csv_group second list of values in each group\n","      dist1_list = [dist1[1] for dist1 in formatted_csv_group]\n","      # Summarize each body part's group with mean\n","      dist1_group_mean = self.rgb_timeseries_mean(dist1_list)\n","      # y = group mean, x = every date value, x labels = every 5th date value\n","      # Specified in rgb_date_time function\n","      ax.plot(formatted_csv_group[0][0], dist1_group_mean, label=legend_label[j-1], linewidth=4)\n","      plt.xticks(formatted_csv_group[0][0], labels=formatted_csv_group[0][2])\n","      j += 1\n","    return\n","\n","  # A0.csv frequency of merged 'Activity'.\n","  def rgb_timeseries_frequency(self):\n","    x = self.data[0]\n","    y = self.data[2]\n","    fig, ax = plt.subplots(figsize=(15, 17))\n","    ax.bar(x, y, linewidth=2)\n","    plt.title('Number of Activities, June-October, 2024')\n","    plt.xticks(x, labels=x, rotation=90, ha='right', fontsize=10)\n","    plt.grid()\n","    plt.margins()\n","    # plt.savefig('A1 AZ Merged Activities with Outliers' + '.jpg')\n","    return\n","\n","  # A0.csv duration of merged 'Activity' hours.\n","  def rgb_timeseries_duration(self):\n","    x = self.data[0]\n","    y = self.data[1]\n","    fig, ax = plt.subplots(figsize=(15, 17))\n","    ax.bar(x, y, linewidth=2)\n","    plt.title('Duration of Activities, June-October, 2024')\n","    plt.xticks(x, labels=x, rotation=90, ha='center', fontsize=10)\n","    plt.grid()\n","    plt.margins()\n","    plt.ylabel('Hours')\n","    #plt.text(.1,33.5, \"Sleep was recorded for one detailed week.\",\n","    #      bbox={'facecolor': 'white', 'alpha': .75, 'pad': 10})\n","    # plt.savefig('A1 AZ Merged Activities Duration with Outliers' + '.jpg')\n","    return\n","\n","\n","  ##############################################################################'\n","  # Early iterations of A0 graphs.\n","  # Graph 0) Occurrence of each activity. Includes every Activity.\n","  def graph_0_frequency(self):\n","    '''\n","    # The activities with multiple same day were inconsistently collected.\n","    x = [i[0][0] for i in A0_sort_bin[1:]] # or A0_sort_unique\n","    y = [len(k[1:]) for k in A0_sort_bin[1:]] # number of each activity\n","    fig, ax = plt.subplots(figsize=(12, 15))\n","    ax.bar(x, y, linewidth=2)\n","    plt.title('Number of Activities, May 2024')\n","    plt.xticks(x, labels=x, rotation=90, ha='right', fontsize=10)\n","    plt.grid()\n","    plt.margins()\n","    # plt.xlabel('Activity')\n","    # plt.legend()\n","    # plt.savefig('AZ Activity' + '.jpg')\n","    plt.show()\n","    '''\n","    return\n","  # Graph 1) Duration of each activity. Includes every Activity duration in hours.\n","  def graph_1_duration(self):\n","    '''\n","    # One option is to sort by the number in each bin to arrange by frequency.\n","    # sort_unique_len = []\n","    # for i in range(len(A0_sort_bin)):\n","    #   sort_unique_len.append([A0_sort_bin[i][0][0],len(A0_sort_bin[i][1:])])\n","    # sort_unique_int = sorted(sort_unique_len, key=lambda sort_sub: sort_sub[1],\n","                               # reverse=True\n","\n","    # todo use a list of lists in the sort functions (i.e. [x,y])\n","    # sort_int = merge_sort_int(A0_sort_bin)\n","    # x = [i[0] for i in sort_unique_int[1:]] # or A0_sort_unique\n","    # y = [k[1] for k in sort_unique_int[1:]]\n","\n","    x = [i[0][0] for i in A0_sort_bin[1:]] # or A0_sort_unique\n","    y = []\n","    for j in A0_sort_bin[1:]:\n","      y_labeled = []\n","      for m in j[1:]:\n","        y_labeled.append(int(m[2]))\n","      y.append(round(sum(y_labeled) / 60,4))\n","\n","    # Remove rest and read\n","    y_count = len(y)\n","    y_sum = sum(y)\n","    y_mean = round(y_sum / y_count, 4)\n","    yy = y\n","    count_2 = 0\n","    for xx in yy:\n","      num = round(xx - y_mean, 4)\n","      # Outliers greater than or less than 30\n","      if num > 30 or num < -30:\n","        # print(count_2, xx, '-', y_mean, num)\n","        x.pop(count_2)\n","        y.pop(count_2)\n","      count_2 += 1\n","\n","    fig, ax = plt.subplots(figsize=(15, 12))\n","    ax.bar(x, y, linewidth=2)\n","    plt.title('Duration of Activities, May 2024')\n","    plt.xticks(x, labels=x, rotation=90, ha='center', fontsize=10)\n","    plt.grid()\n","    plt.margins()\n","    plt.ylabel('Hours')\n","    plt.text(.1,33.5, \"Exceeded 50 hours: Rest and Sleep\",\n","          bbox={'facecolor': 'white', 'alpha': .75, 'pad': 10})\n","    # plt.legend()\n","    # plt.savefig('AZ Activity Duration' + '.jpg')\n","    plt.show()\n","    '''\n","    return\n","\n","# Merge sort is the fastest for worst case scenario sorting: N log(n)\n","# Implementation is from W3 and modified for AZ with ascii ord():\n","# https://www.w3schools.com/dsa/dsa_algo_mergesort.php\n","class Graphs_sort:\n","  # Initialize the input variables\n","  def __init__(self, data):\n","    self.data = data\n","\n","  def class_questions(self):\n","    '''\n","    Part 0.\n","    # The secondary todos would make good computer science class questions.\n","\n","    # Secondary todo 6: Modify merge sort to accept entire CSV as one list input.\n","    # The output is the AZ sorted CSV.\n","\n","    # Secondary todo 7: Handle multi word activity descriptions consistently.\n","    # if first word in two word string is the same as the comparison word, use\n","    # the letter of the second word in the two word string and compare with '0'.\n","\n","    # i.e.        'Guitar 2, Rest 1' and 'Guitar'\n","    # and         'Guitar'           and 'Guitar 2, Rest 1'\n","    #\n","    # should be:  'Guitar 2, Rest 1' and 'Guitar'\n","    # and         'Guitar 2, Rest 1' and 'Guitar'\n","\n","    # Secondary todo 7: switch the second capital letter to lower case if exists.\n","    # Capital then lower case ASCII order is probably a remnant of backward\n","    # compatibility issues with  early low bit hobby computers where every word\n","    # was capitalized and it would have been a pain to rewrite the OS since\n","    # there was no market in the 1970's.\n","    # i.e. 'RPI Firmware' and 'Read' since capital 'P' is a lower number\n","    # on the ASCII chart it would be ordered first regardless of the second word's\n","    # lowercase 'e' despite 'e' appearing before 'p' alphabetically. Write a\n","    # method to switch 'P' to lower case 'p' for ASCII number ordering and swapping\n","    # the two strings. Something along the lines of:\n","\n","    if i > 0 and i is upper case:\n","      # upper case 'P' to lower case 'p'\n","      # ord(i) <- the lower case letter\n","\n","    # Secondary todo 8: Un daisy chain the functions for reusability.\n","    # The functions merge, c_replace, filter_stop are daisy chained\n","    # and called in sort_ascii with filter_stop().\n","\n","    ############################################################################\n","\n","    Part 1.\n","    Merge similar activities.\n","\n","    This loop uses even more C-like syntax that replaces elements in a list\n","    without using the built-in append() or pop() function.\n","\n","    Question: rewrite with regular Python. Extra points if it takes one loop\n","    because I'm unsure if it's possible.\n","\n","    It is included as an example for why languages such as C++ and Python exist.\n","    They make programming easier since basic functions don't have to be written\n","    each time and provide stability, although former CS professers would say that\n","    even C++ abstracts at too high of a level. Presumably academic curriculum\n","    teaches C++ as a recruitment method since old computers had limited GUI and\n","    were still very new to people in the 1970's-1990's.\n","\n","    When I began learning programming, I perceived low level languages such as C,\n","    Assembly, and binary as insanely complicated and might as well have been\n","    wizardry. The main question is where do you even begin to start learning\n","    since it takes over a decade to even have reasonable opinions.\n","\n","    Notes:\n","\n","    [0, 10]\n","    [12, 13]\n","    [15, 19]\n","    [20, 22]\n","    [28, 30]\n","    [32, 37]\n","\n","    # dont want these numbers: print(root_pos+1, count_0, extra_numbers)\n","    # do want these numbers: print(root_pos, extra_numbers)\n","\n","    # make a new list with 'count_0' for each first repeat and 'extra_numbers'?\n","    # while true:\n","      # splice 0:9   . skip 'extra_numbers' or '2' places '10' and '11'.\n","      # splice 12:12 . skip 'extra_numbers' or '2' places '13' and '14'.\n","      # splice 15:18 . skip 'extra_numbers' or 'n' places...etc.\n","\n","    #                                                           0 : ((count_0-extra_numbers) + 1)   <- first position\n","    #                                  (count0+1(extra_numbers)))                                   <- is 'extra_numbers' at 'count_0 + 1' or 'count_1'\n","    # (((count_0-extra_numbers) + 1) + extra_numbers) : ((count_0-extra_numbers) + 1)   <- is the '1' or 'True' value found in the previous while '(((count_0-extra_numbers) + 1)'\n","    # (((count_0-extra_numbers) + 1) + (count0+1(extra_numbers))) :                                 <- final pos is blank (the remaining) if count_1 > len(spliced)\n","\n","    print(x[0:10])    # skip = 0      (count_0-extra_numbers) = 9    count_0 = 11     extra_numbers = 2\n","      print(x[12:13])   # skip = 2    (count_0-extra_numbers) = 12    count_0 = 14    extra_numbers = 2\n","      print(x[15:19])   # skip = 2    (count_0-extra_numbers) = 18    count_0 = 19    extra_numbers = 1\n","      print(x[20:22])   # skip = 1    (count_0-extra_numbers) = 21    count_0 = 27    extra_numbers = 6\n","      print(x[28:30])   # skip = 6    (count_0-extra_numbers) = 29    count_0 = 31    extra_numbers = 2\n","      print(x[32:37])   # skip = 2    (count_0-extra_numbers) = 36    count_0 = 39    extra_numbers = 3\n","    print(x[40:])     # skip = 3      (count_0-extra_numbers) = 36    count_0 = 39    extra_numbers = NA\n","\n","    ############################################################################\n","    Part 2.\n","\n","    Merge sort the list AZ based on the activity field.\n","    How efficient is the implementation of the Merge Sort algorithm from W3\n","    schools compared with Python's built in function? Are the other basic\n","    functions comparable?\n","    # todo time performance of A0_sort.sort_ascii() vs built in sorted() function.\n","    ord_list = ['ord_list'] + [ord(A0_activity_filter[x][0]) for x in range(1,len(A0_activity_filter))]\n","    # 0) time sort_ascii()\n","    A0_sort_merged = A0_sort.sort_ascii(ord_list,A0_activity_filter,A0_sort.data[1],A0_sort_duration)\n","    # 1) format data into list of list to use with sorted()\n","    A0_sort_built = []\n","    for row in range(len(ord_list)):\n","      A0_sort_built.append([ord_list[row],A0_activity_filter[row],A0_sort.data[1][row],A0_sort_duration[row]])\n","    # 2) Sort by name using built in sorted() and time.\n","    sort_unique_int = sorted(A0_sort_built, key=lambda sort_sub: sort_sub[1],\n","                              # reverse=True\n","                              )\n","    # 3) Results: sort_ascii() = ___ seconds.\n","    #                 sorted() = ___ seconds.\n","    # Secondary todo: time the built in functions compared with the c_python_clones.\n","\n","    ############################################################################\n","\n","    Part Z.\n","    Did not finish lol. Physical and mental exercises not Theology.\n","    Graph zzz: Plot the probability density function (PDF).\n","\n","    Class question. It would be exhaustive and difficult to graph each 'Activity'\n","    category's duration. The duration's are binned to reduce the number of\n","    features and graphed as a histogram bar chart.\n","\n","    - Continuous random variable - non integers (decimals) on a number line.\n","    - \"The probability of a continuous random variable falling within a\n","      specific interval is represented by the area under its probability\n","      density function (PDF).\" 0.\n","      0) https://online.stat.psu.edu/stat414/lesson/14/14.1\n","\n","    The equation using NumPy is from Google's generative AI code examples and\n","    the code used is built off that. Numpy uses Float64 datatype instead of\n","    Python's Float which might account for the slight variations in probability\n","    density function numbers despite the equations being the same. Or it could\n","    be a rounding issue. Or Pi and Euler use different decimal places in NumPy.\n","\n","    # Graph the distribution.\n","    sort_unique_len = len(y) # 53 or 52 without headers\n","    sort_unique_mean = sum(y) / len(y)\n","    # Standard deviation.\n","    stn = 0\n","    counter = float(0)\n","    for ii in y:\n","      n1 = ii - sort_unique_mean\n","      n1_sqr = n1 ** 2\n","      stn = stn + n1_sqr\n","      counter += 1\n","    # Sample variance (n-1)\n","    # Population variance (n)\n","    counter = (counter - 1)\n","    sort_stn_small_sqr = float(stn) / counter\n","    sort_stn_small = sort_stn_small_sqr ** .5\n","    plt.hist(y, bins=7, density=True, cumulative=False, alpha=0.6, color='g')\n","\n","    # c_max and c_min exist but aren't used.\n","    # todo: c_range() function similar to Python's for i in range().\n","\n","    # Generate numbers between the minimum and maximum at nearly equidistant\n","    # intervals. The goal is to not use libraries, it is similar to NumPy's:\n","    # import numpy as np\n","    # z = np.linspace(min(y), max(y), sort_unique_len)\n","    step = (max(y) - min(y)) / (sort_unique_len - 1)\n","    z = [min(y) + i * step for i in range(sort_unique_len)]\n","\n","    # These two numbers are from Google's first result.\n","    pi = 3.14159265358979323846 # Pi to 20 places or np.pi.\n","    euler = 2.71828182845904523536 # Euler to 20 places or np.exp().\n","    pdf = []\n","\n","    # Calculate the PDF, similar to:\n","    # np_pdf = (1 / (sort_stn_small * (2 * pi) **.5)) * np.exp(-0.5 * ((z - sort_unique_mean) / sort_stn_small) ** 2)\n","    for num in z:\n","      euler_x = -0.5 * ((num - sort_unique_mean) / sort_stn_small) ** 2\n","      exp = euler ** euler_x\n","      c_pdf = (1 / (sort_stn_small * (2 * pi) ** .5)) * exp\n","      pdf.append(c_pdf)\n","\n","    # Add labels and title\n","    plt.plot(z, pdf, 'r', linewidth=2)\n","    plt.title('Activity Distribution')\n","    plt.xlabel('Binned Occurrences')\n","    plt.ylabel('Probability Density (PDF)')\n","    # plt.savefig('Activity Frequency with outliers' + '.jpg')\n","    plt.grid()\n","    return\n","\n","    '''\n","    return\n","\n","  # Merge calls ord_sum to calculate the ASCII of the string.\n","  # secondary todo: refractor to accept additional columns\n","  def merge(self,left_in,right_in):\n","      result = []\n","      result_activity = []\n","      result_id = []\n","      result_dur = []\n","      i = j = 0\n","      while i < len(left_in[1]) and j < len(right_in[1]):\n","        left = left_in[0][i]\n","        right = right_in[0][j]\n","        left_activity = left_in[1][i]\n","        right_activity = right_in[1][j]\n","        left_id = left_in[2][i]\n","        right_id = right_in[2][j]\n","        left_dur = left_in[3][i]\n","        right_dur = right_in[3][j]\n","        if left < right: # or (left_activity_replace == right_activity and left < right) ?\n","          result.append(left)\n","          result_activity.append(left_activity)\n","          result_id.append(left_id)\n","          result_dur.append(left_dur)\n","          i += 1\n","        elif left > right: # or left_activity == right_activity ?\n","          result.append(right)\n","          result_activity.append(right_activity)\n","          result_id.append(right_id)\n","          result_dur.append(right_dur)\n","          j += 1\n","        else:\n","          if len(left_activity) > len(right_activity):\n","            length = len(right_activity)\n","          else: # same length?\n","            length = len(left_activity)\n","          # Find where the two words are different at k.\n","          for k in range(1,length):\n","            if left_activity[k] != right_activity[k]:\n","              break\n","          # Handles when the comparison first words are the same but\n","          # one of the comparisons have a space and second word.\n","          left_ord = ord(left_activity[k])\n","          right_ord = ord(right_activity[k])\n","          if left_activity[:length] == right_activity[:length]:\n","            if length < len(right_activity):\n","              if left_activity == right_activity[:length]:\n","                left_ord = -1\n","                # Comparison right_activity[length:] is longer and different.\n","                right_ord = ord(right_activity[length:][0])\n","          if left_activity[:k] == right_activity[:k] and left_ord < right_ord:\n","            result.append(left)\n","            result_activity.append(left_activity)\n","            result_id.append(left_id)\n","            result_dur.append(left_dur)\n","            i += 1\n","          else:\n","            result.append(right)\n","            result_activity.append(right_activity)\n","            result_id.append(right_id)\n","            result_dur.append(right_dur)\n","            j += 1\n","      result.extend(left_in[0][i:])\n","      result.extend(right_in[0][j:])\n","      result_activity.extend(left_in[1][i:])\n","      result_activity.extend(right_in[1][j:])\n","      result_id.extend(left_in[2][i:])\n","      result_id.extend(right_in[2][j:])\n","      result_dur.extend(left_in[3][i:])\n","      result_dur.extend(right_in[3][j:])\n","\n","      return [result,result_activity,result_id,result_dur]\n","\n","  # Filters the verb endings using c_replace(). Similar to the previous one liner:\n","  # activity_arr = [filter_word if x == filter_word + 'ing' or x == filter_word + 'ed' else x for x in self.data[6]]\n","  def filter_stop(self,column):\n","    filtered_column = []\n","    for i in column:\n","      if 'Walked' in i:\n","        filtered_column.append(\"Walk\")\n","      elif 'Juggling' in i:\n","        filtered_column.append(\"Juggle\")\n","      elif 'Driving' in i:\n","        filtered_column.append(\"Drive\")\n","      elif i == 'Lifts':\n","        # Could append since this is hard coded but I wanted to test.\n","        verb_less = i.replace(\"s\", \"\")\n","        # verb_less = self.c_replace(i, \"s\", \"\")\n","        filtered_column.append(verb_less)\n","      elif 'ing' in i:\n","        verb_less = i.replace(\"ing\", \"\")\n","        # verb_less = self.c_replace(i, \"ing\", \"\")\n","        filtered_column.append(verb_less)\n","      else:\n","        filtered_column.append(i)\n","    return filtered_column\n","\n","  # Calculates duration using end - start.\n","  def sort_time(self,activity,start,end):\n","    # There are two extra '40 minutes' because of 60 minutes hours. 100 - 60 = 40\n","    # Multiplying the two extra '40 minutes' makes '80 minutes'\n","    # and are then subtracted from the '200' minute value that was\n","    # calculated using two hours with '100' increments,\n","    # instead of '60' increments characteristic of 60 minute hours.\n","    duration = ['Duration']\n","    for i in range(1,len(start)):\n","\n","      '''\n","      # This method uses nearly identical amount of if/else as the other one\n","      # and also converts minutes to a fraction and then to minutes again\n","      # losing accuracy from circular rounding.\n","      dur = float(end[i]) - float(start[i])\n","      dur_decimal = 0\n","      if dur > 60:\n","        dur_convert = str(float(dur / 60))\n","        for j in range(len(dur_convert)):\n","          if dur_convert[j] == '.':\n","            dur_decimal = dur_convert[j:]\n","      dur_minute = float(dur_decimal) + dur\n","      print(end[i], start[i], '=', dur, activity[i], dur_decimal, i)\n","      # activity,start,end\n","      '''\n","\n","      # Checks to see if the Activity or Start column is empty.\n","      # if len(activity[i]) == 0 or len(start[i]) == 0:\n","      #  continue\n","      # Estimates sleep at 7 hours.\n","      dur = 0\n","      if 'Sleep' == activity[i]:\n","        dur = str(7*60)\n","      else:\n","\n","        # Gets the end hour.\n","        if len(end[i]) == 4:    # handles 1030 4 digits\n","          end_sub = end[i][:2]\n","        elif len(end[i]) == 3:  # handles 0930 3 digits\n","          end_sub = end[i][0]\n","        else:                   # handles 0030 2 digits\n","          end_sub = 0\n","        # Gets the start hour.\n","        if len(start[i]) == 4:    # handles 1030 4 digits\n","          start_sub = start[i][:2]\n","        elif len(start[i]) == 3:  # handles 0930 3 digits\n","          start_sub = start[i][0]\n","        else:                   # handles 0030 2 digits\n","          start_sub = 0\n","\n","        # Subtracts 40 minutes since there are 60 in an hour not 100.\n","        if start_sub == end_sub:\n","          if int(end[i]) == int(start[i]):\n","            dur = str(5)\n","          else:\n","            dur = str(int(end[i]) - int(start[i]))\n","        else:\n","\n","\n","          # Turn over from one day to another.\n","          if int(end[i]) < int(start[i]):\n","            # First day's amount of hours (24 - the start time hour)\n","            first_day = 23 - int(start_sub)\n","            first_day_minutes = 60 - int(start[i][2:]) # the last two digits are the minutes\n","\n","            # Not used in the datasets edge case test\n","            # 2230 - 930\n","            # hour difference is 10 hours\n","            # and 30 (first day) + 30 (second day) minutes is 60 minutes\n","            # 11 hours or 660 minutes\n","            # second_day = end[i][:2] calculated in the previous if else\n","            # end[i] = '1030'\n","            # end_sub = 10\n","            # end[i] = '945'\n","            # end_sub = 9\n","\n","            # Second day's hours added to the first day's hours as 'dur' as minutes.\n","            if len(end[i]) == 4:\n","              second_day_minutes = end[i][2:]\n","            elif len(end[i]) == 3:\n","              second_day_minutes = end[i][1:]\n","            else: # There are no extra hours\n","              second_day_minutes = end[i]\n","\n","            end_sub = end_sub + first_day\n","            hunid = (int(end_sub)) * 60 # hour difference converted to minutes\n","            dur = hunid + first_day_minutes + int(second_day_minutes)\n","            # print(i, end[i], ' - ', start[i], ' ) - ', hunid,  ' = ', dur, activity[i])\n","\n","          else:\n","            hunid = (int(end_sub) - int(start_sub)) * 40\n","            dur = str( ( int(end[i]) - int(start[i]) ) - hunid)\n","      print(i, end[i], ' - ', start[i], ' ) - ', hunid,  ' = ', dur, activity[i])\n","      duration.append(dur)\n","\n","    #for j in duration:\n","    #  print(j)\n","    return duration\n","\n","  # def sort_ascii(self,time_ID,ord_list,activity_filter,duration):\n","  def sort_ascii(self,ord_list,activity_filter,time_ID,duration):\n","    length = len(time_ID) - 1\n","    step = 1\n","    while step < length:\n","      for i in range(1, length, 2 * step):\n","        # Time vs space trade off: if you want less space calculate the duration\n","        # with another loop before sorting. Otherwise, the End and Start columns\n","        # are included in sorting and space is linear * number of columns (4).\n","        left = [ord_list[i:i + step],activity_filter[i:i + step], time_ID[i:i + step], duration[i:i + step]]\n","        right = [ord_list[i + step:i + 2 * step],\n","                 activity_filter[i + step:i + 2 * step], time_ID[i + step:i + 2 * step], duration[i + step:i + 2 * step]]\n","        merged = self.merge(left, right)\n","        # Place the merged array back into the original array\n","        for j in range(len(merged[0])):\n","          ord_list[i + j] = merged[0][j]\n","          activity_filter[i + j] = merged[1][j]\n","          time_ID[i + j] = merged[2][j]\n","          duration[i + j] = merged[3][j]\n","      step *= 2  # Double the sub-array length for the next iteration\n","    return [activity_filter,time_ID,duration]\n","\n","  # Returns the time_id and unique activity lists.\n","  def sort_unique_words(self,activity_col):\n","    # A0_length is 0-225\n","    activity_unique = []\n","    # Unique words in Activity\n","    for i in range(len(activity_col)):\n","      if activity_col[i] not in activity_unique:\n","        if len(activity_col[i]) == 0:\n","          continue\n","        else:\n","          activity_unique.append(activity_col[i])\n","    return activity_unique\n","\n","  # Returns the sorted list into AZ bins.\n","  # Dimensions: 'sort_unique_words' by the number of occurances in 'sort_ascii'.\n","  def sort_unique_bin(self,sort_unique_words,sort_ascii):\n","\n","    # Once the word is different than the next word, bin the next\n","    # word (or words) since the list is already sorted.\n","\n","    # Empty 'unique_bin' is generated with int. Could use '0's but these\n","    # are 0,1,2,...n!\n","    unique_bin = [\n","        [[x],[x]] for x in range(len(sort_unique_words))\n","        ]\n","    count = 0\n","    for i in range(len(sort_ascii[0])):\n","      # Avoids checking 'sort_ascii' past the length of the list.\n","      if i == (len(sort_ascii[0])-1):\n","        break\n","      if sort_ascii[0][i] == sort_ascii[0][i+1]:\n","        # Words are the same, 'count' does not get incremented.\n","        if unique_bin[count][0][0].__class__ == str:\n","          # If first key or 'unique_bin[count]' is str, don't include it.\n","          unique_bin[count].append([sort_ascii[0][i],sort_ascii[1][i],sort_ascii[2][i]])\n","        else:\n","          unique_bin[count] = [\n","              [sort_unique_words[count]],\n","              [sort_ascii[0][i],sort_ascii[1][i],sort_ascii[2][i]]\n","              ]\n","      elif (sort_ascii[0][i-1] != sort_ascii[0][i]) and (sort_ascii[0][i] != sort_ascii[0][i+1]):\n","        # Previous word and next word are different.\n","        unique_bin[count] = [\n","            [sort_unique_words[count]],\n","            [sort_ascii[0][i],sort_ascii[1][i],sort_ascii[2][i]]\n","            ]\n","        count += 1\n","      elif (sort_ascii[0][i-1] == sort_ascii[0][i]) and (sort_ascii[0][i] != sort_ascii[0][i+1]):\n","        # Previous word is the same, next word is different.\n","        unique_bin[count].append([sort_ascii[0][i],sort_ascii[1][i],sort_ascii[2][i]])\n","        count += 1\n","      else:\n","        count += 1\n","    # Append last element of sorted list onto the bin list at end\n","    unique_bin[-1].append([sort_ascii[0][-1],\n","                           sort_ascii[1][-1],\n","                           sort_ascii[2][-1]])\n","\n","    return unique_bin\n","\n","\n","\n","\n","\n","  # todo use one list instead of several lists (also for sort_ascii())\n","  def merge_int(self, left, right):\n","      result = []\n","      result_unique = []\n","      i = j = 0\n","      while i < len(left[0]) and j < len(right[0]):\n","        print(left[0][i], right[0][j])\n","        if left[0][i] < right[0][j]:\n","            result.append(left[0][i])\n","            result_unique.append(left[1][i])\n","            i += 1\n","        else:\n","            result.append(right[0][j])\n","            result_unique.append(right[1][i])\n","            j += 1\n","      result.extend(left[0][i:])\n","      result.extend(right[0][j:])\n","      result.extend(left[1][i:])\n","      result.extend(right[1][j:])\n","      return result\n","  # tod0\n","  def merge_sort_int(self, array_int, array_str):\n","      step = 1  # Starting with sub-arrays of length 1\n","      length = len(array_int[0])\n","      while step < length:\n","        for i in range(0, length, 2 * step):\n","          left = [array_int[i:i + step][0],array_int[i:i + step][1]]\n","          right = [array_int[i + step:i + 2 * step][0], array_int[i + step:i + 2 * step][1]]\n","          merged = self.merge_int(left, right)\n","          # Place the merged array back into the original array\n","          for j in range(len(merged[0])):\n","            array_int[0][i + j] = merged[0][j]\n","            array_int[1][i + j] = merged[1][j]\n","        step *= 2  # Double the sub-array length for the next iteration\n","      return array_int\n","\n","\n","  # todo: use multiple variables\n","  # Merges entries if the first word in the string is the same. Uses C syntax.\n","  # If you're a stickler, replace 'for i in range()' with 'while iterator <= len(data)'\n","  def merge_similar_activities(self, sorted_list):\n","    # Specific formatting for this dataset.\n","    ######################################################################\n","\n","    x = [i[0][0] for i in sorted_list[1:]] # or A0_sort_unique\n","    z = [len(k[1:]) for k in sorted_list[1:]] # frequency of each activity\n","    y = []\n","    # Calculate the hours for duration.\n","    for j in sorted_list[1:]:\n","      y_labeled = []\n","      for m in j[1:]:\n","        y_labeled.append(float(m[2]))\n","      y.append(round(sum(y_labeled) / 60, 4))\n","\n","    ######################################################################\n","\n","    # Combining similar Activities. The dataset uses the same word plus\n","    # a number to denote multiple of the same activities on the same day.\n","    y_mean = round(sum(y) / float(len(y)), 4)\n","    count_0 = 0\n","    count_1 = 1\n","    root_bool = False\n","\n","    # Time complexity is the number of Activities.\n","    #for count_0 in range(len(y)):\n","    x[count_0] = x[count_0]\n","    y[count_0] = y[count_0]\n","    z[count_0] = z[count_0]\n","    spliced = []\n","    while True:\n","      if count_1 == len(y):\n","        x[count_0] = x[count_0]\n","        y[count_0] = y[count_0]\n","        z[count_0] = z[count_0]\n","        break\n","      x[count_1] = x[count_1]\n","      y[count_1] = y[count_1]\n","      z[count_1] = z[count_1]\n","      x_0 = x[count_0].split(' ') # c_split\n","      x_1 = x[count_1].split(' ') # c_split\n","      # Comparison operators to find the first instance of the word.\n","      # i.e. 'Eat' followed by 'Eat 0', 'Eat 1', etc.\n","      if x_0[0] == x_1[0]:\n","        if x_1[1] != 'outside':\n","          if ' ' not in x[count_0]:\n","            #if len(x_1) < 3:\n","            if root_bool == False:\n","              root_pos = count_0\n","              root_bool = True\n","      if root_bool == True:\n","        if y[root_pos] != y[count_0]:\n","          # 'y' value at 'count_0' is cumulative\n","          y[root_pos] += y[count_0]\n","          z[root_pos] += z[count_0]\n","        # The first 'y' value at 'root_pos' is the same as the total.\n","        x[count_0] = x[root_pos]\n","        y[count_0] = y[root_pos]\n","        z[count_0] = z[root_pos]\n","      else:\n","        x[count_0] = x[count_0]\n","        y[count_0] = y[count_0]\n","        z[count_0] = z[count_0]\n","\n","      # print(root_bool, count_0, x[count_0], 'duration: ', y[count_0])\n","\n","      # This checks to see if the word and next word are different or the next\n","      # word is the same and is has three or more words, 'root_bool' is False.\n","      if x_0[0] != x_1[0] or len(x_1) >= 3:\n","        if root_bool == True:\n","          extra_numbers = (count_0+1) - (root_pos+1)\n","          # 'spliced' numbers list gets three values that slice the remaining\n","          # identical words in another loop.\n","          spliced = spliced + [[(count_0-extra_numbers),count_0, extra_numbers]]\n","          # The values at 'root_pos' get set to the current value at 'count_0'.\n","          x[count_0-extra_numbers] = x[count_0]\n","          y[count_0-extra_numbers] = y[count_0]\n","          z[count_0-extra_numbers] = z[count_0]\n","          root_bool = False\n","      count_0 += 1\n","      count_1 += 1\n","    return [x,y,z,spliced]\n","\n","  # Uses 'merge_similar_activities' to splice the data.\n","  def merge_activities_splice(self,merged_activiites):\n","    # Time complexities is the number of repeated words that are being merged.\n","    x = []\n","    y = []\n","    z = []\n","    spler = []\n","    for k in range(len(merged_activiites[3])+1): # or while the length is less than the\n","      # The end splice.\n","      if k == len(merged_activiites[3]):\n","        # spler = spler + [[middle_0,0]] <- checks that the numbers are correct\n","        x = x + merged_activiites[0][middle_0:]\n","        y = y + merged_activiites[1][middle_0:]\n","        z = z + merged_activiites[2][middle_0:]\n","        break\n","      # The first splice.\n","      if x == []:\n","        # spler = [[0,merged_activiites[3][k][0]+1]]\n","        x = merged_activiites[0][0:merged_activiites[3][k][0]+1]\n","        y = merged_activiites[1][0:merged_activiites[3][k][0]+1]\n","        z = merged_activiites[2][0:merged_activiites[3][k][0]+1]\n","        # This part is carried into the next splice.\n","        middle_0 = merged_activiites[3][k][0]+1 + merged_activiites[3][k][2]\n","      else: # The middle splices.\n","        # spler = spler + [[middle_0, merged_activiites[3][k][0]+1]]\n","        x = x + merged_activiites[0][middle_0:merged_activiites[3][k][0]+1]\n","        y = y + merged_activiites[1][middle_0:merged_activiites[3][k][0]+1]\n","        z = z + merged_activiites[2][middle_0:merged_activiites[3][k][0]+1]\n","        # This part is carried into the next splice.\n","        middle_0 = merged_activiites[3][k][0]+1 + merged_activiites[3][k][2]\n","    return [x,y,z]\n","\n","# Functions mimicking Python built in methods written with C-like syntax.\n","class c_python_clones:\n","  # Initialize the input variables\n","  # def __init__(self, data):\n","  #  self.data = data\n","\n","  # Not used: Returns the length of the input similar to len(element).\n","  def c_len(self, input):\n","    if input.__class__ == str or input.__class__ == list:\n","      total = 0\n","      for i in input:\n","        total += 1\n","    elif input.__class__ == int or input.__class__ == float:\n","      print(\"TypeError: object of type 'int' has no len()\")\n","    return total\n","\n","  # Not used: Returns the split string at subword similar to word.split(sub_word).\n","  def c_split(self,word,sub_word):\n","    j = len(sub_word) # sub_word_len\n","    word_len = len(word)\n","    start = 0\n","    split_return = []\n","    # Testing each 'word' with the length of the 'sub_word'.\n","    for i in range(word_len):\n","      sub_word_test = word[i:j]\n","      # If they match, append the sub string of the 'word'.\n","      if sub_word_test == sub_word:\n","        split_return.append(word[start:(j - len(sub_word))])\n","        start = i + len(sub_word)\n","      if j == (word_len):\n","        j = word_len\n","      else:\n","        j += 1\n","    # Append the remaining string after the final 'sub_word'.\n","    split_return.append(word[start:])\n","    return split_return\n","\n","  # Not used: Input string and replace the word with the sub_word.\n","  # Similar to Python's string.replace(word,subword)\n","  def c_replace(self,string,word,sub_word):\n","    str_replace = \"\"\n","    word_len = len(word)\n","    str_len = len(string)\n","    count = 0\n","    for i in range(str_len-word_len+1):\n","      if string[count:word_len] == word:\n","        str_replace += sub_word\n","        count += len(word)\n","        word_len += len(word)\n","      else:\n","        str_replace += string[count]\n","        count += 1\n","        word_len += 1\n","    return str_replace\n","\n","  # For stability they are not used.\n","  def c_min(self,array):\n","    smallest = array[0]\n","    for i in array:\n","      if i < smallest:\n","        smallest = i\n","    return smallest\n","\n","  def c_max(self,array):\n","    largest = array[0]\n","    for i in array:\n","      if i > largest:\n","        largest = i\n","    return largest\n","  def c_sum(self,array):\n","    total = 0\n","    for i in array:\n","      total += i\n","    return total\n","\n","  # todo\n","  def c_round(num):\n","\n","    return\n","\n","  # todo\n","  def c_append(item):\n","\n","    return\n","\n","  # todo\n","  def c_pop(list):\n","\n","    return\n","\n","  # todo\n","  def c_range(list):\n","\n","    return\n","\n","##############################################################################\n","# Part Z: Run the functions                                                  #\n","##############################################################################\n","\n","# Part A: The path of the CSV to be parsed\n","def CSV_running(path,unflipped_col):\n","  # Create the CSV_Parser class object and open the files\n","  parser = CSV_Parser(path)\n","  read = parser.file_opener()\n","  # Index the comma position from the CSV and split the characters into their values\n","  comma_indexed = parser.comma_index(read, path, 0)\n","  # Get the width of columns of the commas\n","  comma_width = parser.comma_index(read, path, 1)\n","  # Sort the list into verticle columns\n","  # The P0 csv gets flipped, except for the Stm column\n","  # Divide by two - the list of comma places is doubled for the start/end value\n","  col_width = int(((comma_width - 1 ) / 2) - 1)\n","  vert = []\n","  for i in range(0,comma_width-1,2):\n","    value_list = parser.csv_value_list(comma_indexed, read, col_width, i)\n","    if unflipped_col == 0:\n","      vert.append(value_list)\n","    else:\n","      if value_list[0] in unflipped_col:\n","        vert.append(value_list)\n","      else:\n","        flip = parser.csv_flipper(value_list, col_width)\n","        vert.append(flip)\n","  return vert\n","\n","# One month of May, 2024 observations\n","P0_path = \"/content/P0.csv\"\n","B0_path = \"/content/B0.csv\"\n","# A0 is a TSV because there are blank cells\n","A0_path = \"/content/A0.tsv\"\n","P0_unflipped_col = ['ID','Date','Day','Stm']\n","# P0_vert = CSV_running(P0_path,P0_unflipped_col)\n","# B0_vert = CSV_running(B0_path,0)\n","# A0_vert = CSV_running(A0_path,0)\n","# Four months of July-October observations\n","# P1.csv contains the pain scale and B1.csv contains the food records\n","# P1_path = \"/content/P1-Observations-PaperFigures.csv\"\n","# B1_path = \"/content/B1.csv\"\n","# A1 is a tsv because of blank cells\n","A1_path = \"/content/A1.tsv\"\n","# List of columns to not be flipepd\n","# P1_unflipped_col = ['ID','Date','Day','Stm','Notes','Notes2']\n","# P1_vert = CSV_running(P1_path,P1_unflipped_col)\n","# B1_vert = CSV_running(B1_path,0)\n","A1_vert = CSV_running(A1_path,0)\n","\n","# Part B: Get descriptive statistics\n","def stats_def(P1_vert,B1_vert):\n","  stats_class = Statistics()\n","  # The first three columns are skipped because they are ID, Date, and Day\n","  # These two loops calculate the means and moments\n","  P1_means_list = []\n","  P1_stnd_list = []\n","  B1_means_list = []\n","  B1_stnd_list = []\n","  # secondary todo: might make these functions\n","  for l in P1_vert[3:]:\n","    P1_means = stats_class.mu(l)\n","    P1_means_list.append(P1_means)\n","    P1_mnt2_4 = stats_class.mnt(P1_means[0],P1_means[1],l)\n","    P1_stnd_list.append(P1_mnt2_4[1])\n","  for m in B1_vert[2:]:\n","    B1_means = stats_class.mu(m)\n","    B1_means_list.append(B1_means)\n","    B1_mnt2_4 = stats_class.mnt(B1_means[0],B1_means[1],m)\n","    B1_stnd_list.append(B1_mnt2_4[1])\n","  # The nested loops calculates the covariance and correlations between B0 and P0\n","  for n in range(len(P1_vert[3:])):\n","    print(\"x: \", P1_vert[n+3][0])\n","    for o in range(len(B1_vert[2:])):\n","      print(\"    and \", B1_vert[o+2][0])\n","      P1B1_covar = stats_class.covar(P1_means_list[n],B1_means_list[o],P1_vert[n+3],B1_vert[o+2])\n","      P1B1_cor = stats_class.cor(P1B1_covar,P1_stnd_list[n],B1_stnd_list[o])\n","      print(P1B1_cor)\n","    print()\n","\n","# Part C: Data visualization ASCII\n","def P1_ASCII_graph(P1_vert):\n","  title_full = ['Stamina',\n","                'Feet','Ankle','Calves',\n","                'Knees','Quadriceps','Gluteus','Groin',\n","                'Abdominals','Lower Back',\n","                'Latissimus Dorsi','Trapezius','Shoulders',\n","                'Chest','Triceps','Biceps',\n","                'Neck','Head']\n","  graph_count = 3\n","  for p in P1_vert[3:]:\n","    #if graph_count == 4:\n","    #  break\n","    # Initialize graph class\n","    P1_graph = Graph(P1_vert, 1, graph_count)\n","    # ASCII Graphs\n","    # date_col_num = 1 # data_col_num = each successive column\n","    # this would be a loop over columns 3-20, 1st column is the date\n","    # print(p[0])\n","    # print()\n","    P1_hi_lo = P1_graph.hi_lo(graph_count)\n","    date_hi_lo = P1_graph.hi_lo(1)\n","    P1_binned = P1_graph.binned(P1_hi_lo)\n","    P1_time_series = P1_graph.time_series(date_hi_lo,P1_binned)\n","    # P1_graph.time_series_print(P1_time_series[0],P1_time_series[1])\n","    # P1_file_out = \"/content/P1_\" + p[0] + \".txt\"\n","    # P1_time_series_write = P1_graph.time_series_write(p[0],P1_file_out,P1_time_series[0],P1_time_series[1])\n","    # print(\"\\n\")\n","    graph_count += 1\n","\n","def B1_ASCII_graph(B1_vert):\n","  # Did not finish\n","  graph_count = 2\n","  for p in B1_vert[2:]:\n","    #if graph_count == 4:\n","    #  break\n","    # Initialize graph class\n","    B1_graph = Graph(B1_vert, 1, graph_count)\n","    # ASCII Graphs\n","    # date_col_num = 1 # data_col_num = each successive column\n","    # this would be a loop over columns 3-20, 1st column is the date\n","    # print(p[0])\n","    # print()\n","    B1_hi_lo = B1_graph.hi_lo(graph_count)\n","    date_hi_lo = B1_graph.hi_lo(1)\n","    B1_binned = B1_graph.binned(B1_hi_lo)\n","    B1_time_series = B1_graph.time_series(date_hi_lo,B1_binned)\n","    # B0_graph.time_series_print(B0_time_series[0],B0_time_series[1])\n","    # print()\n","    # B0_file_out = \"/content/B0_\" + p[0] + \".txt\"\n","    # B0_time_series_write = B0_graph.time_series_write(p[0],B0_file_out,B0_time_series[0],B0_time_series[1])\n","    # print(\"\\n\")\n","    graph_count += 1\n","\n","# Part D: Data visualization RGB\n","def P1_RGB_graph(P1_vert):\n","\n","  title_full = ['Stamina',\n","                'Feet','Ankle','Calves',\n","                'Knees','Quadriceps','Gluteus','Groin',\n","                'Abdominals','Lower Back',\n","                'Latissimus Dorsi','Trapezius','Shoulders',\n","                'Chest','Triceps','Biceps',\n","                'Neck','Head']\n","  P1_rgb = Graphs_rgb(P1_vert)\n","  P1_B1 = 0\n","  start_val = 3\n","  # Draws the bar charts\n","  P1_rgb_bar = P1_rgb.rgb_timeseries_bar(title_full,start_val,P1_B1)\n","  # RGB Line Graphs by Group\n","  # Uses the position of each body part name in the title_full list\n","  P1_groups_num = [3,4,7,11,13,16,19,21]\n","  P1_title_label = ['Stamina','Lower Legs','Upper Legs','Core','Upper Back','Arms','Head']\n","  # P1_rgb_line = P1_rgb.rgb_timeseries_line(title_full,start_val,P1_groups_num,P1_title_label,P1_B1)\n","\n","  # Line graphs by upper/lower body group means\n","  def small():\n","    csv_groups_list = [[3,4],[4,7,11,13],[13,16,19,21]]\n","    legend_label = [['Stamina'],['Lower Legs','Upper Legs','Core'],['Upper Back','Arms','Head']]\n","    k0 = 0\n","    for csv_groups_num in csv_groups_list:\n","      fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(15, 5))\n","      P1_rgb_line_smallest = P1_rgb.rgb_timeseries_small(csv_groups_num,legend_label[k0],ax)\n","      # Plot formatting\n","      plt.margins()\n","      plt.grid()\n","      plt.yticks(range(1,6))\n","      if sum(csv_groups_num) == sum(csv_groups_list[1]):\n","        ax.legend()\n","        plt.title(\"Lower Body\")\n","        plt.ylabel(\"Pain\")\n","        #plt.savefig(\"Lower Body Pain.jpg\")\n","      elif sum(csv_groups_num) == sum(csv_groups_list[2]):\n","        ax.legend()\n","        plt.title(\"Upper Body\")\n","        plt.ylabel(\"Pain\")\n","        #plt.savefig(\"Upper Body Pain.jpg\")\n","      else:\n","        plt.ylabel(\"Stamina\")\n","        #plt.savefig(\"Stamina.jpg\")\n","      k0 += 1\n","\n","  # Smallest on one graph\n","  def smallest():\n","    # csv_groups_list = [[3,4],[4,12],[13,21]]\n","    # legend_label = [['Stamina'],['Lower Body'], ['Upper Body']]\n","    csv_groups_list = [[4,21]]\n","    legend_label = [['Pain']]\n","    fig, ax = plt.subplots(1, 1, layout='constrained', figsize=(15, 5))\n","    k1 = 0\n","    for csv_groups_num in csv_groups_list:\n","      P1_rgb_line_smallest = P1_rgb.rgb_timeseries_small(csv_groups_num,legend_label[k1],ax)\n","      k1 += 1\n","    # Plot formatting\n","    plt.margins()\n","    plt.grid()\n","    # plt.legend()\n","    plt.yticks(range(1,6))\n","    plt.ylabel(\"Pain\")\n","    plt.savefig('P1_smallerest.jpg')\n","  # small()\n","  # smallest()\n","\n","def B1_RGB_graph(B1_vert):\n","  title_full = ['Calories','Exercise',            # Group 0\n","              'Salt', 'Fat', 'Protein',           # Group 1\n","              'Carbohydrates', 'Alcohol Servings' # Group 3\n","              ]                                   # etc\n","  B1_rgb = Graphs_rgb(B1_vert)\n","  P1_B1 = 1\n","  start_val = 2\n","  # Part D RGB Graphs: B1.csv\n","  B1_rgb_bar = B1_rgb.rgb_timeseries_bar(title_full,start_val,P1_B1)\n","  # RGB Line Graphs by Group for B0.csv\n","  # Uses the position of each title in the title_full list\n","  B1_groups_num = [2,3,4,8,9]\n","  B1_title_label = ['Calories','Exercise','Nutrients','Alcohol Servings']\n","  # Line graph is not appropriate for calories, exercise, and alcohol servings\n","  #B1_rgb_line = B1_rgb.rgb_timeseries_line(title_full,start_val,B1_groups_num,B1_title_label,P1_B1)\n","\n","# A0_vert\n","def A0_RGB_graph(A0_vert):\n","  # Part D RGB Graphs: A0.tsv\n","  A0_sort = Graphs_sort(A0_vert)\n","  # Calculates the duration of each activity.\n","  A0_sort_duration = A0_sort.sort_time(A0_sort.data[6],A0_sort.data[4],A0_sort.data[5])\n","  # Removes endings for similar words such as: 'Walk', 'Walks', 'Walked', 'Walking'.\n","  A0_activity_filter = A0_sort.filter_stop(A0_sort.data[6])\n","  # Sorts the list using an implementation of merge sort.\n","  ord_list = ['ord_list'] + [ord(A0_activity_filter[x][0]) for x in range(1,len(A0_activity_filter))]\n","  A0_sort_merged = A0_sort.sort_ascii(ord_list,A0_activity_filter,A0_sort.data[1],A0_sort_duration)\n","  # Finds the unique occurances of each word in 'Activity'.\n","  A0_sort_unique = A0_sort.sort_unique_words(A0_sort_merged[0])\n","  # Bins the sorted list using the unique words.\n","  A0_sort_bin = A0_sort.sort_unique_bin(A0_sort_unique,A0_sort_merged)\n","  # Merges the bins based on if the first word in the string are the same.\n","  # i.e. 'Eat' <- 'Eat 1' <- 'Eat 2' <- 'Eat 3'\n","  A0_sort_similar = A0_sort.merge_similar_activities(A0_sort_bin)\n","  A0_sort_similar_splice = A0_sort.merge_activities_splice(A0_sort_similar)\n","  # Graphs\n","  A0_sort_graph = Graphs_rgb(A0_sort_similar_splice)\n","  A0_sort_graph.rgb_timeseries_frequency()\n","  A0_sort_graph.rgb_timeseries_duration()\n","\n","# A1_vert\n","def A1_RGB_graph(A1_vert):\n","  # Part D RGB Graphs: A0.tsv\n","  A1_sort = Graphs_sort(A1_vert)\n","  # Calculates the duration of each activity.\n","  A1_sort_duration = A1_sort.sort_time(A1_sort.data[6],A1_sort.data[4],A1_sort.data[5])\n","  '''\n","  # Removes endings for similar words such as: 'Walk', 'Walks', 'Walked', 'Walking'.\n","  A1_activity_filter = A1_sort.filter_stop(A1_sort.data[6])\n","  # Sorts the list using an implementation of merge sort.\n","  ord_list = ['ord_list'] + [ord(A1_activity_filter[x][0]) for x in range(1,len(A1_activity_filter))]\n","  A1_sort_merged = A1_sort.sort_ascii(ord_list,A1_activity_filter,A1_sort.data[1],A1_sort_duration)\n","  # Finds the unique occurances of each word in 'Activity'.\n","  A1_sort_unique = A1_sort.sort_unique_words(A1_sort_merged[0])\n","  # Bins the sorted list using the unique words.\n","  A1_sort_bin = A1_sort.sort_unique_bin(A1_sort_unique,A1_sort_merged)\n","  # Merges the bins based on if the first word in the string are the same.\n","  # i.e. 'Eat' <- 'Eat 1' <- 'Eat 2' <- 'Eat 3'\n","  # A1_sort_similar = A1_sort.merge_similar_activities(A1_sort_bin)\n","  # A1_sort_similar_splice = A1_sort.merge_activities_splice(A1_sort_similar)\n","  # Graphs\n","  # A1_sort_graph = Graphs_rgb(A1_sort_similar_splice) # the time might be messed up since there were 40 walk occurences but only 5 hours.\n","  counter = 481\n","  for i in range(len(A1_sort_merged[0][481:533])):\n","    print(counter, A1_sort_merged[0][counter], A1_sort_merged[1][counter], A1_sort_merged[2][counter])\n","    counter += 1\n","  # A1_sort_graph = Graphs_rgb(A1_sort_bin)\n","  # A1_sort_graph.rgb_timeseries_frequency() # (0,2)\n","  # A1_sort_graph.rgb_timeseries_duration()  # (0,1)\n","  '''\n","# P1_RGB_graph(P1_vert)\n","# B1_RGB_graph(B1_vert)\n","# A0_RGB_graph(A0_vert)\n","A1_RGB_graph(A1_vert)\n"],"metadata":{"id":"IxtfVwAjhyaV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744770086519,"user_tz":300,"elapsed":189,"user":{"displayName":"David Leifer","userId":"06279506333224389759"}},"outputId":"ebee72ba-59b9-4d1a-d809-427e24faf159"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0]\n","1 1000  -  800  ) -  80  =  120 Skateboard paper\n","2 1100  -  1000  ) -  40  =  60 Guitar\n","3 1730  -  1700  ) -  40  =  30 Skateboarding basement\n","4 1800  -  1730  ) -  40  =  30 Skateboarding\n","5 1815  -  1800  ) -  40  =  15 Stretched\n","6 1100  -  900  ) -  80  =  120 Skateboard paper\n","7 1130  -  1100  ) -  80  =  30 Longboarding L\n","8 1630  -  1600  ) -  80  =  30 Juggling\n","9 1700  -  1630  ) -  40  =  30 Guitar\n","10 1000  -  800  ) -  80  =  120 Skateboard paper\n","11 1045  -  1000  ) -  80  =  45 Longboarding L\n","12 1630  -  1600  ) -  80  =  30 Juggling\n","13 1700  -  1630  ) -  40  =  30 Guitar\n","14 1100  -  900  ) -  80  =  120 Skateboard paper\n","15 1130  -  1100  ) -  80  =  30 Walk\n","16 1615  -  1600  ) -  80  =  15 Longboarding L\n","17 1700  -  1600  ) -  40  =  60 Skateboard paper\n","18 1830  -  1800  ) -  40  =  30 Guitar\n","19 1100  -  900  ) -  80  =  120 Skateboard paper\n","20 1700  -  1600  ) -  40  =  60 Skateboard paper\n","21 1730  -  1700  ) -  40  =  30 Juggling\n","22 1830  -  1800  ) -  40  =  30 Guitar\n","23 1100  -  900  ) -  80  =  120 Skateboard paper\n","24 1500  -  1400  ) -  40  =  60 Skateboard paper\n","25 1630  -  1600  ) -  40  =  30 Juggling\n","26 1700  -  1630  ) -  40  =  30 Run\n","27 600  -  500  ) -  40  =  60 Run\n","28 1015  -  1000  ) -  40  =  15 Juggling\n","29 1100  -  1015  ) -  40  =  45 Skateboarding basement\n","30 1715  -  1700  ) -  40  =  15 Core\n","31 1915  -  1830  ) -  40  =  45 Stretching\n","32 850  -  810  ) -  40  =  40 Run\n","33 1030  -  1000  ) -  40  =  30 Guitar\n","34 1400  -  1100  ) -  120  =  180 Read\n","35 1430  -  1400  ) -  120  =  30 Juggling\n","36 1630  -  1530  ) -  40  =  60 Skateboarding basement\n","37 1915  -  1900  ) -  40  =  15 Core\n","38 1930  -  1915  ) -  40  =  15 Stretching\n","39 1130  -  930  ) -  80  =  120 Skateboard paper\n","40 1200  -  1145  ) -  40  =  15 Walk\n","41 1230  -  1200  ) -  40  =  30 Lift\n","42 1245  -  1230  ) -  40  =  15 Core\n","43 1300  -  1245  ) -  40  =  15 Stretch\n","44 1500  -  1430  ) -  40  =  30 Skateboard paper\n","45 1515  -  1500  ) -  40  =  15 Juggling\n","46 1915  -  1900  ) -  40  =  15 Guitar\n","47 835  -  800  ) -  40  =  35 Run\n","48 1200  -  1030  ) -  80  =  90 Skateboard paper\n","49 1245  -  1200  ) -  80  =  45 Guitar\n","50 1250  -  1245  ) -  80  =  5 No juggling\n","51 1700  -  1645  ) -  40  =  15 Skateboarding basement\n","52 2015  -  2000  ) -  40  =  15 Stretch\n","53 845  -  800  ) -  40  =  45 Walk\n","54 1300  -  1130  ) -  80  =  90 Read\n","55 1530  -  1330  ) -  80  =  120 Guitar\n","56 1645  -  1545  ) -  40  =  60 Skateboarding basement\n","57 945  -  730  ) -  80  =  135 Skateboard paper\n","58 1300  -  1200  ) -  40  =  60 Guitar\n","59 1600  -  1530  ) -  40  =  30 Walking\n","60 1700  -  1630  ) -  40  =  30 Juggling\n","61 1745  -  1700  ) -  40  =  45 Skateboarding basement\n","62 1800  -  1745  ) -  40  =  15 Core\n","63 1815  -  1800  ) -  40  =  15 Stretch\n","64 530  -  500  ) -  40  =  30 Guitar\n","65 630  -  530  ) -  40  =  60 Read\n","66 930  -  900  ) -  40  =  30 Longboarding L\n","67 1630  -  1600  ) -  40  =  30 Run juggle\n","68 1715  -  1630  ) -  40  =  45 Skateboarding basement\n","69 1725  -  1715  ) -  40  =  10 Lift\n","70 730  -  630  ) -  40  =  60 Skateboard paper\n","71 930  -  730  ) -  80  =  120 Guitar\n","72 1130  -  930  ) -  80  =  120 Skateboard paper\n","73 1900  -  1800  ) -  40  =  60 Walk\n","74 1000  -  900  ) -  40  =  60 Guitar\n","75 1100  -  1000  ) -  40  =  60 Skateboarding\n","76 1245  -  1130  ) -  40  =  75 Cooking\n","77 1445  -  1345  ) -  40  =  60 Reading\n","78 1545  -  1445  ) -  40  =  60 Guitar\n","79 1600  -  1545  ) -  40  =  15 Stretch\n","80 1700  -  1630  ) -  40  =  30 Juggling\n","81 945  -  815  ) -  40  =  90 Skateboard paper\n","82 1015  -  1000  ) -  40  =  15 Guitar\n","83 1215  -  1100  ) -  40  =  75 Skateboarding\n","84 1300  -  1215  ) -  40  =  45 Stretch\n","85 1545  -  1530  ) -  40  =  15 Juggling\n","86 1930  -  1800  ) -  40  =  90 Skateboard crafting\n","87 900  -  845  ) -  40  =  15 Juggling\n","88 1115  -  915  ) -  80  =  120 Skateboard paper\n","89 1435  -  1335  ) -  40  =  60 Skateboarding basement\n","90 1525  -  1435  ) -  40  =  50 Skateboarding outside\n","91 1530  -  1525  ) -  40  =  5 Core\n","92 1535  -  1530  ) -  40  =  5 Stretch\n","93 2000  -  1900  ) -  40  =  60 Guitar\n","94 2300  -  2100  ) -  80  =  120 Read\n","95 1430  -  930  ) -  200  =  300 Break day drink beer\n","96 1630  -  1600  ) -  200  =  30 Juggle\n","97 1800  -  1700  ) -  40  =  60 Guitar\n","98 2330  -  2230  ) -  40  =  60 Walk\n","99 30  -  0  ) -  40  =  30 Guitar\n","100 1030  -  830  ) -  80  =  120 Skateboard paper\n","101 1045  -  1030  ) -  80  =  15 Guitar\n","102 1145  -  1045  ) -  40  =  60 Longboarding\n","103 1600  -  1400  ) -  80  =  120 Skateboard paper\n","104 1630  -  1600  ) -  80  =  30 Guitar\n","105 1930  -  1800  ) -  40  =  90 Skateboarding\n","106 900  -  800  ) -  40  =  60 Longboarding\n","107 1015  -  930  ) -  40  =  45 Skateboard paper\n","108 1130  -  1100  ) -  40  =  30 Guitar\n","109 1330  -  1200  ) -  40  =  90 Skateboard paper\n","110 1530  -  1330  ) -  80  =  120 Skateboard crafting\n","111 2207  -  2200  ) -  80  =  7 Stretch\n","112 1000  -  900  ) -  40  =  60 Skateboard paper\n","113 1130  -  1000  ) -  40  =  90 Skateboarding\n","114 1330  -  1230  ) -  40  =  60 Skateboard paper\n","115 1615  -  1545  ) -  40  =  30 Stretch\n","116 1045  -  830  ) -  80  =  135 Skateboard paper\n","117 1145  -  1100  ) -  80  =  45 Running\n","118 1200  -  1145  ) -  40  =  15 Stretch\n","119 1700  -  1600  ) -  40  =  60 Skateboard paper\n","120 1900  -  1800  ) -  40  =  60 Guitar\n","121 230  -  100  ) -  40  =  90 Skateboard paper\n","122 1000  -  800  ) -  80  =  120 Skateboard paper\n","123 1215  -  1015  ) -  80  =  120 Skateboarding\n","124 1223  -  1215  ) -  80  =  8 Core\n","125 1230  -  1223  ) -  80  =  7 Stretch\n","126 1530  -  1400  ) -  40  =  90 Skateboard paper\n","127 1030  -  845  ) -  80  =  105 Skateboard paper\n","128 1200  -  1045  ) -  80  =  75 Skateboarding\n","129 1215  -  1200  ) -  80  =  15 Core\n","130 1230  -  1215  ) -  80  =  15 Stretch\n","131 1515  -  1400  ) -  40  =  75 Read\n","132 2030  -  1930  ) -  40  =  60 Guitar\n","133 945  -  900  ) -  40  =  45 Stretch\n","134 1030  -  900  ) -  40  =  90 Guitar\n","135 1130  -  1100  ) -  40  =  30 Walk\n","136 1615  -  1600  ) -  40  =  15 Stretch\n","137 1545  -  1515  ) -  40  =  30 Juggle\n","138 2000  -  1900  ) -  40  =  60 Guitar\n","139 2030  -  2000  ) -  40  =  30 Juggle\n","140 2040  -  2030  ) -  40  =  10 Skateboarding basement\n","141 915  -  815  ) -  40  =  60 Longboarding\n","142 930  -  915  ) -  40  =  15 Stretch\n","143 1230  -  1200  ) -  40  =  30 Juggling\n","144 1415  -  1400  ) -  40  =  15 Juggling\n","145 1730  -  1715  ) -  40  =  15 Stretch\n","146 1845  -  1730  ) -  40  =  75 Guitar\n","147 2130  -  2100  ) -  40  =  30 Longboarding\n","148 1000  -  800  ) -  80  =  120 Read\n","149 1130  -  1000  ) -  40  =  90 Longboarding\n","150 1140  -  1130  ) -  40  =  10 Core\n","151 1150  -  1140  ) -  40  =  10 Stretch\n","152 1500  -  1430  ) -  40  =  30 Read\n","153 1730  -  1630  ) -  40  =  60 Juggling\n","154 1800  -  1730  ) -  40  =  30 Longboarding\n","155 1030  -  900  ) -  40  =  90 Skateboard paper\n","156 1215  -  1045  ) -  80  =  90 Juggling\n","157 1345  -  1300  ) -  80  =  45 Guitar\n","158 1530  -  1400  ) -  40  =  90 Skateboard paper\n","159 1615  -  1545  ) -  40  =  30 Longboarding\n","160 1700  -  1615  ) -  40  =  45 Juggling\n","161 50  -  2230  ) -  60  =  140 Walking\n","162 1000  -  900  ) -  40  =  60 Guitar\n","163 1015  -  1000  ) -  40  =  15 Stretching\n","164 1230  -  1115  ) -  40  =  75 Walking\n","165 1645  -  1630  ) -  40  =  15 Juggling\n","166 1730  -  1645  ) -  40  =  45 Longboarding\n","167 1845  -  1830  ) -  40  =  15 Stretching\n","168 1900  -  1845  ) -  40  =  15 Juggling\n","169 2359  -  2300  ) -  40  =  59 Writing\n","170 200  -  0  ) -  80  =  120 Writing\n","171 1030  -  800  ) -  80  =  150 Writing\n","172 1115  -  1100  ) -  80  =  15 Walking\n","173 1145  -  1115  ) -  80  =  30 Juggling\n","174 1300  -  1230  ) -  40  =  30 Writing\n","175 1400  -  1330  ) -  40  =  30 Guitar\n","176 1530  -  1400  ) -  40  =  90 Stretching\n","177 1630  -  1530  ) -  40  =  60 Guitar\n","178 1900  -  1745  ) -  80  =  75 Longboarding\n","179 1910  -  1900  ) -  80  =  10 Stretching\n","180 2115  -  2100  ) -  80  =  15 Stretching\n","181 1045  -  800  ) -  80  =  165 Skateboard paper\n","182 1300  -  1100  ) -  80  =  120 Skateboarding\n","183 1330  -  1330  ) -  80  =  5 Stretching\n","184 1815  -  1800  ) -  80  =  15 Stretching\n","185 2300  -  2200  ) -  40  =  60 Guitar\n","186 1000  -  800  ) -  80  =  120 Skateboard paper\n","187 1100  -  1000  ) -  40  =  60 Guitar\n","188 1145  -  1100  ) -  40  =  45 Skateboard paper\n","189 1300  -  1230  ) -  40  =  30 Juggling\n","190 100  -  15  ) -  40  =  45 Guitar\n","191 1030  -  815  ) -  80  =  135 Skateboard paper\n","192 1330  -  1200  ) -  40  =  90 Skateboarding\n","193 1400  -  1330  ) -  40  =  30 Stretching\n","194 1700  -  1600  ) -  40  =  60 Guitar\n","195 945  -  900  ) -  40  =  45 Skateboard paper\n","196 1030  -  1000  ) -  40  =  30 Guitar\n","197 1200  -  1030  ) -  80  =  90 Skateboard paper\n","198 1300  -  1230  ) -  40  =  30 Walking\n","199 1345  -  1300  ) -  40  =  45 Stretching\n","200 1815  -  1745  ) -  40  =  30 Juggling\n","201 1900  -  1815  ) -  40  =  45 Longboarding\n","202 2330  -  2300  ) -  40  =  30 Guitar\n","203 930  -  730  ) -  80  =  120 Skateboard paper\n","204 1000  -  945  ) -  40  =  15 Juggling\n","205 1100  -  1015  ) -  40  =  45 Skateboarding\n","206 1115  -  1100  ) -  40  =  15 Core\n","207 1130  -  1115  ) -  40  =  15 Stretching\n","208 1400  -  1330  ) -  40  =  30 Guitar\n","209 1915  -  1900  ) -  40  =  15 Guitar\n","210 2045  -  2030  ) -  40  =  15 Stretching\n","211 1030  -  800  ) -  80  =  150 Skateboard paper\n","212 1100  -  1045  ) -  40  =  15 Juggling\n","213 1205  -  1115  ) -  40  =  50 Skateboarding\n","214 1220  -  1205  ) -  40  =  15 Stretching\n","215 1945  -  1915  ) -  40  =  30 Guitar\n","216 2000  -  1945  ) -  40  =  15 Stretching\n","217 2245  -  2200  ) -  40  =  45 Guitar\n","218 530  -  500  ) -  40  =  30 Guitar\n","219 930  -  900  ) -  40  =  30 Guitar\n","220 1015  -  930  ) -  40  =  45 Skateboard paper\n","221 1205  -  1105  ) -  40  =  60 Skateboarding\n","222 1305  -  1300  ) -  40  =  5 Stretching\n","223 1530  -  1515  ) -  40  =  15 Guitar\n","224 1910  -  1800  ) -  40  =  70 Walking\n","225 2030  -  2000  ) -  40  =  30 Stretching\n","226 2300  -  2230  ) -  40  =  30 Guitar\n","227 1000  -  800  ) -  80  =  120 Skateboard paper\n","228 1015  -  1000  ) -  80  =  15 Guitar\n","229 1030  -  1015  ) -  80  =  15 Walking\n","230 1150  -  1050  ) -  40  =  60 Skateboarding\n","231 1240  -  1230  ) -  40  =  10 Stretching\n","232 2015  -  2000  ) -  40  =  15 Stretching\n","233 1000  -  800  ) -  80  =  120 Skateboard paper\n","234 1015  -  1000  ) -  80  =  15 Guitar\n","235 1100  -  1030  ) -  40  =  30 Walking\n","236 1205  -  1200  ) -  40  =  5 Stretching\n","237 1535  -  1445  ) -  40  =  50 Longboarding L\n","238 1715  -  1700  ) -  40  =  15 Stretching\n","239 1030  -  830  ) -  80  =  120 Skateboard paper\n","240 1130  -  1100  ) -  80  =  30 Juggling\n","241 1245  -  1155  ) -  40  =  50 Longboarding L\n","242 1910  -  1900  ) -  40  =  10 Stretching\n","243 1115  -  945  ) -  80  =  90 Skateboard paper\n","244 1145  -  1115  ) -  80  =  30 Guitar\n","245 1215  -  1200  ) -  80  =  15 Stretching\n","246 1545  -  1500  ) -  80  =  45 Walking\n","247 1800  -  1730  ) -  40  =  30 Slowed heart rate\n","248 1845  -  1800  ) -  40  =  45 Examined legs\n","249 100  -  0  ) -  40  =  60 Skateboard paper\n","250 1530  -  1500  ) -  40  =  30 Skateboard paper\n","251 1545  -  1530  ) -  40  =  15 Data backup\n","252 1620  -  1600  ) -  40  =  20 Running\n","253 1030  -  1000  ) -  40  =  30 Skateboard paper\n","254 1245  -  1200  ) -  40  =  45 Walking\n","255 1830  -  1805  ) -  40  =  25 Guitar\n","256 1115  -  1000  ) -  40  =  75 Skateboard paper\n","257 1145  -  1115  ) -  40  =  30 Guitar\n","258 1230  -  1145  ) -  40  =  45 Running\n","259 2100  -  2000  ) -  40  =  60 Skateboard paper\n","260 1000  -  930  ) -  40  =  30 Guitar\n","261 1045  -  1030  ) -  40  =  15 Walking\n","262 1200  -  1130  ) -  40  =  30 Guitar\n","263 1645  -  1600  ) -  40  =  45 Walking\n","264 1415  -  1030  ) -  160  =  225 Driving\n","265 900  -  500  ) -  160  =  240 Driving\n","266 1900  -  1800  ) -  40  =  60 Longboarding L\n","267 2200  -  2030  ) -  80  =  90 Skateboard paper\n","268 1130  -  930  ) -  80  =  120 Skateboard paper\n","269 1900  -  1800  ) -  40  =  60 Running\n","270 1950  -  1945  ) -  40  =  5 Closed AC vent in room\n","271 945  -  900  ) -  40  =  45 Guitar\n","272 1145  -  945  ) -  80  =  120 Skateboard paper\n","273 1255  -  1200  ) -  80  =  55 Skateboarding\n","274 1325  -  1255  ) -  40  =  30 Core\n","275 1530  -  1400  ) -  40  =  90 Nap time\n","276 1545  -  1530  ) -  40  =  15 Rested\n","277 1700  -  1630  ) -  40  =  30 Guitar\n","278 1930  -  1900  ) -  40  =  30 Guitar\n","279 2045  -  2030  ) -  40  =  15 Stretching\n","280 2120  -  2115  ) -  40  =  5 Boiled water\n","281 2150  -  2145  ) -  40  =  5 Boiled water\n","282 1115  -  1100  ) -  40  =  15 Lotion\n","283 1130  -  1130  ) -  40  =  420 Sleep\n","284 130  -  100  ) -  40  =  30 Woke up\n","285 330  -  315  ) -  40  =  15 Woke up\n","286 815  -  815  ) -  40  =  5 Woke up\n","287 820  -  815  ) -  40  =  5 Eat\n","288 845  -  820  ) -  40  =  25 Guitar\n","289 955  -  935  ) -  40  =  20 Walking\n","290 1100  -  1015  ) -  40  =  45 Running\n","291 1130  -  1100  ) -  40  =  30 Stretching\n","292 1145  -  1130  ) -  40  =  15 Shower\n","293 1215  -  1145  ) -  40  =  30 Eat\n","294 1230  -  1215  ) -  40  =  15 Stretching\n","295 1330  -  1230  ) -  40  =  60 Nap time\n","296 1400  -  1330  ) -  40  =  30 Rest\n","297 1430  -  1400  ) -  40  =  30 Stretching\n","298 1815  -  1800  ) -  40  =  15 Eat\n","299 2205  -  2145  ) -  40  =  20 Stretching\n","300 930  -  800  ) -  40  =  90 Skateboard paper\n","301 1000  -  930  ) -  40  =  30 Stretching\n","302 1020  -  1000  ) -  40  =  20 Walking\n","303 1345  -  1315  ) -  40  =  30 Guitar\n","304 1545  -  1500  ) -  40  =  45 Juggling\n","305 1610  -  1545  ) -  40  =  25 Running\n","306 1620  -  1610  ) -  40  =  10 Stretching\n","307 730  -  715  ) -  40  =  15 Guitar\n","308 845  -  730  ) -  40  =  75 Skateboard paper\n","309 915  -  900  ) -  40  =  15 Walking\n","310 1200  -  1145  ) -  40  =  15 Walking\n","311 1500  -  1430  ) -  40  =  30 Walking\n","312 1630  -  1600  ) -  40  =  30 Walking\n","313 1700  -  1635  ) -  40  =  25 Running\n","314 1830  -  1815  ) -  40  =  15 Stretching\n","315 745  -  740  ) -  40  =  5 Guitar\n","316 945  -  745  ) -  80  =  120 Skateboard paper\n","317 1030  -  950  ) -  40  =  40 Skateboarding\n","318 1035  -  1030  ) -  40  =  5 Stretching\n","319 1040  -  1035  ) -  40  =  5 Core\n","320 1050  -  1040  ) -  40  =  10 Stretching\n","321 1330  -  1200  ) -  40  =  90 Skateboard paper\n","322 1600  -  1545  ) -  40  =  15 Walking\n","323 1445  -  1415  ) -  40  =  30 Guitar\n","324 2030  -  1930  ) -  40  =  60 Walking\n","325 2130  -  2100  ) -  40  =  30 Stretching\n","326 800  -  700  ) -  40  =  60 Skateboard paper\n","327 830  -  800  ) -  40  =  30 Guitar\n","328 1300  -  1230  ) -  40  =  30 Stretching\n","329 1445  -  1400  ) -  40  =  45 Walking\n","330 1700  -  1545  ) -  80  =  75 Longboarding\n","331 830  -  815  ) -  80  =  15 Stretching\n","332 900  -  830  ) -  40  =  30 Guitar\n","333 1030  -  930  ) -  40  =  60 Guitar\n","334 1200  -  1045  ) -  80  =  75 Stretching\n","335 1845  -  1800  ) -  80  =  45 Guitar\n","336 845  -  815  ) -  80  =  30 Guitar\n","337 1045  -  1000  ) -  80  =  45 Walking\n","338 1230  -  1200  ) -  80  =  30 Guitar\n","339 1700  -  1645  ) -  40  =  15 Guitar\n","340 2020  -  2000  ) -  40  =  20 Walking\n","341 2120  -  2100  ) -  40  =  20 Walking\n","342 900  -  845  ) -  40  =  15 Guitar\n","343 1220  -  1200  ) -  40  =  20 Longboarding L\n","344 1430  -  1230  ) -  80  =  120 Reading\n","345 1045  -  1000  ) -  80  =  45 Skateboarding\n","346 1215  -  1100  ) -  40  =  75 Running\n","347 1300  -  1215  ) -  40  =  45 Stretching\n","348 2115  -  2030  ) -  40  =  45 Stretching\n","349 945  -  900  ) -  40  =  45 Guitar\n","350 1115  -  945  ) -  80  =  90 Skateboard paper\n","351 1120  -  1115  ) -  80  =  5 Stretching\n","352 1920  -  1900  ) -  80  =  20 Guitar\n","353 330  -  300  ) -  80  =  30 Guitar\n","354 900  -  845  ) -  40  =  15 Guitar\n","355 1000  -  915  ) -  40  =  45 Skateboard cleaning\n","356 1100  -  1015  ) -  40  =  45 Skateboarding\n","357 1215  -  1115  ) -  40  =  60 Running\n","358 1240  -  1215  ) -  40  =  25 Stretching\n","359 1230  -  1100  ) -  40  =  90 Skateboard paper\n","360 1245  -  1230  ) -  40  =  15 Guitar\n","361 1530  -  1400  ) -  40  =  90 Skateboard paper\n","362 2100  -  2015  ) -  40  =  45 Walking\n","363 1615  -  1600  ) -  40  =  15 Guitar\n","364 1745  -  1645  ) -  40  =  60 Longboarding L\n","365 1815  -  1745  ) -  40  =  30 Running\n","366 1930  -  1900  ) -  40  =  30 Reading\n","367 330  -  300  ) -  40  =  30 Guitar\n","368 945  -  930  ) -  40  =  15 Guitar\n","369 1130  -  1030  ) -  40  =  60 Hiking\n","370 1945  -  1800  ) -  40  =  105 Guitar\n","371 830  -  815  ) -  40  =  15 Guitar\n","372 900  -  845  ) -  40  =  15 Walking\n","373 1100  -  1000  ) -  40  =  60 Skateboarding\n","374 1130  -  1100  ) -  40  =  30 Running\n","375 1200  -  1130  ) -  40  =  30 Cool down\n","376 1445  -  1430  ) -  40  =  15 Guitar\n","377 1715  -  1700  ) -  40  =  15 Reading\n","378 1910  -  1900  ) -  40  =  10 Juggling\n","379 1945  -  1930  ) -  40  =  15 Guitar\n","380 2215  -  2100  ) -  40  =  75 Reading\n","381 845  -  830  ) -  40  =  15 Guitar\n","382 1100  -  900  ) -  80  =  120 Reading\n","383 1130  -  1100  ) -  80  =  30 Walking\n","384 1515  -  1500  ) -  80  =  15 Longboarding L\n","385 1745  -  1730  ) -  80  =  15 Juggling\n","386 2130  -  2100  ) -  80  =  30 Walking\n","387 720  -  700  ) -  80  =  20 Longboarding L\n","388 1015  -  1000  ) -  80  =  15 Guitar\n","389 1300  -  1100  ) -  80  =  120 Longboarding L\n","390 1515  -  1500  ) -  80  =  15 Guitar\n","391 1745  -  1730  ) -  80  =  15 Guitar\n","392 1845  -  1815  ) -  80  =  30 Juggling\n","393 1900  -  1845  ) -  40  =  15 Walking\n","394 1030  -  800  ) -  80  =  150 Skateboard Paper\n","395 1045  -  1030  ) -  80  =  15 Guitar\n","396 1300  -  1245  ) -  40  =  15 Guitar\n","397 1415  -  1300  ) -  40  =  75 Skateboard Paper\n","398 1530  -  1445  ) -  40  =  45 Longboarding L\n","399 1615  -  1550  ) -  40  =  25 Running\n","400 1900  -  1830  ) -  40  =  30 Guitar\n","401 1030  -  815  ) -  80  =  135 Skateboard paper\n","402 1045  -  1030  ) -  80  =  15 Guitar\n","403 2000  -  1900  ) -  40  =  60 Skateboard videos\n","404 1145  -  1115  ) -  40  =  30 Guitar\n","405 1100  -  1030  ) -  40  =  30 Skateboard videos\n","406 1115  -  1100  ) -  40  =  15 Guitar\n","407 1915  -  1900  ) -  40  =  15 Guitar\n","408 1945  -  1915  ) -  40  =  30 Reading\n","409 2100  -  1945  ) -  80  =  75 Skateboard videos\n","410 900  -  815  ) -  40  =  45 Skateboard paper\n","411 930  -  900  ) -  40  =  30 Guitar\n","412 1300  -  1230  ) -  40  =  30 Walking\n","413 1315  -  1300  ) -  40  =  15 Core\n","414 1000  -  930  ) -  40  =  30 Guitar\n","415 1100  -  1000  ) -  40  =  60 Longboarding\n","416 1130  -  1100  ) -  40  =  30 Core\n","417 2000  -  1930  ) -  40  =  30 Guitar\n","418 2330  -  2300  ) -  40  =  30 Guitar\n","419 1100  -  930  ) -  80  =  90 Skateboard paper\n","420 1145  -  1100  ) -  80  =  45 Longboarding L\n","421 1515  -  1500  ) -  80  =  15 Guitar\n","422 1145  -  1100  ) -  80  =  45 Longboarding L\n","423 1210  -  1145  ) -  40  =  25 Core\n","424 1615  -  1600  ) -  40  =  15 Guitar\n","425 1045  -  945  ) -  40  =  60 Longboarding L\n","426 1915  -  1900  ) -  40  =  15 Guitar\n","427 1000  -  930  ) -  40  =  30 Guitar\n","428 1115  -  1030  ) -  40  =  45 Longboarding L\n","429 1145  -  1130  ) -  40  =  15 Core\n","430 1830  -  1800  ) -  40  =  30 Plyometrics\n","431 2000  -  1900  ) -  40  =  60 Stretching\n","432 1115  -  1030  ) -  40  =  45 Longboarding L\n","433 1430  -  1400  ) -  40  =  30 Guitar\n","434 1730  -  1700  ) -  40  =  30 Stretching\n","435 945  -  930  ) -  40  =  15 Stretching\n","436 1000  -  945  ) -  40  =  15 Guitar\n","437 1115  -  1015  ) -  40  =  60 Skateboarding\n","438 1215  -  1200  ) -  40  =  15 Stretching\n","439 1500  -  1430  ) -  40  =  30 Walking\n","440 1830  -  1800  ) -  40  =  30 Guitar\n","441 2020  -  2000  ) -  40  =  20 Stretching\n","442 1100  -  1030  ) -  40  =  30 Guitar\n","443 1330  -  1300  ) -  40  =  30 Longboarding L\n","444 1345  -  1330  ) -  40  =  15 Stretching\n","445 1515  -  1430  ) -  40  =  45 Walking\n","446 1745  -  1730  ) -  40  =  15 Stretching\n","447 1915  -  1900  ) -  40  =  15 Guitar\n","448 1030  -  1000  ) -  40  =  30 Guitar\n","449 1215  -  1115  ) -  40  =  60 Skateboarding\n","450 1215  -  1200  ) -  40  =  15 Stretching\n","451 1330  -  1230  ) -  40  =  60 Longboarding L\n","452 1745  -  1715  ) -  40  =  30 Walking\n","453 815  -  800  ) -  40  =  15 Stretching\n","454 1930  -  1900  ) -  40  =  30 Guitar\n","455 930  -  900  ) -  40  =  30 Guitar\n","456 1100  -  1000  ) -  40  =  60 Skateboarding\n","457 1930  -  1900  ) -  40  =  30 Guitar\n","458 2100  -  2030  ) -  40  =  30 Stretching\n","459 1600  -  1515  ) -  40  =  45 Longboarding L\n","460 1615  -  1600  ) -  40  =  15 Core\n","461 1630  -  1615  ) -  40  =  15 Stretching\n","462 945  -  930  ) -  40  =  15 Stretching\n","463 1915  -  1830  ) -  40  =  45 Walking\n","464 845  -  815  ) -  40  =  30 Guitar\n","465 945  -  900  ) -  40  =  45 Longboarding L\n","466 1500  -  1430  ) -  40  =  30 Guitar\n","467 1745  -  1715  ) -  40  =  30 Walking\n","468 1925  -  1900  ) -  40  =  25 Running\n","469 1935  -  1925  ) -  40  =  10 Core\n","470 1945  -  1935  ) -  40  =  10 Stretching\n","471 945  -  915  ) -  40  =  30 Guitar\n","472 1115  -  1015  ) -  40  =  60 Longboarding L\n","473 1615  -  1600  ) -  40  =  15 Guitar\n","474 1300  -  1230  ) -  40  =  30 Guitar\n","475 1320  -  1300  ) -  40  =  20 Stretching\n","476 1715  -  1615  ) -  40  =  60 Longboarding L\n","477 1915  -  1845  ) -  40  =  30 Stretching\n","478 915  -  900  ) -  40  =  15 Guitar\n","479 1000  -  945  ) -  40  =  15 Walking\n","480 1025  -  1015  ) -  40  =  10 Stretching\n","481 1230  -  1115  ) -  40  =  75 Skateboarding\n","482 2000  -  1900  ) -  40  =  60 Walking\n","483 915  -  900  ) -  40  =  15 Stretching\n","484 1000  -  945  ) -  40  =  15 Guitar\n","485 1300  -  1200  ) -  40  =  60 Longboarding L\n","486 1815  -  1800  ) -  40  =  15 Guitar\n","487 1000  -  930  ) -  40  =  30 Guitar\n","488 1515  -  1500  ) -  40  =  15 Stretching\n","489 1700  -  1600  ) -  40  =  60 Skateboard Cleaning\n","490 900  -  830  ) -  40  =  30 Guitar\n","491 1030  -  930  ) -  40  =  60 Skateboarding\n","492 1215  -  1045  ) -  80  =  90 Skateboard Cleaning\n","493 1530  -  1500  ) -  80  =  30 Guitar\n","494 1600  -  1530  ) -  40  =  30 Walking\n","495 2100  -  2030  ) -  40  =  30 Walking\n","496 2330  -  2315  ) -  40  =  15 Stretching\n","497 845  -  815  ) -  40  =  30 Guitar\n","498 1015  -  945  ) -  40  =  30 Walking\n","499 1115  -  1045  ) -  40  =  30 Guitar\n","500 1400  -  1330  ) -  40  =  30 Walking\n","501 1515  -  1500  ) -  40  =  15 Stretching\n","502 1600  -  1545  ) -  40  =  15 Stretching\n","503 1645  -  1630  ) -  40  =  15 Stretching\n","504 850  -  830  ) -  40  =  20 Guitar\n","505 1100  -  1000  ) -  40  =  60 Skateboarding\n","506 1115  -  1100  ) -  40  =  15 Core\n","507 1830  -  1800  ) -  40  =  30 Guitar\n","508 1400  -  1300  ) -  40  =  60 Walking\n","509 1500  -  1400  ) -  40  =  60 Longboard L\n","510 1510  -  1500  ) -  40  =  10 Stretching\n","511 2345  -  2330  ) -  40  =  15 Stretching\n","512 1115  -  1045  ) -  40  =  30 Guitar\n","513 1145  -  1115  ) -  40  =  30 Stretching\n","514 1330  -  1230  ) -  40  =  60 Walking\n","515 1430  -  1330  ) -  40  =  60 Longboard L\n","516 1445  -  1430  ) -  40  =  15 Stretching\n","517 1600  -  1545  ) -  40  =  15 Stretching\n","518 2015  -  2000  ) -  40  =  15 Guitar\n","519 1100  -  1030  ) -  40  =  30 Guitar\n","520 1830  -  1800  ) -  40  =  30 Walking\n","521 1900  -  1830  ) -  40  =  30 Longboard L\n","522 1920  -  1900  ) -  40  =  20 Stretching\n","523 1030  -  1000  ) -  40  =  30 Guitar\n","524 1245  -  1130  ) -  40  =  75 Walking\n","525 1330  -  1245  ) -  40  =  45 Longboard L\n","526 1400  -  1330  ) -  40  =  30 Stretching\n","527 1930  -  1900  ) -  40  =  30 Guitar\n","528 930  -  900  ) -  40  =  30 Guitar\n","529 1000  -  930  ) -  40  =  30 Stretching\n","530 1200  -  1130  ) -  40  =  30 Walking\n","531 1215  -  1200  ) -  40  =  15 Stretching\n","532 1815  -  1800  ) -  40  =  15 Guitar\n","533 2115  -  2100  ) -  40  =  15 Stretching\n","534 930  -  900  ) -  40  =  30 Guitar\n","535 1000  -  930  ) -  40  =  30 Stretching\n","536 1200  -  1130  ) -  40  =  30 Walking\n","537 1215  -  1200  ) -  40  =  15 Stretching\n","538 1815  -  1800  ) -  40  =  15 Guitar\n","539 2115  -  2100  ) -  40  =  15 Stretching\n"]}]},{"cell_type":"code","source":["# |a = 'abcdefg'\n","for i in range(1,len(a)):\n","print(a[:i])\n","print(a[i])\n","print('zzzzz')\n","\n","print(a[-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":71},"id":"nyadd2nCLeqQ","executionInfo":{"status":"ok","timestamp":1744643639651,"user_tz":300,"elapsed":60,"user":{"displayName":"David Leifer","userId":"06279506333224389759"}},"outputId":"cf768e16-ff55-4904-e797-9dbe27c094a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ab\n","7\n"]},{"output_type":"execute_result","data":{"text/plain":["\"\\nfor i in range(1,len(a)):\\nprint(a[:i])\\nprint(a[i])\\nprint('zzzzz')\\n\\nprint(a[-1])\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["abc = [\n","        [[0], ['a']],\n","        [[1], ['b']],\n","        [[2], ['c']],\n","        [[3], ['d']],\n","        [[4], ['e']],\n","        [[5], ['f']],\n","        [[6], ['g']],\n","        [[7], ['h']],\n","        [[8], ['i']],\n","        [[9], ['j']],\n","                      ]\n","# for i in range(10):\n","print(abc[1:2])"],"metadata":{"id":"SLyAYj_xtzdg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744137032714,"user_tz":300,"elapsed":6,"user":{"displayName":"David Leifer","userId":"06279506333224389759"}},"outputId":"3bb4744a-366f-4183-a0ca-7f695d31af9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[1], ['b']]]\n"]}]},{"cell_type":"code","source":["a = '1234'\n","b = '567'\n","c = '89'\n","\n","\n","if len(a) == 4:\n","  end_sub = a[2:]\n","  print(end_sub)\n","\n","if len(b) == 3:\n","  end_sub = b[:1]\n","  print(end_sub)\n","\n","if len(c) == 2:\n","  end_sub = c"],"metadata":{"id":"kti4uK-eVPeZ","executionInfo":{"status":"ok","timestamp":1744769169351,"user_tz":300,"elapsed":9,"user":{"displayName":"David Leifer","userId":"06279506333224389759"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1c1bc210-f9b0-4b10-b49e-62551d97b986"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["34\n","5\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"DpBo2LQ20Abv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bNU5wcEd0AeC"},"execution_count":null,"outputs":[]}]}